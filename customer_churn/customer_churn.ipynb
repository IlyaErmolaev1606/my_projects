{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I. Изучение общей информации:\n",
    "* [1. Изучение файлов с данными, получение общей информации](#1-section)\n",
    "\n",
    "##### II. Подготовка данных:\n",
    "* [1. Разбиение на выборки](#2-section)\n",
    "\n",
    "##### III. Исследование моделей классификации:\n",
    "* [1. Константная модель](#3-section)\n",
    "* [2. Дисбаланс классов](#4-section)\n",
    "* [3. Баланс классов](#5-section)\n",
    "* [4. Изменение порога](#6-section)\n",
    "* [5. Визуализация метрик](#7-section)\n",
    "\n",
    "##### IV. Тестирование модели:\n",
    "* [1. Проверка модели](#8-section)\n",
    "\n",
    "##### [V. Общий вывод](#9-section)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Изучение общей информации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1-section'></a>\n",
    "### 1. Изучение файлов с данными, получение общей информации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим необходимые библиотеки\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    confusion_matrix, \n",
    "    recall_score, \n",
    "    precision_score, \n",
    "    f1_score, \n",
    "    precision_recall_curve, \n",
    "    roc_auc_score, \n",
    "    roc_curve,\n",
    "    make_scorer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import xgboost as xgb\n",
    "import catboost as ctb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "3     1.0       0.00              2          0               0   \n",
       "4     2.0  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# запишем датасет Churn.csv в 'churn'\n",
    "try:\n",
    "    churn = pd.read_csv('***/Churn.csv')\n",
    "except:\n",
    "    print('Ошибка при чтении файла!')\n",
    "# посмотрим на первые 5 строк\n",
    "churn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 14)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# общая информация о датафрейме\n",
    "print(churn.shape)\n",
    "churn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenure - это количество недвижимости у клиента. Значит Tenure должен иметь тип данных int64. Также в данном столбце присутствуют пропуски.\n",
    "\n",
    "Пропуски могли возникнуть из-за разных причин. Данные могли затереться при копировании/скачивании/форматировании. А может клиенты специально не указали данные о количестве своей недвижимости. Заменим их на ноль. Посмотрим, как отработает модель. \n",
    "\n",
    "Но для начала приведем названия столбцов к нижнему регистру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# приведем названия столбцов к нижнему регистру\n",
    "churn.columns = churn.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подробнее изучим данные\n",
    "# создадим функцию, возвращающую детальную информацию о данных по столбцу\n",
    "def get_info_column(data, column):\n",
    "    print('{: ^}'.format(\"_\" * (len(\"Числовое описание данных столбца\") + len(column) + 1)))\n",
    "    print()\n",
    "    print(\"Числовое описание данных столбца\", column)\n",
    "    print()\n",
    "    print(\n",
    "        \"Коэффициент корреляции Пирсона с целевым признаком: {:.2f}\".format(\n",
    "            data[column].corr(data[\"exited\"])\n",
    "        )\n",
    "    )\n",
    "    print()\n",
    "    print(data[column].describe())\n",
    "    print()\n",
    "    if len(churn[column].unique()) > 2:\n",
    "        if data[column].value_counts().min() != data[column].value_counts().max():\n",
    "            print('Наиболее частотные значения столбца')\n",
    "            print()\n",
    "            print(data[column].value_counts().head())\n",
    "            print()\n",
    "            print('Наименее частотные значения столбца')\n",
    "            print()\n",
    "            print(data[column].value_counts().tail())\n",
    "            print()\n",
    "        print()\n",
    "        if data[column].dtype in ['int64', 'float64']:\n",
    "            if data.groupby(column)[column].count().max() != data.groupby(column)[column].count().min():\n",
    "                print('Максимальные значения столбца')\n",
    "                print()\n",
    "                print(data.groupby(column)[column].count()[::-1].head())\n",
    "                print()\n",
    "                print('Минимальные значения столбца')\n",
    "                print()\n",
    "                print(data.groupby(column)[column].count().head()[::-1])\n",
    "                print()\n",
    "            print(\"Диаграмма размаха столбца\", column)\n",
    "            sns.boxplot(x=data[column])\n",
    "            plt.show()\n",
    "            print()\n",
    "            print(\"Гистограмма для столбца\", column)\n",
    "            ax = sns.distplot(data[column])\n",
    "            plt.show()\n",
    "    else:\n",
    "        print()\n",
    "        print('Распределение данных столбца', column)\n",
    "        print()\n",
    "        print(data[column].value_counts())\n",
    "        churn[column].value_counts().plot(kind='pie', subplots=True, figsize=(9,6), autopct='%1.1f%%')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________\n",
      "\n",
      "Числовое описание данных столбца creditscore\n",
      "\n",
      "Коэффициент корреляции Пирсона с целевым признаком: -0.03\n",
      "\n",
      "count    10000.000000\n",
      "mean       650.528800\n",
      "std         96.653299\n",
      "min        350.000000\n",
      "25%        584.000000\n",
      "50%        652.000000\n",
      "75%        718.000000\n",
      "max        850.000000\n",
      "Name: creditscore, dtype: float64\n",
      "\n",
      "Наиболее частотные значения столбца\n",
      "\n",
      "850    233\n",
      "678     63\n",
      "655     54\n",
      "705     53\n",
      "667     53\n",
      "Name: creditscore, dtype: int64\n",
      "\n",
      "Наименее частотные значения столбца\n",
      "\n",
      "419    1\n",
      "417    1\n",
      "373    1\n",
      "365    1\n",
      "401    1\n",
      "Name: creditscore, dtype: int64\n",
      "\n",
      "\n",
      "Максимальные значения столбца\n",
      "\n",
      "creditscore\n",
      "850    233\n",
      "849      8\n",
      "848      5\n",
      "847      6\n",
      "846      5\n",
      "Name: creditscore, dtype: int64\n",
      "\n",
      "Минимальные значения столбца\n",
      "\n",
      "creditscore\n",
      "363    1\n",
      "359    1\n",
      "358    1\n",
      "351    1\n",
      "350    5\n",
      "Name: creditscore, dtype: int64\n",
      "\n",
      "Диаграмма размаха столбца creditscore\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Гистограмма для столбца creditscore\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# применим функцию get_info_column() к столбцу 'creditscore'\n",
    "get_info_column(churn,'creditscore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кредитный рейтинг имеет нормальное распределение. Заметим всплеск на значении 850. Это максимально возможное значение рейтинга. То есть 233 человека являются максимально добропорядочными с точки зрения кредитного доверия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________\n",
      "\n",
      "Числовое описание данных столбца age\n",
      "\n",
      "Коэффициент корреляции Пирсона с целевым признаком: 0.29\n",
      "\n",
      "count    10000.000000\n",
      "mean        38.921800\n",
      "std         10.487806\n",
      "min         18.000000\n",
      "25%         32.000000\n",
      "50%         37.000000\n",
      "75%         44.000000\n",
      "max         92.000000\n",
      "Name: age, dtype: float64\n",
      "\n",
      "Наиболее частотные значения столбца\n",
      "\n",
      "37    478\n",
      "38    477\n",
      "35    474\n",
      "36    456\n",
      "34    447\n",
      "Name: age, dtype: int64\n",
      "\n",
      "Наименее частотные значения столбца\n",
      "\n",
      "92    2\n",
      "88    1\n",
      "82    1\n",
      "85    1\n",
      "83    1\n",
      "Name: age, dtype: int64\n",
      "\n",
      "\n",
      "Максимальные значения столбца\n",
      "\n",
      "age\n",
      "92    2\n",
      "88    1\n",
      "85    1\n",
      "84    2\n",
      "83    1\n",
      "Name: age, dtype: int64\n",
      "\n",
      "Минимальные значения столбца\n",
      "\n",
      "age\n",
      "22    84\n",
      "21    53\n",
      "20    40\n",
      "19    27\n",
      "18    22\n",
      "Name: age, dtype: int64\n",
      "\n",
      "Диаграмма размаха столбца age\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Гистограмма для столбца age\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEGCAYAAACD7ClEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXyc1X3v8c9vNNp3y5K8aPEmG9vY2CBsh30xCSQkDgGCoWlISkpyA23TtLdN2lfThJvbljQ3JC00CQUaSkKAkBBMcANhXwLGMsbGlm0sL8harMXarV06948ZgSIka2yNNDN6vu/Xyy/NPHNG89P4kb5zzvOc85hzDhER8SZfpAsQEZHIUQiIiHiYQkBExMMUAiIiHqYQEBHxMH+kCxhp5syZbt68eZEuQ0Qkpmzbtq3ROZd7ss+LuhCYN28eZWVlkS5DRCSmmNm7p/I8DQeJiHiYQkBExMMUAiIiHqYQEBHxMIWAiIiHKQRERDxMISAi4mEKARERD1MIiIh4WNTNGJbIe3BL5ajbb1hbNMWViMhkU09ARMTDFAIiIh6mEBAR8TCFgIiIhykEREQ8TCEgIuJhIYWAmV1uZvvMrMLMvjbK44lm9nDw8S1mNm/YYyvN7DUz221mb5tZUvjKFxGRiRg3BMwsDrgLuAJYBlxvZstGNLsJaHbOLQLuAG4PPtcP/BT4knNuOXAR0Be26kVEZEJC6QmsASqccwedc73AQ8CGEW02APcHbz8KXGpmBnwY2Omc2wHgnDvmnBsIT+kiIjJRocwYngscGXa/Clg7VhvnXL+ZtQI5wGLAmdlTQC7wkHPuOyNfwMxuBm4GKCrSrNTJoFnAIjKaUHoCNso2F2IbP3Ae8EfBr1eZ2aUfaOjc3c65UudcaW5ubggliYhIOIQSAlVA4bD7BUDNWG2CxwEygabg9hedc43OuU5gM3DmRIsWEZHwCCUEtgIlZjbfzBKAjcCmEW02ATcGb18DPOecc8BTwEozSwmGw4VAeXhKFxGRiRr3mEBwjP9WAn/Q44D7nHO7zew2oMw5twm4F3jAzCoI9AA2Bp/bbGbfIxAkDtjsnHtykn4WERE5SSEtJe2c20xgKGf4tm8Mu90NXDvGc39K4DRRERGJMpoxLCLiYbqojEzIaKee6rRTkdihnoCIiIcpBEREPEwhICLiYQoBEREPUwiIiHiYQkBExMMUAiIiHqYQEBHxMIWAiIiHKQRERDxMISAi4mEKARERD1MIiIh4mEJARMTDFAIiIh6mEBAR8TCFgIiIhykEREQ8TCEgIuJhCgEREQ9TCIiIeJhCQETEw0IKATO73Mz2mVmFmX1tlMcTzezh4ONbzGxecPs8M+sys7eC/34U3vJFRGQi/OM1MLM44C7gMqAK2Gpmm5xz5cOa3QQ0O+cWmdlG4HbguuBjB5xzq8Jct4iIhEEoPYE1QIVz7qBzrhd4CNgwos0G4P7g7UeBS83MwlemiIhMhlBCYC5wZNj9quC2Uds45/qBViAn+Nh8M9tuZi+a2fmjvYCZ3WxmZWZW1tDQcFI/gIiInLpQQmC0T/QuxDa1QJFzbjXwVeBBM8v4QEPn7nbOlTrnSnNzc0MoSUREwmHcYwIEPvkXDrtfANSM0abKzPxAJtDknHNAD4BzbpuZHQAWA2UTLVzgwS2Vo26/YW3RFFciIrEqlJ7AVqDEzOabWQKwEdg0os0m4Mbg7WuA55xzzsxygweWMbMFQAlwMDyli4jIRI3bE3DO9ZvZrcBTQBxwn3Nut5ndBpQ55zYB9wIPmFkF0EQgKAAuAG4zs35gAPiSc65pMn4QERE5eaEMB+Gc2wxsHrHtG8NudwPXjvK8XwK/nGCNIiIySTRjWETEwxQCIiIephAQEfEwhYCIiIcpBGRU3X0D1LZ20TcwGOlSRGQShXR2kHhDY0cPP37xAI9tr6axoxcAn8GsjCTWLsjh+jWFaEkokelFISD09g9y5/MV3PPyQbr7Blicn86ZRdlkpyRQ19ZNRUMHj22vprtvgH/51EoyU+IjXbKIhIlCwOPq27v58k/fpOzdZj62cjZfvWwxWw7+4Xy+9c7xyv5GfldeR3ntKzx+y7lkpSREqGIRCScdE/CwmpYuPv7vr7C7po07b1jNXTecycLctA+085lxweJcfvaFtdS0dPFXj+xgcHDkGoIiEosUAh5V397Nfa8ewu/z8asvn8OVK+eM+5y1C3L4+48u5dm99dz9spaAEpkONBzkQS2dvfzXq4fxmfGzL6xl3szUkJ974znzeONwE//61D7WLcgZ/wkiEtXUE/CY3v5BfvL7w/T0D/D5c+edVAAAmBn/cvVKslMS+KfNewisFi4isUoh4DG/2VlDQ3sPN6wpZnZm8il9j4ykeP7skkW8caiJivqOMFcoIlNJIeAhb1e3UvZuMxcszmVR3gcPAJ+MjWsKmZuVzNPldeoNiMQwhYBHtHb18dj2Kgqyk1m/NH/C3y/RH8dX1pdQ3dLF7pq2MFQoIpGgEPCIzW/X0j/guK60kDhfeGb9XrV6LrlpiTy/r169AZEYpRDwgDcONfF2dSsXLM4lJy0xbN/XH+fjnEU51LZ2U9XcFbbvKyJTRyEwzQ0MOr65aTeZyfFcUJIb9u+/qiCLBL+PLYd01VCRWKQQmOYeKTtCeW0bV5w+iwR/+P+7E+PjWFWQxc6qFrp6B8L+/UVkcikEprHe/kF+8Mx+zirOZsXczEl7nTXzZ9A/6HizsnnSXkNEJodCYBp7bHsVR9u6+YtLSyZ1Ceg5WckUZifzxqEmHSAWiTFaNmKaGhh0/OjFg6yYm8n5JTP5+RtHJvX11syfwS/frKayqXPMNg9uqfzAthvWFk1mWSIyDvUEpqn/2VXLocbjfPmihVNyIZjlczLx+4yd1a2T/loiEj4hhYCZXW5m+8yswsy+NsrjiWb2cPDxLWY2b8TjRWbWYWZ/HZ6y5UScc9z1/AEW5qbykeWzpuQ1k+LjWJyfzq6qVga0zLRIzBg3BMwsDrgLuAJYBlxvZstGNLsJaHbOLQLuAG4f8fgdwP9MvFwJxesHm9hT28YXL1yIL0wTw0KxsiCT9p5+3tDpoiIxI5SewBqgwjl30DnXCzwEbBjRZgNwf/D2o8ClFhyDMLNPAgeB3eEpWcbzsy3vkpkczyfOGP8aAeF02qwMEuJ8PLGzZkpfV0ROXSghMBcYflSxKrht1DbOuX6gFcgxs1Tgb4FvTbxUCUVHTz9P7T7K1WcWkBQfN6WvneD3cdrsdH676yh9A4NT+toicmpCCYHRxhNGDvqO1eZbwB3OuROuN2xmN5tZmZmVNTQ0hFCSjGXbu830DbiInXWzcm4WTcd7+f2BYxF5fRE5OaGEQBVQOOx+ATCyv/9eGzPzA5lAE7AW+I6ZHQa+Avydmd068gWcc3c750qdc6W5ueFf2sArBp1j6+Em1s6fMeGlok/V4vw00hP9bN5ZG5HXF5GTE8o8ga1AiZnNB6qBjcANI9psAm4EXgOuAZ5zgVlD5w81MLNvAh3OuTvDULeM4kB9B03He/mjdcURq8Ef5+PCJbk8u7eewUE3pQemReTkjdsTCI7x3wo8BewBHnHO7Taz28zsE8Fm9xI4BlABfBX4wGmkMvm2VTaTHB/HR5ZP/HoBE7F+aT6NHT2aMyASA0KaMeyc2wxsHrHtG8NudwPXjvM9vnkK9UmIuvsGKK9p46zibBL9U3tAeKSLluQS5zOeKa9jVWFWRGsRkRPTjOFporymjf5Bx+oo+KOblZJAaXE2z+ypi3QpIjIOhcA08daRFmakJlA4IyXSpQCBIaG9R9upah57LSERiTyFwDTQ2tXHgYYOVhVmTck6QaFYvyxwXOLZPfURrkRETkQhMA3srGrBQVSNv8+fmcqC3FQNCYlEOYXANPDWkRYKs5OZGcbrB4fD+qX5vH7wGO3dfZEuRUTGoBCIcY3tPdS2drOyIHp6AUPWL82nb8Dx8v7GSJciImNQCMS4XTWBc/GXz8mIcCUfdGZRFlkp8RoSEoliCoEYt6umlcLsZLJSEiJdygf443xcsiSP5/fW6xoDIlFKIRDDmo73UtPSzemTeBH5ibp0aT7NnX26CL1IlFIIxLBdwWUZojkELlg8k/g405CQSJRSCMSwXTWtFGQnkx2FQ0FD0pPiWbcgh2fKFQIi0UghEKOqmjupau7i9DnR2wsYculpeRxoOM6xjp5IlyIiIygEYtTTuwOfrKPxrKCRLl0amD2852h7hCsRkZEUAjHq2b115KUnkhNlE8RGUzgjhSX56eytbYt0KSIygkIgBrV29bHlYBNLZ0d/L2DI+mV5HD52nK7egUiXIiLDKARi0IvvNNA/6Fg6Kz3SpYTs0qX5DDp4p05DQiLRRCEQg54pryMnNYGCKFk2OhSrCrJITfSz56iGhESiiUIgxvQNDPLCvnouOS0PX5QsGx0Kn884LT+dd+raNXtYJIooBGLM1sNNtHX3v7defyw5bXY63X2DvHvseKRLEZGgkK4xLFPrwS2VH9h2w9oiAJ4pryfB7+P8kpn8envNVJc2IYvy0ojzGXuPtrMgNy3S5YgICoGY8+I79axbkENKwtT/140WTicj0R/HwtxUymvbuOL0WVFzFTQRL1MIxJDqli4ONBzn+jVFYfueE/3DfrKWz87ksbeqOdrWzezM5Cl9bRH5IB0TiCGv7G8A4ILFuRGu5NQtnZOBAbtrdJaQSDRQCMSQl/Y3kp+RSEle7I6npyX6Kc5JoVwhIBIVQgoBM7vczPaZWYWZfW2UxxPN7OHg41vMbF5w+xozeyv4b4eZXRXe8r1jYNDxakUj5y3Kjfmx9OVzMjna1q0F5USiwLjHBMwsDrgLuAyoAraa2SbnXPmwZjcBzc65RWa2EbgduA7YBZQ65/rNbDaww8yecM71h/0nmeZ217TS0tnHBYtnRrqUCVs2O4Mn36494ZDQic6QEpHwCaUnsAaocM4ddM71Ag8BG0a02QDcH7z9KHCpmZlzrnPYH/wkQLOETtHQxdrPXRT7IZCdmsCcrCTKtaCcSMSFEgJzgSPD7lcFt43aJvhHvxXIATCztWa2G3gb+NJovQAzu9nMysysrKGh4eR/Cg946Z0Gls/JYGYMrBoaimWzM6ls6uRoa3ekSxHxtFBCYLQB6JGf6Mds45zb4pxbDpwNfN3Mkj7Q0Lm7nXOlzrnS3NzYPfNlsvT0D/BmZTPnlcR+L2DIiuAlMX+zM7YmvIlMN6GEQBVQOOx+ATDyN/e9NmbmBzKBpuENnHN7gOPA6adarFcdajxO34DjgpLpE5C56YnMyUziiZ21kS5FxNNCCYGtQImZzTezBGAjsGlEm03AjcHb1wDPOedc8Dl+ADMrBpYAh8NSuYfsr+8gKd7HWcXZkS4lrFYWZLHjSIvWEhKJoHFDIDiGfyvwFLAHeMQ5t9vMbjOzTwSb3QvkmFkF8FVg6DTS8wicEfQW8BjwZedcY7h/iOmuoq6DtfNzSIqPi3QpYbWiYGhISL0BkUgJadkI59xmYPOIbd8YdrsbuHaU5z0APDDBGj2tpbOXho4ezp9GxwOGZKckcFZxNpvequGWixdFuhwRT9LaQVGuor4DgPNP4njAVK8HNBEfXzmbbz5Rzr6j7SyJoSuliUwXWjYiyu2v7yA9yc/i/NhdKuJEPrZyDj6DTTuqI12KiCcpBKLYoHMcaOigJC8t5peKGEtueiLnl+TyqzerdcUxkQhQCESx2pZuOnsHWBTDC8aF4trSAmpbu3m1QucMiEw1hUAU21/fDsDCaX4VrvVL88lMjufRbVWRLkXEcxQCUWx/fQezM5NIT4qPdCmTKik+jg2r5vDU7qO0dvVFuhwRT1EIRKme/gEqj3XG9LUDTsY1ZxXQ0z/IEzu0jITIVFIIRKnDjccZcI5Fed44bXLF3EyW5KfzCw0JiUwphUCU2l/fgd9nFOekRLqUKWFmXHd2ITuOtLC7pjXS5Yh4hkIgSu2v72D+zFTi47zzX3T1mQUkxfv46euxM9lNJNZ55y9MDGnt6qOhvcczxwOGZKbE84kz5vD4W9V09w1EuhwRT1AIRKGK4KmhXjkeMNxn1hXT2TvA9srmSJci4glaOygK7a/vID3RT35GbF5FbCJrF60syOKMgky2HGpi3YKcaTtTWiRaqCcQZQYGHRX1HSyaxktFjOeP1hVT397DoUZdZ0BksikEoszb1a109g6wON97Q0FDPnHGHFIS4rSMhMgUUAhEmRf3NWAw7dcLOpGk+DjWLchh79F2Gtp7Il2OyLSmYwJR5oV36pmbnUxq4h/+18TSNQLCYd2CHF56p4FXKxr55Oq5kS5HZNpSTyCKNB/vZceRFk8PBQ1JS/SzuiiLNyub6ejpj3Q5ItOWQiCKvFzRyKBDIRB07qKZ9A86thw6FulSRKYthUAUeXFfA1kp8RRkJ0e6lKiQl57Ekvx0Xj9wjL6BwUiXIzItKQSixOCg48V3Gji/JBefR08NHc15JTM53jvAW5UtkS5FZFpSCESJ8to2Gjt6uHBx6BeU94IFM1OZk5nEKxWNDOrykyJhpxCIEs/sqcMMhcAIZsZ5JTNp6OjhhXfqI12OyLSjU0SjxDN76lhdmEVuemwuFTGZVszN4qnddfznS4e45LT8Dzw+2umzN6wtmorSRGJeSD0BM7vczPaZWYWZfW2UxxPN7OHg41vMbF5w+2Vmts3M3g5+vSS85U8PNS1d7Kpu47JlsyJdSlSK8xnnLMzhtYPH2HFExwZEwmncEDCzOOAu4ApgGXC9mS0b0ewmoNk5twi4A7g9uL0R+LhzbgVwI/BAuAqfTp7ZUwfAZcs++ClXAs6eN4OMJD93PV8R6VJEppVQegJrgArn3EHnXC/wELBhRJsNwP3B248Cl5qZOee2O+eGLhq7G0gyM413jPC78joWzEz19FIR40mKj+Nz587n6fI69h1tj3Q5ItNGKCEwFzgy7H5VcNuobZxz/UArkDOizdXAdufcBxaDMbObzazMzMoaGhpCrX1aaOvu4/WDx1ivXsC4Pn/OPFIS4viPF9QbEAmXUEJgtJPWR56rd8I2ZracwBDRF0d7Aefc3c65UudcaW6ut86OeXFfA30DTkNBIchOTeAz64p5YkcNh7XMtEhYhHJ2UBVQOOx+AVAzRpsqM/MDmUATgJkVAI8Bn3XOHZhwxTFqrDNYni6vY0ZqAmcWZUegqtjzhfPn85PfH+au5yv412vPiHQ5IjEvlJ7AVqDEzOabWQKwEdg0os0mAgd+Aa4BnnPOOTPLAp4Evu6cezVcRU8X3X0DPLunjo8szyfOp1nCochLT+Iza4v51fZqXXRGJAzGDYHgGP+twFPAHuAR59xuM7vNzD4RbHYvkGNmFcBXgaHTSG8FFgH/YGZvBf/lhf2niFEv7Kuns3eAj62YE+lSYsqXLlpAfJzxb8/uj3QpIjEvpMlizrnNwOYR274x7HY3cO0oz/s28O0J1jht/WZnLTNSE1i3YEakS4kpeelJ3Pihedz98kFuuXhhpMsRiWlaNiJCevsHeXZPPZefPgt/nP4bTtYXL1xISnwcdzyj3oDIROivT4Tsq2unq2+AK1fOjnQpMWlGagJ/ct58ntxZS1VzZ6TLEYlZCoEIebu6lZlpCaydP3I6hYTqixcuZGZaApvfrsU5rTAqciq0gFwE9PQPsO9oGxvPLvL8WUETuXZyWqKfv7xsMX//2C7Ka9tYPiczjJWJeIN6AhFQXtNG34Dj42forKCJuq60kLz0RH676yj9g7r6mMjJUghEwPYjLWSnxFNarAliE+WP83HF6bM4dryX1w7oWsQiJ0shMMXauvo4UN/BqsIsfB4fCgqXJbMyOG1WOs/uqaelszfS5YjEFIXAFNtR1YIDVheqFxBOH185B4fjybdrI12KSExRCEyx7ZUtFGQnM1NXEAur7NQELl6Sx+6aNvYdbYt0OSIxQyEwhWpbuzja1s3qwqxIlzItnVcyk9y0RB5/q4b27r5IlyMSExQCU2h7ZQs+gxUFCoHJ4Pf5uPrMubR29fFPm/dGuhyRmKB5AlOkf3CQNyubWTo7g7REve2TpSgnlfNKZvLzNyq54vRZXLD4/etTjDUnQRelFy9TT2CK7Kltp7N3gNJiLRY32dYvzWdhbip/+8udtGlYSOSEFAJTZNu7TWQmx1OSr+sIT7b4OB//79OrqGvr5v/+Zk+kyxGJagqBKVDd0sX+ug7OLMrGZ5obMBVWFWbxxQsX8nDZEZ7fVx/pckSilganp8CjZVU44KwRM4Qnsm6OjO8r60t4dk8dX/vlTp7+ywsjXY5IVFJPYJINDDoeKTvCwtxUZqQmRLocT0n0x/Hda8+gsaOXb27aHelyRKKSQmCSPb+3nuqWLtZoyeiIWFmQxZ9dsojHtlezo6ol0uWIRB2FwCR74PV3yc9IZNnsjEiX4lm3XryI1UVZPP5WtdYWEhlBITCJDjce58V3Grh+ja4bEEn+OB/fv24Vg4Pwi21VDOoCNCLvUQhMop9teRe/z7h+jSYjRVpxTipXrpzNocbjvLK/MdLliEQNnR00Sbr7BnikrIqPLJ9FfkZSpMuZFiZ6NtVZxdnsq2vnd+V1LMpLY05WcpgqE4ld6glMkl9vr6a1q4/PrCuOdCkSZGZ8ctVcUhLjeLjsCH0DuhKZiEJgEjjnuPeVQyydncG6BVomIpqkJvq55swCGtp72LSjJtLliERcSCFgZpeb2T4zqzCzr43yeKKZPRx8fIuZzQtuzzGz582sw8zuDG/p0eul/Y3sr+/gC+fNxzRDOOqU5Kdz0ZJctr3bTNnhpkiXIxJR44aAmcUBdwFXAMuA681s2YhmNwHNzrlFwB3A7cHt3cA/AH8dtopjwD0vHyQvPVEXko9iQ4vMbdpRw67q1kiXIxIxofQE1gAVzrmDzrle4CFgw4g2G4D7g7cfBS41M3POHXfOvUIgDDxh39F2Xt7fyGc/VEyCX6Nt0cpnxnVnF5Ga6OcL95dR3dIV0vMe3FL5gX8isSyUv1JzgSPD7lcFt43axjnXD7QCIU+RNbObzazMzMoaGhpCfVpUuuflgyTF+7hhrQ4IR7u0RD+f/VAxx3v6+ey9W2g6rolk4j2hhMBog9ojZ9uE0mZMzrm7nXOlzrnS3Nzc8Z8QpapbunhsezWfLi3UOkExYnZmMvfcWMqR5i4+/19v0KwgEI8JZZ5AFVA47H4BMPK0iqE2VWbmBzKBaX/EbeRQwBPBs02+eOHCSJQjp2jtghzuuuFMbvnZm1z1H69y7+fOZmGurvsg3hBKCGwFSsxsPlANbARuGNFmE3Aj8BpwDfCcc96am9/R08/Ww01ctXouczUJKeJOdqz+smX5/Pzmtdz839u46q5X+edPreSjK2bp7C6Z9sYNAedcv5ndCjwFxAH3Oed2m9ltQJlzbhNwL/CAmVUQ6AFsHHq+mR0GMoAEM/sk8GHnXHn4f5TIerWikYFBx5cuUi8gVp1VPINf33Iu/+tn27jlwTc5b9FMvvHxZSzOTz+l7zdaEOl6xhJtQlo2wjm3Gdg8Yts3ht3uBq4d47nzJlBfTOjs7ef1g8c4fW6mhhFiXOGMFH795XN58I1KvvvUPj58x0tctCSXz587n/MWzYx0eSJhp7WDwuCldxrp7R/k4iV5kS5FwsAf5+OzH5rHlSvn8N+vHeanr1dy431vkJeeyMK8NM6Ym0nhjBQNFcm0oBCYoPbuPl472MjKgkxmZWqhuOlkRmoCX1m/mC9ftIiny4/ymx21PLOnjtcOHCMrOZ4VczNZUZCJc06BIDFLITBBz+9rYGDQsX5pfqRLkUmS4Pdx5co5XLlyDve9cog9tW28Xd3K7w8c4+WKRjbtqOFjK2bzsZWzWTE3U4EgMUUhMAHNnb1sPdTEWcXZ5KQlRrocmQJJ8XGsLspmdVE2Xb0DlNe20XS8h3tfOcSPXzpI4YxkPrpiNleumKMegsQEhcAEPL37KGboWIBHJSfEcVZxNjesLaKls5eny+vY/HYt9758iB+/eJDctETOLM5mVWEWmcnxY36fsU5n1ZlEMhUUAqeo7HATO6pauXhJLlkp788O1loy3pSVksCnSwv5dGkhLZ29/M+uo/zohQM8tfsoT+8+yqK8NM4szuZTZ84lKT4u0uWKvEchcAoGBx3ffGI3mcnxXLhYvQD5Q1kpCVy/pgjnoLGjh+2VzWyvbOHhrUfY/HYtV66cw2fWFbF8TmakSxVRCJyKX2w7wq7qNq4rLdRKoXJCM9MSuWzZLC5dms+hxuM0d/by6+3V/PyNSs5ZmMOfnr+AQefw6diBRIhC4CQ1dvRw+2/3UVqczcoCfZKbrsI9rOczY2FuGjesLeIfP97Hz9+o5CevHubzP9lKbnoi5y2ayeqiLPw+faiQqaU97iT94+O76eju558/tUJnfsgpyUyO50sXLuSlv7mYO647A7/PeGx7NT94Zj+7qlvx2LJbEmHqCZyEJ3fW8uTbtfzvjyyhJD+drYebI12ShMFEP/Wf6vMT/D6uWl1AZ88A++ra+e2uozz4RiVFM1K44vRZE6pJJFQKgRA1dvTwjcd3sWJuJl+8YEGky5FpxMw4bVYGJXnpvFnZzDN76vjxSwd591gnX//oaRTnpEa6RJnGFAIhGBh0/MVD2+no6ee7156BP06jaBJ+cT7j7HkzOKMgi1cqGnhpfwPPfq+Oz50zj1svKTnhXAORU6UQCMH3n3mHVyuO8Z2rV7Jk1qktKywCoQ0dJfh9XHJaPv901Qq++/Q+7nnlEL98s5q/XF/C9WuK9CFEwkohMI7n99bz789V8OnSAj59duH4TxAJk7yMJL5zzRl89kPz+PaT5fzD47u595VD3HjOPK45q4D0pBP3DDQTWUKhEDiB3TWt3PrgmyybncFtG06PdDniUafPzeTnf7qO35XX8e0n9/CtJ8r55817WZSXRkl+GkUzUrjl4kWaiSynRCEwwtCnp5bOXn704gH8cT7u+9zZ+gWTiDIzPrx8Fo0dvVQ1d7L1cDP769opr20D4M7nK5iTmcyC3FQW5qaxIDeVw42dzMpMIi1Rv+YyNu0dozje089Pfn+YngvG3u8AAAolSURBVP5BvnjBQl0nQKJKQXYKBdkpOOdo6OjhaGs3szOTOdjYwcGG4/yi7AjHewfea5+TmkBxTirzclIozkkddXVTXQrTuxQCI3T29HPvK4doOt7LjefMY1ZmkhaFk4gYb78zM/LSk8hLT/qDP9jOOeraevjPlw9S09LF4WOd7D3axpuVgXkt9792mLOCq5sWZCczOzOZ+vZuUhL8JMfHEefTJEgvsWibnVhaWurKysoi8trNx3v56L+9TEN7D3+8rpiSU7zAuEi0cc7R2NHLu8eOE+czyt5t5lDj8VHbJvp9pCTEsWRWOkUzUlicn86qwiyWz8kkOUHDotHKzLY550pP9nnqCQRVHuvkc//1Bg3tPXxGASDTjJmRm55Ibnrie72G4z391LZ2Ud3SzZM7a+nq7aezb4Cu3gE6evrp7R/kub0NPFJWBQTmMSzOTyctMY7C7BQKZ6SQm56Iz0xDRzFMIQBsr2zmC/eXMeAcf3LufObN1AxNmf5SE/0syktnUV461c1dH3h86A97fVs3O6pa2VnVwltHWig73PzekilJ8T4Ks1Ooa+tm2ZwMls3OYG5WMj4NKcUMTw8HDQ46/vPlg3z36X3MykziJ59fw5aDTVPy2iKxatA5jnX0UtnUSWVTJ0eaOqlr72boT0laop8ls9JZOjud9u5+ZmUkkZ+R9N4Zduo1TA4NB52kvUfb+Namcl47eIyPLM/nXz61kuzUBIWAyDh8w4aWzirOBuCTq+fwTl0He2rb2Fvbxp6j7Tz+Vg3t3f3vPS8jyU9eRhL769spyUunJD+Nkry0P7gyn0y9kELAzC4HfgDEAfc45/5lxOOJwH8DZwHHgOucc4eDj30duAkYAP7cOfdU2Ko/SYODjh1VLdzzyiGe3FlLeqKf71y9kmtLC7QstMgEpCT4WVWYxarCrPe2Oef44QsHONraTV17D/Vt3dS39/DQG0fo6nv/FNbc9ERK8tKYPzM10GvITGJWRhKzMpOYmZZIRpJfS2VMonFDwMzigLuAy4AqYKuZbXLOlQ9rdhPQ7JxbZGYbgduB68xsGbARWA7MAZ4xs8XOuQEmyeCgo3dgkI6eflo6e6lv6+Gdunb2Hm3n+X311LX1kJoQx60XL+IL58/XpxCRMBjrdNaslASyUhI4bfb72zaeXUhNaxf76zrYX98e/NrBr96s/oNwGC490U9GcjxZKfFkBr+mJfpJio8L/PP7SBy6He8jyf/+7QS/j/g4H/FxRnycD7/PR4Lf8Pt8xPt9xPtszJAZa7jcAc6Bw4GDweDtoeY+M3y+wNc4M3w+I85n+Cy4zff+9kgLpSewBqhwzh0EMLOHgA3A8BDYAHwzePtR4E4LfLTeADzknOsBDplZRfD7vRae8t+340gLV//w9/QPjv6flpHk55yFM/nI6flcclq+VmQUiZCHth5573ZaYjyri7JZXRQYVuobGKStq4+27n7auvo43ttPV+8AXcGzlrr6Bqhu7mJ/fQc9fQP0DTj6BwfpG4iuY5snw2cEA8L42IrZfO+6VVP6+qGEwFzgyLD7VcDasdo45/rNrBXICW5/fcRz5458ATO7Gbg5eLfDzPaFVP1Jehv48cS+xUygMRy1THN6n8an92h8nnuP3gHu2HjSTxt6n4pP5TVDCYHR+isjY3esNqE8F+fc3cDdIdQSUWZWdipH371G79P49B6NT+9RaCb6PoVytKUKGL6GcgFQM1YbM/MDmUBTiM8VEZEICSUEtgIlZjbfzBIIHOjdNKLNJuDG4O1rgOdc4IjKJmCjmSWa2XygBHgjPKWLiMhEjTscFBzjvxV4isApovc553ab2W1AmXNuE3Av8EDwwG8TgaAg2O4RAgeR+4FbJvPMoCkQ9UNWUULv0/j0Ho1P71FoJvQ+Rd2MYRERmTqagSEi4mEKARERD1MIjMHMCs3seTPbY2a7zewvgttnmNnvzGx/8Gt2pGuNNDOLM7PtZvab4P35ZrYl+B49HDyhwNPMLMvMHjWzvcF96kPal/6Qmf1l8Hdtl5n93MyStC+Bmd1nZvVmtmvYtlH3HQv4NzOrMLOdZnbmeN9fITC2fuCvnHNLgXXALcFlML4GPOucKwGeDd73ur8A9gy7fztwR/A9aiawrIjX/QD4rXPuNOAMAu+X9qUgM5sL/DlQ6pw7ncBJKENL0Hh9X/oJcPmIbWPtO1cQOAuzhMAE3B+O980VAmNwztU6594M3m4n8Es7l8BSGPcHm90PfDIyFUYHMysAPgbcE7xvwCUElg8BvUeYWQZwAYGz6HDO9TrnWtC+NJIfSA7ONUoBatG+hHPuJQJnXQ431r6zAfhvF/A6kGVmszkBhUAIzGwesBrYAuQ752ohEBRAXuQqiwrfB/4GGAzezwFanHNDawiPulSIxywAGoD/Cg6b3WNmqWhfeo9zrhr4LlBJ4I9/K7AN7UtjGWvfGW2ZnxO+ZwqBcZhZGvBL4CvOubZI1xNNzOxKoN45t2345lGaev08ZD9wJvBD59xq4DgeHvoZTXBMewMwn8CKw6kEhjZG8vq+NJ6T/v1TCJyAmcUTCICfOed+FdxcN9S9Cn6tj1R9UeBc4BNmdhh4iEDX/fsEuqBDExG1VEjg01iVc25L8P6jBEJB+9L71gOHnHMNzrk+4FfAOWhfGstY+85JL9WjEBhDcGz7XmCPc+57wx4avkTGjcDjU11btHDOfd05V+Ccm0fgIN5zzrk/Ap4nsHwIePw9AnDOHQWOmNmS4KZLCcyi1770vkpgnZmlBH/3ht4j7UujG2vf2QR8NniW0DqgdWjYaCyaMTwGMzsPeJnACtRD491/R+C4wCNAEYEd91rnnOevSWlmFwF/7Zy70swWEOgZzAC2A58JXlPCs8xsFYGD5wnAQeDzBD6EaV8KMrNvAdcRODNvO/AFAuPZnt6XzOznwEUEloyuA/4R+DWj7DvBAL2TwNlEncDnnXMnvGi7QkBExMM0HCQi4mEKARERD1MIiIh4mEJARMTDFAIiIh6mEBAR8TCFgIiIhykERIYxs1+b2bbguvY3B7fdZGbvmNkLZvafZnZncHuumf3SzLYG/50b2epFTp4mi4kMY2YzgjMvk4GtwEeAVwms9dMOPAfscM7damYPAv/hnHvFzIqAp4LXnxCJGf7xm4h4yp+b2VXB24XAHwMvDi3nYGa/ABYHH18PLAvM1Acgw8zSg9efEIkJCgGRoOD6R+uBDznnOs3sBWAfMNane1+wbdfUVCgSfjomIPK+TKA5GACnEbisaApwoZllB5c0vnpY+6eBW4fuBBeJE4kpCgGR9/0W8JvZTuD/AK8D1cA/EVg99hkCyxu3Btv/OVAavKB3OfClqS9ZZGJ0YFhkHGaW5pzrCPYEHgPuc849Fum6RMJBPQGR8X3TzN4CdgGHCKzlLjItqCcgIuJh6gmIiHiYQkBExMMUAiIiHqYQEBHxMIWAiIiH/X+06NSCbiyZWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# применим функцию get_info_column() к столбцу 'age'\n",
    "get_info_column(churn, 'age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные по возрасту немного скошены вправо. Самому юному клиенту 18 лет, самому возрастному - 92 года. Возраст большинства клиентов находится в диапазоне 31 - 44 лет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________\n",
      "\n",
      "Числовое описание данных столбца tenure\n",
      "\n",
      "Коэффициент корреляции Пирсона с целевым признаком: -0.02\n",
      "\n",
      "count    9091.000000\n",
      "mean        4.997690\n",
      "std         2.894723\n",
      "min         0.000000\n",
      "25%         2.000000\n",
      "50%         5.000000\n",
      "75%         7.000000\n",
      "max        10.000000\n",
      "Name: tenure, dtype: float64\n",
      "\n",
      "Наиболее частотные значения столбца\n",
      "\n",
      "1.0    952\n",
      "2.0    950\n",
      "8.0    933\n",
      "3.0    928\n",
      "5.0    927\n",
      "Name: tenure, dtype: int64\n",
      "\n",
      "Наименее частотные значения столбца\n",
      "\n",
      "4.0     885\n",
      "9.0     882\n",
      "6.0     881\n",
      "10.0    446\n",
      "0.0     382\n",
      "Name: tenure, dtype: int64\n",
      "\n",
      "\n",
      "Максимальные значения столбца\n",
      "\n",
      "tenure\n",
      "10.0    446\n",
      "9.0     882\n",
      "8.0     933\n",
      "7.0     925\n",
      "6.0     881\n",
      "Name: tenure, dtype: int64\n",
      "\n",
      "Минимальные значения столбца\n",
      "\n",
      "tenure\n",
      "4.0    885\n",
      "3.0    928\n",
      "2.0    950\n",
      "1.0    952\n",
      "0.0    382\n",
      "Name: tenure, dtype: int64\n",
      "\n",
      "Диаграмма размаха столбца tenure\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Гистограмма для столбца tenure\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# применим функцию get_info_column() к столбцу 'tenure'\n",
    "get_info_column(churn, 'tenure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "\n",
      "Числовое описание данных столбца balance\n",
      "\n",
      "Коэффициент корреляции Пирсона с целевым признаком: 0.12\n",
      "\n",
      "count     10000.000000\n",
      "mean      76485.889288\n",
      "std       62397.405202\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%       97198.540000\n",
      "75%      127644.240000\n",
      "max      250898.090000\n",
      "Name: balance, dtype: float64\n",
      "\n",
      "Наиболее частотные значения столбца\n",
      "\n",
      "0.00         3617\n",
      "105473.74       2\n",
      "130170.82       2\n",
      "113063.83       1\n",
      "80242.37        1\n",
      "Name: balance, dtype: int64\n",
      "\n",
      "Наименее частотные значения столбца\n",
      "\n",
      "183555.24    1\n",
      "137648.41    1\n",
      "112689.95    1\n",
      "115465.28    1\n",
      "74681.90     1\n",
      "Name: balance, dtype: int64\n",
      "\n",
      "\n",
      "Максимальные значения столбца\n",
      "\n",
      "balance\n",
      "250898.09    1\n",
      "238387.56    1\n",
      "222267.63    1\n",
      "221532.80    1\n",
      "216109.88    1\n",
      "Name: balance, dtype: int64\n",
      "\n",
      "Минимальные значения столбца\n",
      "\n",
      "balance\n",
      "16893.59       1\n",
      "14262.80       1\n",
      "12459.19       1\n",
      "3768.69        1\n",
      "0.00        3617\n",
      "Name: balance, dtype: int64\n",
      "\n",
      "Диаграмма размаха столбца balance\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Гистограмма для столбца balance\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEGCAYAAACgt3iRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXRc1Znv/e+jWfIgyfKAJQ+SsRlsBgPCYEggDM2QdDDpmI6BgAm8oUkgQ+cOwMq9SRY3WStOupvQHSAQcEJI3GYIt3HfTjCEMQE8yEweQFgekOVRtmxZniRLet4/aosIuUoqqVRVkv37rFVLR/vs8+x9VFI9Oufss4+5OyIiIonISHcHRERk8FMyERGRhCmZiIhIwpRMREQkYUomIiKSsKx0dyAdRo4c6eXl5enuhojIoLJixYqd7j4q2rpjMpmUl5dTVVWV7m6IiAwqZvZRrHU6zSUiIglTMhERkYQpmYiISMKUTEREJGFKJiIikjAlExERSZiSiYiIJEzJREREEqZkIiIiCTsm74AfrBYsrY277nXnTEhiT0REPklHJiIikjAlExERSZiSiYiIJEzJREREEqZkIiIiCVMyERGRhCmZiIhIwuJKJmZ2hZlVm1mNmd0VZX2umT0R1i81s/JO6+4O5dVmdnlPMc2sIsRYG2LmhPLbzGylmb1jZn8xs6k9tSEiIqnRYzIxs0zgfuBKYCpwbecP8uAWYLe7TwbuBeaFbacCc4BpwBXAA2aW2UPMecC97j4F2B1iAyxw91PdfTrwE+Bfumuj1z8JERHps3iOTGYANe6+3t1bgIXArC51ZgGPheWngUvMzEL5QndvdvcNQE2IFzVm2ObiEIMQ82oAd9/bqb0hgHdqO1obIiKSIvEkkzJgU6fv60JZ1Dru3go0AiXdbBurvATYE2Ic0ZaZ3W5m64gcmXyzF/3DzG41syozq6qvr+9hl0VEpDfiSSYWpczjrNNf5ZEF9/vd/XjgTuB/9aJ/uPvD7l7p7pWjRo2KsomIiPRVPMmkDhjf6ftxwJZYdcwsCygEGrrZNlb5TqAoxIjVFkROi13di/6JiEgSxZNMlgNTwiirHCIXuxd1qbMImBuWZwMvubuH8jlhtFcFMAVYFitm2OblEIMQ81kAM5vSqb3PAWs7tR2tDRERSZEep6B391YzuwNYDGQC8919tZndA1S5+yLgUeBxM6shckQyJ2y72syeBNYArcDt7t4GEC1maPJOYKGZ/RB4O8QGuMPMLgUOExnlNbenNkREJDUscjBwbKmsrPSqqqp0d6PX9DwTEUknM1vh7pXR1ukOeBERSZiSiYiIJEzJREREEqZkIiIiCVMyERGRhCmZiIhIwpRMREQkYUomIiKSMCUTERFJmJKJiIgkTMlEREQSpmQiIiIJUzIREZGEKZmIiEjClExERCRhSiYiIpIwJRMREUmYkomIiCRMyURERBKmZCIiIglTMhERkYQpmYiISMKUTEREJGFxJRMzu8LMqs2sxszuirI+18yeCOuXmll5p3V3h/JqM7u8p5hmVhFirA0xc0L5d8xsjZm9Z2YvmtnETtu0mdk74bWobz8KERHpqx6TiZllAvcDVwJTgWvNbGqXarcAu919MnAvMC9sOxWYA0wDrgAeMLPMHmLOA+519ynA7hAb4G2g0t1PA54GftKp/YPuPj28rurVT0BERBIWz5HJDKDG3de7ewuwEJjVpc4s4LGw/DRwiZlZKF/o7s3uvgGoCfGixgzbXBxiEGJeDeDuL7v7gVC+BBjX+90VEZFkiCeZlAGbOn1fF8qi1nH3VqARKOlm21jlJcCeECNWWxA5Wvljp+/zzKzKzJaY2dVx7JOIiPSjrDjqWJQyj7NOrPJoSay7+n9tyOzLQCVwYafiCe6+xcwmAS+Z2Up3X9dlu1uBWwEmTJgQpRkREemreI5M6oDxnb4fB2yJVcfMsoBCoKGbbWOV7wSKQowj2jKzS4HvAle5e3NHubtvCV/XA68AZ3TdCXd/2N0r3b1y1KhRcey2iIjEK55kshyYEkZZ5RC5oN51xNQiYG5Yng285O4eyueE0V4VwBRgWayYYZuXQwxCzGcBzOwM4CEiiWRHR8NmVmxmuWF5JHA+sKY3PwQREUlMj6e53L3VzO4AFgOZwHx3X21m9wBV7r4IeBR43MxqiByRzAnbrjazJ4l8uLcCt7t7G0C0mKHJO4GFZvZDIiO4Hg3lPwWGAk9FrtNTG0ZunQw8ZGbtRJLjj91dyUREJIUscjBwbKmsrPSqqqp0d6PXFiytjbvudefoupCI9C8zW+HuldHW6Q54ERFJmJKJiIgkTMlEREQSpmQiIiIJUzIREZGEKZmIiEjClExERCRhSiYiIpIwJRMREUmYkomIiCRMyURERBKmZCIiIglTMhERkYQpmYiISMKUTEREJGFKJiIikjAlExERSZiSiYiIJEzJREREEqZkIiIiCVMyERGRhCmZiIhIwpRMREQkYUomIiKSsLiSiZldYWbVZlZjZndFWZ9rZk+E9UvNrLzTurtDebWZXd5TTDOrCDHWhpg5ofw7ZrbGzN4zsxfNbGKnbeaG+mvNbG7ffhQiItJXPSYTM8sE7geuBKYC15rZ1C7VbgF2u/tk4F5gXth2KjAHmAZcATxgZpk9xJwH3OvuU4DdITbA20Clu58GPA38JLQxAvg+cA4wA/i+mRX39gchIiJ9F8+RyQygxt3Xu3sLsBCY1aXOLOCxsPw0cImZWShf6O7N7r4BqAnxosYM21wcYhBiXg3g7i+7+4FQvgQYF5YvB15w9wZ33w28QCRxiYhIisSTTMqATZ2+rwtlUeu4eyvQCJR0s22s8hJgT4gRqy2IHK38sRf9w8xuNbMqM6uqr6+PuqMiItI38SQTi1Lmcdbpr/K/NmT2ZaAS+Gkv+oe7P+zule5eOWrUqCibiIhIX8WTTOqA8Z2+HwdsiVXHzLKAQqChm21jle8EikKMI9oys0uB7wJXuXtzL/onIiJJFE8yWQ5MCaOscohcUF/Upc4ioGMU1WzgJXf3UD4njPaqAKYAy2LFDNu8HGIQYj4LYGZnAA8RSSQ7OrW9GLjMzIrDhffLQpmIiKRIVk8V3L3VzO4g8gGdCcx399Vmdg9Q5e6LgEeBx82shsgRyZyw7WozexJYA7QCt7t7G0C0mKHJO4GFZvZDIiO4Hg3lPwWGAk9FrtNT6+5XuXuDmf0fIgkK4B53b0jgZyIiIr1kkYOBY0tlZaVXVVWluxu9tmBpbdx1rztnQhJ7IiLHIjNb4e6V0dbpDngREUmYkomIiCRMyURERBKmZCIiIglTMhERkYQpmYiISMKUTEREJGFKJiIikjAlExERSZiSiYiIJEzJREREEqZkIiIiCVMyERGRhCmZiIhIwpRMREQkYUomIiKSMCUTERFJmJKJiIgkTMlEREQSpmQiIiIJUzIREZGEKZmIiEjClExERCRhSiYiIpKwuJKJmV1hZtVmVmNmd0VZn2tmT4T1S82svNO6u0N5tZld3lNMM6sIMdaGmDmh/AIze8vMWs1sdpf228zsnfBa1Psfg4iIJKLHZGJmmcD9wJXAVOBaM5vapdotwG53nwzcC8wL204F5gDTgCuAB8wss4eY84B73X0KsDvEBqgFbgIWROnmQXefHl5XxbXnIiLSb+I5MpkB1Lj7endvARYCs7rUmQU8FpafBi4xMwvlC9292d03ADUhXtSYYZuLQwxCzKsB3H2ju78HtPdxX0VEJEniSSZlwKZO39eFsqh13L0VaARKutk2VnkJsCfEiNVWNHlmVmVmS8zs6mgVzOzWUKeqvr4+jpAiIhKveJKJRSnzOOv0V3lPJrh7JXAd8DMzO/6IIO4Pu3ulu1eOGjUqjpAiIhKveJJJHTC+0/fjgC2x6phZFlAINHSzbazynUBRiBGrrSO4+5bwdT3wCnBGz7slIiL9JZ5kshyYEkZZ5RC5oN51xNQiYG5Yng285O4eyueE0V4VwBRgWayYYZuXQwxCzGe765yZFZtZblgeCZwPrIljv0REpJ/0mEzC9Ys7gMXA+8CT7r7azO4xs46RU48CJWZWA3wHuCtsuxp4ksiH+3PA7e7eFitmiHUn8J0QqyTExszONrM64BrgITPrqH8yUGVm7xJJRD92dyUTEZEUssjBwLGlsrLSq6qq0t2NXluwtDbuutedMyGJPRGRY5GZrQjXp4+gO+BFRCRhSiYiIpIwJZNBYO+hwzz06jr+6flqXnx/e7q7IyJyhKyeq0g6rdrcyLUPL6GpuZXheVm89MEOTjxuGOOKC9LdNRGRj+nIZIB78JV1mMH/+8an+PalJzAsL4tn3tpMa7tmlRGRgUPJZADb1niIxau38aWzx3NKWSF52ZnMml7Gtr2HeO3DnenunojIx5RMBrAFy2ppc+fL5078uOzkscM5tayQV6p30NzalsbeiYj8lZLJANXS2s6CpbVcdOJoJpYM+cS6cypG0Nru1OzYl6beiYh8kpLJAPXHVVvZua+ZG2dOPGLdxJIh5Gdn8v7WpjT0TETkSEomA9TTK+qYWFLABVOOnOE4M8M48bhhfLBtL+3H4AwGIjLwKJkMQM2tbSzb0MAlJ40hIyParPxw0nHDONDSxqaGAynunYjIkZRMBqC3a/fQ3NrOzONLYtY5YcwwMgze37o3hT0TEYlOyWQAenPdLjIMZlSMiFknLzuTSSOH6rqJiAwISiYD0JvrdzGttJDC/Oxu6500dhj1+5rZ2dScop6JiESnZDLAHGxp4+3a3ZzXzSmuDicdNxyAtTt0dCIi6aVkMsCs+Gg3h9ucc+NIJsUF2QzLy2LT7oMp6JmISGxKJgPMm+t3kplhnF0e+3pJBzNjfHEBtRrRJSJppmQywLy5bhenjStkaG58EzpPGFFAw/4W9jW3JrlnIiKxKZkMIPubW3m3rpGZk3o+xdVh/IjIVPS630RE0knJZABZtbmRtnansrw47m3KivLJMCUTEUkvJZMBZOXmRgBOKSuMe5ucrAzGFuZTu1vJRETSR8lkAFm9ZS9jhucyelher7YbPyKfut0HNU+XiKSNkskAsnJzI6f24qikw/jiAlpa29m+91ASeiUi0rO4komZXWFm1WZWY2Z3RVmfa2ZPhPVLzay807q7Q3m1mV3eU0wzqwgx1oaYOaH8AjN7y8xazWx2l/bnhvprzWxu738M6XegpZV19fuYVtr7ZDLh44vwut9ERNKjx2RiZpnA/cCVwFTgWjOb2qXaLcBud58M3AvMC9tOBeYA04ArgAfMLLOHmPOAe919CrA7xAaoBW4CFnTp3wjg+8A5wAzg+2YW/xXsAWLNlr2406cjkxFDcijIydT9JiKSNvEcmcwAatx9vbu3AAuBWV3qzAIeC8tPA5eYmYXyhe7e7O4bgJoQL2rMsM3FIQYh5tUA7r7R3d8D2ru0fTnwgrs3uPtu4AUiiWtQ6cvF9w4dNy/W6SK8iKRJPMmkDNjU6fu6UBa1jru3Ao1ASTfbxiovAfaEGLHa6kv/BrxVm/cycmguY4bn9mn70qJ86puaaWntmmtFRJIvnmQS7elMXYcNxarTX+XdiWsbM7vVzKrMrKq+vr6HkKm3anMjp5QNJ3Jw1ntlRXk46CK8iKRFPMmkDhjf6ftxwJZYdcwsCygEGrrZNlb5TqAoxIjVVl/6h7s/7O6V7l45atSRj8JNp4Mtbazd0dSn6yUdxhblA7B5jy7Ci0jqxZNMlgNTwiirHCIX1Bd1qbMI6BhFNRt4yd09lM8Jo70qgCnAslgxwzYvhxiEmM/20L/FwGVmVhwuvF8WygaN97ftpd37dr2kQ1F+NvnZmWxtVDIRkdTrMZmE6xd3EPmAfh940t1Xm9k9ZnZVqPYoUGJmNcB3gLvCtquBJ4E1wHPA7e7eFitmiHUn8J0QqyTExszONrM64BrgITNbHdpoAP4PkQS1HLgnlA0aqxK4+N7BzCgrymfLHp3mEpHUi2tqWnf/A/CHLmXf67R8iMiHfLRtfwT8KJ6YoXw9kdFeXcuXEzmFFa2N+cD8bndiAFu1uZERQ3IoLezdne9djS3K4411u2hr153wIpJaugN+AFi5eS/TSvt+8b1DaVE+be3OjiYdnYhIaimZpNmhw22s3Z7YxfcOpYWRi/BbdBFeRFJMySTNqrc10dru/ZJMSobmkJOVoesmIpJy8T3OT5ImkTvfu8owY+zwPB2ZDEALltbGXfe6cyYksSciyaEjkzRbvaWRwvxsxhXn90u80qJ8tjYe0kV4EUkpHZmk2coE73zvqrQojzfXt7Nx136OHzW0X2JK4g62tPFRw37qdh+k6VAr+5sjMwblZWcyNDeLsuJ8xhXnU1yQk+aeivSNkkkatbS2U72tiZs/VdFvMUvDnfCrNjcqmSRZT6euDra08d7mPbz10W427Y6cejSgICeToXmRP71Dh9vZ19z68ZHkccPz2HOwhb87YxzHJThUXCSVlEzS6MPtTRxuc07pwzNMYhk9LI/MDGP1lr3Mmj7o5rs8KuxvbuW1tfUsWb+Lw23O6GG5XHryaMpHDmFcUQE5WZ88u9za3s72xmY27trPys2N/OS5au594UP+vnI8X79oMmVF/XMKVCSZlEzSqOPO9/4YydUhM8M4bngeq7c09ltMiU9bu/N6zU5eqt7B4dZ2Th9fxPnHj6S0KK/b05hZGRmUFedTVpzP+ZNHsmtfM3+u2cnCZZtYuHwTF54wigtPGEV25pGXOHWxXgYKJZM0Wrm5kWF5WUwsKejXuKVFeazavBd377drMdK9LXsO8szbdWzZc4iTjxvG5dOOY/Twvp2mKhmay9XTy/jMCaN4fs12XvpgB+/VNfLFM8uYWDKkn3su0j80miuNVm1u7Jc737saW5hP48HDmkE4BdydJet38eCr69h7sJXrZkzghpnlfU4knRUV5PD3leP5ynnltLW388s/r+fV6h20u0bqycCjZJImh9vaeX9b/9z53lXZxxfh9/Z7bPmrltZ2nlpRx6J3t3D8qCF8+5Ip/XK/UFdTxgzjGxdPYVppIYvXbOc3b27k0OG2fm9HJBFKJmmydvs+Wlrbk/LhM2Z4HhmGrpskUcP+Fh79y3re3bSHS04ezY0zyynITd5Z47zsTOacPZ6rTi+lZsc+fvHqOhr2tyStPZHeUjJJk1Vb+u/O965ysjKYPHooq7foyCQZancd4IsPvsHWxkNcO2MCl5w0howUXJsyM86dVMJXzq9g76HDPPhKDe9s2pP0dkXioWSSJqs2NzIkJ5OKJF1QPaW08OPRYtJ/1tXvY/Yv3qBhfwu3fKoiKf8M9OT4UUP52oWTyc3O5PpfLuGNmp0p74NIV0omabJycyPTSgvJyEjOf7RTS4ezo6lZ09H3o7Xbm/jSQ0tod+fJf5iZ1pFVo4blcusFkxhXXMBNv17O86u3pa0vIqBkkhatbe28v3VvUv+r7YitU139Y339Pq795RLMYOGt53LiccPS3SWG52XzxD+cy9Sxw/n6797iT2u2p7tLcgxTMkmDdfX7OXS4nVPKhietjamlkdirdaorYdv3HuKGR5fhDv/+1XOZPDr9iaRDUUEOv7llBtPKCvn6797i5eod6e6SHKN002IaJOPO966G52VTMXII79YpmSSi8cBhbnx0GXsOtLDw1plMHj2w5jvrmB/sqtNKqW86xFcfq+LGmeVR+6m75SWZlEzSYOXmRvKzM5mU5IkYTx9XyOvrdulO+F7oPHljS2s7v3p9A3V7DjJ3ZjkrNzd+/PyZgSY/J5Obz6vgkb9s4PElG5k7szzpv18inek0Vxqs3tLI1NLhZCbp4nuH6eOLqG9qZmujLsL3Vlu78+/LaqltOMDfV44fcEck0RTkZnHzpyooLsjhsTc3snHn/nR3SY4hSiYp1tburN6yN6mnuDpMn1AMoHsReqndnWfeqqN6exOfP700Je9Vfxmam8Utn6qgMD+bx97cqKduSsoomaTYhp37ONDSxrTS5F1873Dy2GHkZGbwrpJJryxetY23w53t504qSXd3em1YXjY3n19BXnYmv3pjIzv3Nae7S3IMUDJJsXc3hYvv45L/325uViYnlw7nbSWTuL32YT1/rtnJuZNGcPGJo9PdnT4rKsjh5vMrcHfmv76BxoOH090lOcrFlUzM7AozqzazGjO7K8r6XDN7IqxfamblndbdHcqrzezynmKaWUWIsTbEzOmuDTMrN7ODZvZOeP2irz+MVFhRu5thuVlMSdHw0jPGF7GyrpHWtvaUtDeYPVW1iedWb+PUskL+9rTSQT9oYdSwXL5yfgUHW9qY//oGzeUlSdVjMjGzTOB+4EpgKnCtmU3tUu0WYLe7TwbuBeaFbacCc4BpwBXAA2aW2UPMecC97j4F2B1ix2wjWOfu08Prtl79BFLsrY92c8bE4qRffO8wfXwRBw+3sXbHvpS0N1j9ac127npmJZNHD+WaynEpmWsrFcqK8rlh5kR272/hpl8tY1949rxIf4vnyGQGUOPu6929BVgIzOpSZxbwWFh+GrjEIv/WzQIWunuzu28AakK8qDHDNheHGISYV/fQxqDRePAw1dubOCtcGE+F08cXAboI353lGxu4fcFbnFI6nOtnTCAr4+g6+ztp5FCunTGB1Vv28tXHqjR9vSRFPH81ZcCmTt/XhbKoddy9FWgESrrZNlZ5CbAnxOjaVqw2ACrM7G0ze9XMPh1tJ8zsVjOrMrOq+vr6OHa7/72zaQ/uUFmeumRSXlJAUUE279QqmUSzanMjN/9qOWXF+cy/6WxyszPT3aWkOHnscP7pmtN4c/0uvvnvb+u0p/S7eJJJtP/+uz7qLVad/irvro2twAR3PwP4DrDAzI4YKuXuD7t7pbtXjho1Kkqo5FuxsYEM++vRQiqYGaePK+LdOiWTrtbV72Pu/GUMz8/mt7ecQ8nQ3HR3Kam+cMY4fvD5qTwfTum1t+uJjdJ/4kkmdcD4Tt+PA7bEqmNmWUAh0NDNtrHKdwJFIUbXtqK2EU6h7QJw9xXAOuCEOPYr5VbU7ubkscMZmsSHKEVz5oRiqrc30XhAI3o6bN5zkBseWYoZPH7LDErD0ymPdjedX8E/XnoCT6+o44f/9T6uRwBLP4knmSwHpoRRVjlELqgv6lJnETA3LM8GXvLIb+kiYE4YiVUBTAGWxYoZtnk5xCDEfLa7NsxsVLigj5lNCm2sj/9HkBqtbe28XbuHsyam7hRXh5nHl+AOSzbsSnnbA1F9UzNffmQpTc2t/Obmc465aUe+eclkvnJ+OfNf38DPX6pJd3fkKNHjv8ju3mpmdwCLgUxgvruvNrN7gCp3XwQ8CjxuZjVEjkjmhG1Xm9mTwBqgFbjd3dsAosUMTd4JLDSzHwJvh9jEagO4ALjHzFqBNuA2d2/o+48kOT7Y1sSBlra0JJPp44vIy87gzXW7uHzacSlvfyBpPHCYG+cvY1vjIR6/ZcbHsysfS8yM//25qTQeOMw/v/AhRQXZ3DCzPN3dkkEurvMt7v4H4A9dyr7XafkQcE2MbX8E/CiemKF8PZHRXl3Lo7bh7r8Hft/jTqTZW7W7AdKSTHKyMji7fARvrDu2n8i350ALNzy6jJodTTwy92wqy0eku0tpk5FhzJt9GnsPtfK9RasZnp/NrOldx9WIxE+zBqfI8o27OW54HmVpOjd/3vEjmffcB9Q3NTNq2NF9obmrBUtr2d/cyvzXN7CjqZnrZ0xg8+6Dn5gh+FgQbX8/PWUk6+r38Y9PvEPVxsg1PdB09dJ7R9eA+gGqvd15PUzRka5bY2YeHxlFvWT9sXfdZF9zK4/+ZQP1Tc3ccO5EThp77J3aiiU7M4Mbzp1IaVE+v1v60YCdYl8GPiWTFFi5uZGG/S18Jo1zPZ1SOpxhuVm8se7YSiY7mg7xyJ/Xs2t/MzfOLOeEMQPnKYkDRV52JjefX8G44gIWLqvl7XBKVqQ3lExS4NUP6zGLnFJIl6zMDM6ZNII3j6HrJlv2HOTah5ew58Bh5sZ4+qBE5GVn8pXzy6kYOYSnV9SxcNmxdQpQEqdkkgKvVO/gtLLCtN8UN/P4kWzcdYDNx8AzLj7Ytpe/e+ANduxtZu55eupgPHKzMpl7XjlTxgzlrmdW8uvXN6S7SzKIKJkk2Z4DLbyzaQ8XDoDpzM8L103+/GF6ppNJlTdqdnLNg28C8ORtM6kYOSTNPRo8sjMz+PI5E7ls6hh+8J9ruPeFD3Vjo8RFySTJ/rx2J+0OF56QnilcOjvpuGFMGFHAH1ZtS3dXkubZdzYz91fLGFuUxzNfP+/j0UkSv6zMDO6//kxmnzWO+15cy39/6j1aWjWXl3RPySTJXv2wnsL8bKancD6uWMyMz502ltdrdrL7KHu2hbvz4Cvr+NbCdzhrYjFP3XbeMTNFSjJkZ2bw09mn8Y+XnsDv36rjxvlL2aUnNko3lEySqL3defXDej49ZWTKnl/Sk8+dOpa2dmfx6qPn6ORASyvfWvgO8577gM+fXspjN8+gMD873d0a9MyMb106hXu/dDpv1e7hqp+/zioNHZYYlEyS6K3a3dQ3NXPxSem/XtJhWulwJpYU8F8rt6a7K/1i4879fOH+N/jP97bwPy4/kfu+NJ3crKNzGvl0+cIZ43j6tpm0u/PFB9/gieW1uo4iR9Ad8En0VFUdBTmZA2o+LDPjc6eO5aHX1tOwv4URQ3LS3aU+WbC0lg+27uXJFZswjJtmllNckMPC5Zt63lh6FO1u+ZvOK+fJqk3c+fuVLFhay6zpZeRlZ+pueQF0ZJI0B1pa+X/vbeGzp45lSIqnnO/J506LnOp6fpCe6mppbef5Ndv4zZKPGDEkhzsumswU3YyYdMPysvnK+RX8zdQxvFfXyL++uJa125vS3S0ZIJRMkuSPK7exv6WNa84al+6uHGHq2OFUjBzCM29vTndXeu2DbXv5wgOv80p1PWdNLOYfLjie4kF6dDUYZZhx0Ymj+YcLJpGdmcGv3tjI/3z6XRoP6lk5xzolkyR5asUmJpYUMKNi4M1Ma2Zcf84Elm1oGDRTZ7S1R0ZrXfVvr7Ot8RDXnzOBL545juxM/Qqnw4SSIdxx8fi7W24AAA38SURBVGQuPGEUv39rM5fd+yp/WrM93d2SNNJfYhLU7jrAkvUNzD5zXNomduzJnBkTKMzP5sFX1qW7Kz2q2bGP2b94g3nPfcAlJ4/m+X+8gGmlhenu1jEvOzODy6cdx398/XyKC3L4/35TxS2/Xk7Njn3p7pqkgZJJEvxu6UeYwd8NwFNcHYbmZjH3vHKeX7N9wJ733t/cyo//+AFX3vca6+v3c9+c6Txw/Zlpn5ZGPunUcYUsuuNT3HnFSSzb0MDlP3uN//0fq3RfyjFGyaSfbdlzkF+/sZEvTC9L27NL4nXTeeXkZ2fy4KsD6+ikta2dJ5bXcvE/v8IvXl3HrOll/Ok7FzJretmAPdI71uVkZfC1zxzPK//jM1x/zgQWLKvlwp++wv0v17CvuTXd3ZMUUDLpZ5G5jOA7l52Q7q70aMSQHObMGM+z72zhwwFwdNLe7vxh5VauvO/P3Pn7lYwtzOf3X5vJP11z+jH3QK/BqmRoLvfMOoXF376AcyeV8NPF1Zz/45f4l+eraTjKZl2QT7Jj8eajyspKr6qq6ve41duauPK+17j5/Ar+199O7ff4vXkyYLxj/+ubmrniZ68xalgu/3H7+eRlp/6Gv+bWNr77f1fx6of1kSdBDs3lb6aOYVrpcB2JDHKbGg7w6of1rNm6l+xM4/pzJvLVCyYN+KN2ic7MVrh7ZbR1A+sGiEHM3fnRH95nSG4Wt180Od3diduoYbn89JrTuPnXVfzkuWq+9/n+T4KxbG08yBPLN/HbJbXs3NfMmOG5zDl7PKeUFZKhJHJUGD+igC+fO5Edew/x2tp6frvkI37z5kYuPGEUXzp7ApecPFoj8o4SSib95L4X1/Lah/X84PNTB919DxefNIa5Mycy//UNzKgYwRWnJO+O/cYDh3nxg+08+84WXltbjztcdOIoykuGMHn0UB2JHKVGD89j9lnj+bfrzmThslqerNrEbb9dwcihOXzxzHH83ZnjOGGM3v/BTMmkH/zXe1v52Z/W8sUzxzH3vPJ0d6dP7v7syby9aQ9f/90Kvve3U5l7Xnm//WFvazzE82u2sXj1Npaub6C13SktzOMbF03mmsrxjB9R0KtTeDJ4lRXl898uO5FvXTKF19bWs3DZJh75ywYeem095SUFXDbtOC6fNoYzxheTMUAmR5X46JpJgl58fzu3L3iLaaWFLPjqOUmdZDAZ10w629/cyrefeIcX1mznmrPG8e2/OaHX57bdnY92HWD5xgaqNu6m6qMG1tXvB2DSyCEff1icPq7oEx8WSibHrqZDh1mzdS9rtuxlff1+2twZmpvFJSePZkbFCGaUj9BR6wDR3TUTJZM+amlt5yfPfcAjf9nA1LHDeezmGUkfcZTsZAKREVU/fb6ah19bD8BnTx3LBVNGMq20kHEj8snNyiDDjKZDrew+0EJtwwHW1+9nXf0+1tfvY+32fewKo3byszOZMKKA8pFDOPm4YYwentenPsmx49DhNqq3N/H+1r1sbTxEfVPkXpXigmzOmjiCaaXDOfG4YZwwZhjlJQVk6XpLSiWcTMzsCuA+IBN4xN1/3GV9LvAb4CxgF/Ald98Y1t0N3AK0Ad9098XdxTSzCmAhMAJ4C7jB3Vv60kYsiSSTxgOHeWrFJn7z5kfUNhxg7syJ3P3Zk1MyCioVyaTD5j0H+fXrG1i4fBNNh3q+T6AwP5vJo4dy/KghTB9fTGV5Mcs2NOhCuvTZtTPG89GuAyzb2MDyDQ2sqN3Nxp37aQ8fWTmZGUwaNYRxxfmUFuVTVpRPWXE+YwvzGDEklxEFOQzPz9IRTT9KKJmYWSbwIfA3QB2wHLjW3dd0qvN14DR3v83M5gBfcPcvmdlU4N+BGUAp8Ceg4waMqDHN7EngGXdfaGa/AN519wd724a7t8Xap74mk40793PlfX/m4OE2ZpSP4GufOZ6LUvisklQmkw5t7c7GXftZtbmR+qZmlm1ooN2dvOxM8rMzKS7IYeSwXIbkZOqPVpLucFs79U3NbN97iO17D7GjqZk9Bw6zv6U16j89WRlG8ZAcRhTkMGJI5DUkN5MhuVkMzc1iSHgNzc1kSE4WOVkZkVdm5Gt2ZuSV+/GykZWZgRkYkXnuIl/BsMjXzstd66Tgb8TdaWlr51BLOwcPt3GgpZUDLW1sazzElsaD5Gdnck3l+D7FTnRo8Aygxt3Xh2ALgVnAmk51ZgE/CMtPAz+3yE9tFrDQ3ZuBDWZWE+IRLaaZvQ9cDFwX6jwW4j7YhzbejGPfemViSQFf/XQFl59y3FE1N1S8SaogJ4vPnDhwHvQlx57szAxKi/KjPpL50OE2dh9ooelQK/ubW9nf0hb52hz5MN2y5yA1O/aRmWHsb25lX0sr6TjL39t80t99PG1cYZ+TSXfiSSZlQOcnDtUB58Sq4+6tZtYIlITyJV22LQvL0WKWAHvcvTVK/b608TEzuxW4NXy7z8yqY+9y9/5bXzdM3EhgZzwVr09yRxIQ9z4MUIO9/zD492Gw9x/SuA8fAfaNPm8+MdaKeJJJtDzaNVfGqhOrPNpVs+7q96WNTxa4Pww8HKXuoGFmVbEOMQeLwb4Pg73/MPj3YbD3H46OfegqnqEQdUDnY6JxwJZYdcwsCygEGrrZNlb5TqAoxOjaVm/bEBGRFIknmSwHpphZhZnlAHOARV3qLALmhuXZwEseubK/CJhjZrlhlNYUYFmsmGGbl0MMQsxn+9iGiIikSI+nucL1iTuAxUSG8c5399Vmdg9Q5e6LgEeBx8PF7wYiyYFQ70kiF+tbgds7RllFixmavBNYaGY/BN4OselLG0ehQX2aLhjs+zDY+w+Dfx8Ge//h6NiHTzgmb1oUEZH+pdtHRUQkYUomIiKSMCWTQcLMrjCzajOrMbO7BkB/NprZSjN7x8yqQtkIM3vBzNaGr8Wh3MzsX0Pf3zOzMzvFmRvqrzWzuZ3Kzwrxa8K2Cd06bGbzzWyHma3qVJb0/sZqox/34Qdmtjm8D++Y2Wc7rbs79KfazC7vVB71dykMiFka+vpEGBxDGNzyRKi/1MzK+9j/8Wb2spm9b2arzexboXzQvA/d7MOgeR+Sxt31GuAvIoMU1gGTgBzgXWBqmvu0ERjZpewnwF1h+S5gXlj+LPBHIvcEnQssDeUjgPXha3FYLg7rlgEzwzZ/BK5MsL8XAGcCq1LZ31ht9OM+/AD471HqTg2/J7lARfj9yezudwl4EpgTln8BfC0sfx34RVieAzzRx/6PBc4My8OITKk0dTC9D93sw6B5H5L1SnsH9IrjTYr8cSzu9P3dwN1p7tNGjkwm1cDYsDwWqA7LDxGZe+0T9YBrgYc6lT8UysYCH3Qq/0S9BPpczic/iJPe31ht9OM+xPoQ+8TvCJGRkzNj/S6FD9+dQFbX37mObcNyVqhn/fB+PEtkfr5B9z5E2YdB+z7010unuQaHaFPaHDFlTIo58LyZrbDIVDUAY9x9K0D42jGRV6z+d1deF6W8v6Wiv7Ha6E93hNNA8zudvuntPsQ9lRHQMZVRn4VTNGcASxmk70OXfYBB+D70JyWTwSGuKWNS7Hx3PxO4ErjdzC7opm5vp8JJ9/4Opv4+CBwPTAe2Av8cyvtzH/p1/8xsKPB74Nvuvre7qjHaTfv7EGUfBt370N+UTAaHATdljLtvCV93AP+XyEzN281sLED4uiNU7+20OnVhuWt5f0tFf2O10S/cfbu7t7l7O/BL/jordyqmMuo1M8sm8iH8O3d/JhQPqvch2j4MtvchGZRMBod4prRJGTMbYmbDOpaBy4BVfHLKm65T4dwYRuecCzSGUw2LgcvMrDicFriMyPnhrUCTmZ0bRuPc2ClWf0pFf2O10S86PiCDLxB5HzraTfZURr3tqxGZyeJ9d/+XTqsGzfsQax8G0/uQNOm+aKNXfC8iI1s+JDIC5Ltp7sskIqNP3gVWd/SHyPnbF4G14euIUG7A/aHvK4HKTrFuBmrC6yudyiuJ/EGuA35OghcaiTxAbStwmMh/eLekor+x2ujHfXg89PE9Ih82YzvV/27oTzWdRsPF+l0K7+uysG9PAbmhPC98XxPWT+pj/z9F5LTMe8A74fXZwfQ+dLMPg+Z9SNZL06mIiEjCdJpLREQSpmQiIiIJUzIREZGEKZmIiEjClExERCRhSiYi/cDMyq3TbL5x1P+1mc3uuabI4KBkIiIiCVMyEek/WWb2WJjs72kzKzCz75nZcjNbZWYPhzuoPyFWHTN7xczmmdkyM/vQzD4dyjPN7J8s8tyO98zsG6H8LDN7NUy+ubjLXdkiSaVkItJ/TgQedvfTgL1Enj/xc3c/291PAfKBv42yXXd1stx9BvBt4Puh7FYiz8Y4I7T1uzBf1L8Bs939LGA+8KP+30WR6LJ6riIicdrk7q+H5d8C3wQ2mNn/BAqIPMxpNfCfXba7qJs6HZMhriDyLBOAS4k8JKkVwN0bzOwU4BTghXBgk0lk6hWRlFAyEek/XecmcuABInNKbTKzHxCZX+ljZpbXQ53m8LWNv/69WpS2DFjt7jMT3QmRvtBpLpH+M8HMOj7MrwX+EpZ3hudfRBu9lRdHna6eB27rmKbczEYQmURwVEf7ZpZtZtP6uB8ivaYjE5H+8z4w18weIjI77YNEnlG+kshjjpd33cDd95jZL7urE8UjwAnAe2Z2GPilu/88DDX+VzMrJPK3/TMip8xEkk6zBouISMJ0mktERBKmZCIiIglTMhERkYQpmYiISMKUTEREJGFKJiIikjAlExERSdj/D3/USAbxYIZXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# применим функцию get_info_column() к столбцу 'balance'\n",
    "get_info_column(churn, 'balance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3617 человек имеют баланс равный 0. В остальных случаях распределение выглядит нормальным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________\n",
      "\n",
      "Числовое описание данных столбца numofproducts\n",
      "\n",
      "Коэффициент корреляции Пирсона с целевым признаком: -0.05\n",
      "\n",
      "count    10000.000000\n",
      "mean         1.530200\n",
      "std          0.581654\n",
      "min          1.000000\n",
      "25%          1.000000\n",
      "50%          1.000000\n",
      "75%          2.000000\n",
      "max          4.000000\n",
      "Name: numofproducts, dtype: float64\n",
      "\n",
      "Наиболее частотные значения столбца\n",
      "\n",
      "1    5084\n",
      "2    4590\n",
      "3     266\n",
      "4      60\n",
      "Name: numofproducts, dtype: int64\n",
      "\n",
      "Наименее частотные значения столбца\n",
      "\n",
      "1    5084\n",
      "2    4590\n",
      "3     266\n",
      "4      60\n",
      "Name: numofproducts, dtype: int64\n",
      "\n",
      "\n",
      "Максимальные значения столбца\n",
      "\n",
      "numofproducts\n",
      "4      60\n",
      "3     266\n",
      "2    4590\n",
      "1    5084\n",
      "Name: numofproducts, dtype: int64\n",
      "\n",
      "Минимальные значения столбца\n",
      "\n",
      "numofproducts\n",
      "4      60\n",
      "3     266\n",
      "2    4590\n",
      "1    5084\n",
      "Name: numofproducts, dtype: int64\n",
      "\n",
      "Диаграмма размаха столбца numofproducts\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Гистограмма для столбца numofproducts\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEGCAYAAABM7t/CAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXSc1Zkm8OetTVJp36zFkizb2HgDLwibfQ2JExJIB0ggAwnJpH3SDZOQdJ/uyZyepDPnZGbOnKS708OcEBpoIAlb06w+EKBxgJh4QbblBa+ybCRZslZrX6vqnT/qK1sWVaoquZZbped3jg5Vqltfvb42jz7d7373iqqCiIjMZUt2AURENDMGNRGR4RjURESGY1ATERmOQU1EZDhHPA5aUlKitbW18Tg0EVFa2rVrV7eqlgZ7LS5BXVtbi/r6+ngcmogoLYnIJ6Fe49AHEZHhGNRERIZjUBMRGY5BTURkOAY1EZHhGNRERIZjUBMRGY5BTURkOAY1EZHh4nJnoume2dEc8rWvb6hJYCVEROHxjJqIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAR7fAiIicBDALwAvCoal08iyIionOi2YrrRlXtjlslREQU1JzcM5Fij/tQEsVPpGPUCuBtEdklIpviWRAREZ0v0jPqq1W1TUTmAXhHRA6r6gdTG1gBvgkAamp4BkVEFCsRnVGrapv1304ALwNYH6TNo6pap6p1paWlsa2SiGgOCxvUIpItIrmBxwA+C+BAvAsjIiK/SIY+ygC8LCKB9s+o6u/jWhUREZ0VNqhVtQnA6gTUQkREQfDORCIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiw0Uc1CJiF5E9IrI5ngUREdH5ojmj/j6AQ/EqhIiIgosoqEWkCsCtAB6LbzlERDRdpGfU/wTgbwD4QjUQkU0iUi8i9V1dXTEpjoiIIghqEfkigE5V3TVTO1V9VFXrVLWutLQ0ZgUSEc11kZxRXw3gNhE5CeA5ADeJyG/jWhUREZ0VNqhV9UeqWqWqtQDuBrBFVe+Ne2VERASA86iJiIzniKaxqr4H4L24VEJEREHxjJqIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjIcg5qIyHAMaiIiwzGoiYgMx6AmIjJc2KAWkUwR2Skie0XkYxH5aSIKIyIiP0cEbcYB3KSqQyLiBLBVRN5U1e1xro2IiBBBUKuqAhiynjqtL41nUUREdE5EY9QiYheRBgCdAN5R1R1B2mwSkXoRqe/q6op1nUREc1ZEQa2qXlVdA6AKwHoRWRWkzaOqWqeqdaWlpbGuk4hozopq1oeq9gF4D8DGuFRDRESfEsmsj1IRKbAeZwH4DIDD8S6MiIj8Ipn1UQHgKRGxwx/sL6jq5viWRUREAZHM+tgHYG0CaiEioiB4ZyIRkeEY1EREhmNQExEZjkFNRGQ4BjURkeEY1EREhmNQExEZjkFNRGQ4BjURkeEY1EREhmNQExEZjkFNRGQ4BjURkeEY1EREhmNQExEZjkFNRGQ4BjURkeEY1EREhmNQExEZjkFNRGQ4BjURkeEY1EREhmNQExEZjkFNRGQ4BjURkeEY1EREhmNQExEZjkFNRGQ4R7gGIlIN4GkA5QB8AB5V1V/Gu7BYU1U8vKUR7gwHJjw+5Gc5k10SEVFEwgY1AA+Av1LV3SKSC2CXiLyjqgfjXFtMbW/qxS/eOXr2+ZrqAny1rjqJFRERRSbs0IeqtqvqbuvxIIBDAObHu7BY+/UHx1GS48LvH7oWly0oRENLHzoGxpJdFhFRWFGNUYtILYC1AHYEeW2TiNSLSH1XV1dsqouRw6cH8N6RLtx/VS2WlefhcyvLYRNgT3NfsksjIgor4qAWkRwA/w7gIVUdmP66qj6qqnWqWldaWhrLGi/Yox80we2y494rFgAAcjIcWDIvF3tb++BTTXJ1REQziyioRcQJf0j/TlVfim9JsdXWN4rXGtrwtcurUeB2nf3+mpoC9I9O4kT3cBKrIyIKL2xQi4gAeBzAIVX9h/iXFFub97XB41N8++qF531/eXkeMhw2NHD4g4gMF8kZ9dUA7gNwk4g0WF9fiHNdMbPzRC8WlWSjush93vddDhtWVubjQFs/Jjy+JFWX+sYmvfjF20fwwVGzrksQpZOw0/NUdSsASUAtMefzKT46eQYbV5YHfX1NdQF2N59BY+cQVlTmJbi61HfgVD9+8HwDjnUOAQBqityoLclOclVE6Set70w82jmI/tFJXL6wKOjrC4rdsIuguZfj1NFq6xvFHb/6E/pHJ/HIvetQkOXEKw2n4PXx4ixRrKV1UH90ohcAsL42eFA77TZUFGSiuXc0kWWlhdf2tmHc48Nzm67AxlUV+NLqSnQOjuPDxu5kl0aUdtI6qHeePIPyvExUF2WFbFNd5MapvhGeCUbp1YY2rKkuwKLSHADA8oo8rKjIw7uHOzA4Npnk6ojSS9oGtapi54keXL6wCP6JK8HVFLox6VXepRiFox2DONQ+gC+vqTzv+zcvn4dJr+LI6cEkVUaUntI2qFt6R9ExMI71tYUztquxZoM0944koqy08GrDKdgEuPXS84O6PC8TeZkOHLUuLhJRbKRtUO88aY1PLyyesV2B24mcDAdaGNQRUVW82tCGqy8qQWluxnmviQiWluWisXOQQ0lEMZS+QX2iB/lZTiyZlzNjOxFBdZGbZ9QR2t3ch9Yzo/jymuDrci0py8XYpA+tZ9ifRLGStkG9p7kPly0ohM0Wfgp4TZEbPcMTGBn3JKCy1Pb2x6fhstvw2ZVlQV+/qDQHNvGPYxNRbKRlUI9NenG8awirIryJJTArpIVngWHtae7Diso85GYG33ghy2VHdaEbRzs4Tk0UK2kZ1EdOD8KniPhuw6oCNwTgfOowPF4f9p/qx5rqghnbLSnLxam+UQzxNxSimEjLoD7Y7l+FdXlFZEHtcthQnp/JcdUwjnYMYXTSi7U1Mwf10jL/dYHGTg5/EMVCWgb1ofYB5GQ4UF3oDt/YUpmfhfZ+zqWeSUOLf6XB1VUzB3VlQRbcLjuOd/LWfKJYSMugPtg2gOUVuRFdSAyoKMjE0LgHnYMM61AaWs6g0O3EguKZfwDaRFBd6OaYP1GMpF1Q+3yKQ+0DWBHhsEdAeX4mAH/IU3ANLX1YXV0w452eAVVFWegaHMfYpDcBlRGlt7QL6ubeEQxPeKNetrQizz/zIzC+TecbGvfgWOdQ2AuJAdWFbiiAU328QEt0odIuqKO9kBiQ5bKj0O3EoXZeAAtmX2sfVBFxUFcV+n/wtfJGIqILlnZBfah9AHab/1bmaFXkZ+FgW38cqkp9kV5IDHC7HCjOdqHlDM+oiS5U2gX1wbYBLC7NRqbTHvV7y/Mz0dQ9jJEJzv+drqG5D7XFbhRmu8I3tlQXuTnlkSgG0i+oZ3EhMaAyPxOq4DKd06gqGlr6Ih72CKgqzMLAmAenOe2R6IKkVVD3Dk+gvX9s1vsfVuT7x1U5Tn2+0wNj6BwcjzqoA/PYG1rOxKMsojkjrYL60CwvJAYUuJ3IzXTgYDvHqadqaLbGp6MM6vL8TNhF0NDC/iS6EAzqKUQEKyryOJd6moaWPrjstqh/UwnsSbnXuhBJRLOTVkF9sG0AZXkZKMnJCN84hOUVeTh8ehA+Lnx/1p6WPiyvzEOGI/oLtFWFWdjX2sf+JLoA6RXUF3AhMWBFZR5GJrz4hPN/AVgr5rX2Y22Uwx4BVQVuDE940dTNdT+IZittgnrc40Vj59Cshz0CAkHP4Q+/Y53+FfOivZAYUGnd+LL/FIc/iGYrbYL6WMcQPD6d9YyPgCVlOXDYhBcULWdvdJllUJfmZCDLace+VvYn0WylTVAHbh2/0KGPDIcdF83L4Rm1paG5DwVuJ2rDrJgXit0mWFGZhwOnGNREs5U+Qd02ALfLjgXF2Rd8rBUVeVycydLQ0ofVVZGtmBfKJfPzceDUAHcmJ5qlsEEtIk+ISKeIHEhEQbN1sH0Ay8pzYY9iDepQVlTmoWNgHD1D4zGoLHUNjXtwtHNw1uPTAZfMz8eotY8lEUUvkjPqJwFsjHMdF0TVvwb1hV5IDAgMn8z1OxT3t/b7V8wLs/VWOJdW5Z89HhFFL2xQq+oHAHoTUMustZ4ZxeCY54IvJAYEAn+uX1CMdsW8UBaV5sDtsmM/x6mJZiVmY9QisklE6kWkvqurK1aHjUisLiQGFGa7UJGfOecvKDa0nMGCYjeKolgxLxi7TbCyMo9BTTRLMQtqVX1UVetUta60tDRWh43IwbYB2ARYVh6boAb8oT/Xhz5ms2JeKJfML8DHbf3weH0xOR7RXJIWsz72n+rHotIcZLmiv8U5lBWVeWjsGpqze/6d7h9Dx0D0K+aFcklVHsYmfWjkBUWiqKV8UKsq9rX2XfA46nTLK/Lg9SmOdczNYAksTRrLM2oA2MeV9IiiFsn0vGcBbANwsYi0ish/jn9ZkTvVN4ruoQmsqc6P6XFXzPELinta+uC0S8xm0iwqyUZepgN7uDY1UdQc4Rqo6j2JKGS2Arcmz/YW51BqitzIzXBgX2s/vnZ5TA+dEhqa+7CiIm9WW5oFY7MJ1tYUYtcnDGqiaKX80Mdea63kWF5IBPzBsqamALub595iQl6fYv+p/pgNewRctqAQxzqHMDA2GdPjEqW71A/qVv9ayS5H7P8oa2sKceT0AIbH59Zmt8c6BzEy4b3gG12mW1dTCNVzO8YQUWRSOqi9PsX+1n6srort+HTA2poC+NT/w2AuCQTpmurCmB53dXU+RMDhD6IopXRQN3UNYXjCG/MZHwGBxfL3zLEzwIaWPuRnzX7FvFByM524uCwXu5sZ1ETRSOmgvtC1ksMpcLuwqDQbe+ZYsOw82Yt1NRe2Yl4o6xYUoqGZW3MRRSOlg3pvax9yMxxYVHLhS5uGsq6mEHua+6A6N4KlY2AMTV3DuHJxcVyOv66mEIPjHhzrnJvz04lmI6WDel9rPy6pyoctBkubhrK2pgA9wxNoniN7KG5v6gEAXLmoJC7Hv2yBf9ybwx9EkUvZoB6Z8OBQ+0Dchj0C1tX4g2WujFNvO96DvExHzFYinK7WWuSJFxSJIpeyQb3jRC8mvYqr4vQresDSslxku+xzZpx6W1MP1i8sjskGDMGICC5bUIjtTT1zZjiJ6EKlbFBvPdYNl8OGy2uL4vo5dptgdXUBds2BoG7rG8UnPSNxG58OuG5pKVrPjKKpeziun0OULlI6qNfXFsXsFueZrF9YhI/bBtJ+a65txwPj0/EN6huW+pfBff9IYtctJ0pVKRnUnQNjONIxiKsvis8Fr+luWjYPqsAHx9I7WLY39aDA7cSy8ty4fk51kRuLSrPx3tH07k+iWEnJoN7a2A0AuHZJYoJ6VWU+SnIysOVwegfLtqYebFhYFNdZNAHXLy3FjqaeObveN1E0UjaoC93OmG29FY7NJrjh4lK8f6QzbXcoaeoaQuuZUVy1ODE//G64eB7GPT5ss6YDElFoKRfUqoqtx7px1UUlCTnzC7hp2TwMjHnSdjW9zfvaIQJ8bmV5Qj5vw8IiZDhsHKcmikDKBfWxziF0Do7j2gSNTwdcs6QEDptgy+HOhH5uIqgqXtvbhstri1Cen5mQz8x02nHl4mK8z3FqorBSLqjf/vg0AODapYndQDcv04nLa4vw3pH0C+ojHYNo7BzCly6tSOjnXr+0FCe6h3Gc+ygSzSilgtrnU7xQ34orFhVhfkFWwj//pmXzcPj0IE71jSb8s+Np89522AT4/CWJDeovXFIBu03wQn1LQj+XKNWkVFDvONGL5t4RfO3y6qR8/i0rygAAL+1qTcrnx4Oq4vV9bbj6ohKU5GQk9LPL8jLxmeXz8GJ9KyY86XmRligWUiqo/62+BbkZDmxcmdgzv4Dakmxct7QUv9n+CSbTZPbH/lP9+KRnBF9M8LBHwD3ra9AzPIG3D55OyucTpYKwm9uaYmBsEm8caMcd66qQ5Yr/3YihfOuqWnzryY/w5oHTuG11ZdLqiJVndjTDZbclbLbHdNctKcX8giw8u7MZX7w09fszVp7Z0Tzj61/fUJOgSsgEKXNG/freNoxN+vDVuuQMewRcv7QUtcVuPPnhiaTWEQsnu4fxb7tacc/6ahS4XUmpwWYT3LO+Gh829uAk1/4gCiolgnrS68O/fngSy8pzcWmc9keMlM0m+OZVtdjd3Ie9Lak9p/qf3z0Gh03wwI0XJbWOu+qqYbcJHtvalNQ6iEyVEkH9+NYTaOwcwl9/9uK4bA8VrTsvq0K2y46H/9CYskt1NnYO4pWGU/jGlQswLy8xc6dDKcvLxL0bavC7Hc1nt1cjonOMD+rWMyP45X8cwy0ryvAZa9ZFsuVmOvHgTUvwzsEOvLznVLLLiZqq4udvHUWm047vXr842eUAAP76cxejLDcTP3ppf9pcqCWKFeOD+qevHwQA/P1tK5Ncyfk2XbcI62uL8ONXP0ZLim3T9fjWE/j9x6fxF9cvRnGCp+SFkpvpxN/fthKH2gfwxNbUH/8niiVjZ32oKv7PW0fwzsEO/NfPL0vKDS4zsdsEv/jqanz+l3/ED19owNPf3pDU2SiReuvj0/jZG4fw+VXlSR+bnm7jqnLcsqIMP3/7CKoK3bg1SVMGE2nS68O+1n5sb+pB/clenB4YR/fQOIbHPXA5bMhy2lGSk4F5eRlYUJSNBcVuOO3Gn19RjBkZ1F6f4u9eOYBndzbj6xtq8OfXLkp2SUFVF7nxsz9bhYeeb8Adv/oTfn3fZagucie7rKD8N7a0429f3IdLqwrwj19bk9BFrSL187tW4ztPfYQHn92N3pFVuO+KBckuKaa8PsXBtgFsa+rGn4734KMTvRie8C/1urQsBzVFbqyuyseJ7mFMen0YHvfiVN8oDpzqhwJw2AS1JdkYHJvENUtKsLw8z8i/R4qtiIJaRDYC+CUAO4DHVPV/x6MYn0/x7uFOPLzlGPa29uOBGxcbcwExlNvXzEdelhPff3YPvvTwVvzwlqX4s7XzkZvpTHZpZzV2DuF/vnEIWw53YnVVPh77Rl1CdsaZjfwsJ57+9gY88Mxu/PdXDuD9I5144MaLsNbaZDjVTHp9ONoxiJ0nerHteA+2N/VgYMwDAFhcmo2vrKvCVYuLsWFRMYqyz02RnD6PetzjxcnuYTR2DuFY5xD+15uHgTeBArcTGxYW4YpFxbhiUTEuLsud08E90/zzVJ57LuFmLYiIHcBRALcAaAXwEYB7VPVgqPfU1dVpfX19VIUMjE3iq49sw+HTg6guysJDNy/FHZdVRXWMSMXjL/Nk9zB+8EID9jT3Ictpx83L5+GS+flYVpGHebkZKMp2we2yw2m3wWm3xXTz2AmPD6MTXoxMejA64cWZkUmc7B7Gsc4hbDncgaMdQ3C77Pirz16M+6+qjcvGtbHu00mvD7967zge33oC/aOTWFmZh8tri7C2pgCVBVmYl5uBnAwHXA6b/8tui/sPdFWFTwGPzwevT+HxKcYnfegfnUDfyCTOjEyib2QCHQNjONE9guNdQzjUPoBx6/b4miI3rlpcjCsXF+PKRcUzzrYJd8PLzcvn4cPGbmxv6sG2ph609PrXn8nLdGBpWS6WlOWiMj8TJbkZKMnJQEmOCyU5Gchw2pBht5/tt3htYhyJQPaoAmo917PPFYFomvT6MDrpxeiEF6OTXoxMeDE24f/vyKQXQ2MeDI97MDjuwa6TvRj3+DDm8WF80otxjw/jHi9UgaJsF2wisNv8X5lOG9wuB7JcdriddmRnnHuc5bLD7XLA7bJbX9Zr1leWy44Mhx2TXh/GPT6cGZ5A19A4xid92LhqdjePicguVa0L9lokZ9TrATSqapN1sOcA3A4gZFDPRl6mE+sWFGLTdYtw2+pKOFJsHK62JBsv/+XV2Nfah99tb8Yfj3Vh8772kO1tAjjsNkz93yToj0yd/vT8b/jU/+t0MHaboG5BIX7ypRW49ZKKpE/Di4bTbsP3bl6Cb1+zEM/uaMa7hzvw3EfNePJPJ0O+xx/WkX9GpG3V6mNPiH4OpjwvEwtLsnHfFQtwaXUB1tUUoKowdsNiZXmZ+Mq6Knxlnf9kpvXMCHY09WJX8xkc6xjEmwfa0TcyGfY4NsHZH3CB7hABJPDs/P+c7bPA64HnU8NVAUDP/VsNFsTx4nLYkOmwweWwI9NpQ4bDhuyMDNgEmF+QBZ/qeT9kOwfHMDJuhf6EB6OTXkx6Z19gUbZr1kE9k0jOqO8EsFFVv2M9vw/ABlV9cFq7TQA2WU8vBnAk5tXGTgmA7mQXEYFUqRNInVpZZ+ylSq2m17lAVYOu3xzJGXWw845PpbuqPgrg0SgLSwoRqQ/1K4ZJUqVOIHVqZZ2xlyq1pkqdwUQyvtAKYOoCG1UA2uJTDhERTRdJUH8EYImILBQRF4C7AbwW37KIiCgg7NCHqnpE5EEAb8E/Pe8JVf047pXFV0oM0SB16gRSp1bWGXupUmuq1PkpYS8mEhFRcqXWHDgiojmIQU1EZLi0DWoReUJEOkXkQIjXRUT+WUQaRWSfiKxLdI1WHeHqvEFE+kWkwfr6caJrtOqoFpE/iMghEflYRL4fpI0pfRpJrUnvVxHJFJGdIrLXqvOnQdpkiMjzVp/uEJHaRNdp1RFJrfeLSNeUPv1OMmq1arGLyB4R2RzkNSP6NCqqmpZfAK4DsA7AgRCvfwHAm/DPE78CwA5D67wBwGYD+rMCwDrrcS78ywqsMLRPI6k16f1q9VOO9dgJYAeAK6a1+UsAj1iP7wbwvMG13g/g4WT26ZRafgjgmWB/x6b0aTRfaXtGraofAOidocntAJ5Wv+0ACkQk4etqRlCnEVS1XVV3W48HARwCMH9aM1P6NJJak87qpyHrqdP6mn51/3YAT1mPXwRwsyRhlbIIazWCiFQBuBXAYyGaGNGn0UjboI7AfAAtU563wsD/mS1XWr9yvikiSd9BwfpVcS38Z1VTGdenM9QKGNCv1q/oDQA6AbyjqiH7VFU9APoBFCe2Sr8IagWAO6xhrxdFJFk7Uf8TgL8BEGqrIGP6NFJzOagjujXeALvhXwNgNYD/C+CVZBYjIjkA/h3AQ6o6MP3lIG9JWp+GqdWIflVVr6qugf+O3/UismpaE2P6NIJaXwdQq6qXAvgPnDtrTRgR+SKATlXdNVOzIN8z8f/9s+ZyUKfErfGqOhD4lVNV3wDgFJGSZNQiIk74g+93qvpSkCbG9Gm4Wk3qV6uGPgDvAdg47aWzfSoiDgD5SPJQWahaVbVHVcetp/8C4LIElwYAVwO4TUROAngOwE0i8ttpbYzr03DmclC/BuAb1kyFKwD0q2rodUmTRETKA+NnIrIe/r+zniTUIQAeB3BIVf8hRDMj+jSSWk3oVxEpFZEC63EWgM8AODyt2WsAvmk9vhPAFrWugiVSJLVOux5xG/zXBhJKVX+kqlWqWgv/hcItqnrvtGZG9Gk0jNyKKxZE5Fn4r+yXiEgrgJ/AfwEEqvoIgDfgn6XQCGAEwLcMrfNOAH8hIh4AowDuTtI/qqsB3AdgvzVOCQD/DUDNlFqN6FNEVqsJ/VoB4Cnxb85hA/CCqm4Wkf8BoF5VX4P/B85vRKQR/rO+uxNcYzS1fk9EbgPgsWq9P0m1foqhfRox3kJORGS4uTz0QUSUEhjURESGY1ATERmOQU1EZDgGNRGR4RjUlDZEZJm1atseEVkc42PXSogVDiN47w0iclUs66G5hUFN6eTLAF5V1bWqejySN1jzguPtBgAMapo1BjXFjXUWekhE/sVaw/htEckSkfdEpM5qU2Ld7htYz/gVEXldRE6IyIMi8kPrDHm7iBRZ7dZYz/eJyMsiUigiXwDwEIDviH8t6loROSwiT01ZJMhtvf+kiPxYRLYCuCvY8ax2l1mLNm0D8MCUP9f9IvLwlOebReQG6/FGEdltve9d8S8K9V0AP7DO9q8VkbtE5IDV5oN4/z1Q6mNQU7wtAfD/VHUlgD4Ad4RpvwrA1wGsB/AzACOquhbANgDfsNo8DeBvrcV/9gP4ibVexyMA/lFVb7TaXQzgUavdAPzrEAeMqeo1qvpcsONZbf4VwPdU9cpI/qAiUgr/Ghd3WIs93aWqJ6fUtUZV/wjgxwA+Z7W5LZJj09zGoKZ4O6Gqgdu4dwGoDdP+D6o6qKpd8C8/+br1/f0AakUkH0CBqr5vff8p+DdfCKZFVT+0Hv8WwDVTXnseAEIdL8j3fxOmbsC/WcIHqnoCAFQ11EI/HwJ4UkT+HEAihl4oxTGoKd7Gpzz2wr++jAfn/u1lztDeN+W5D9GvTTN9fYSpz4fDvFeCvD9gav3AuT/DTO85V4TqdwH8HfwruDWIiNFrIVPyMagpGU7i3BKYd0bzRlXtB3BGRK61vnUfgPdDNK8RkcCwxT0AtkZ6PGspz34RCZyF/6dp9a8REZv4F8dfb31/G4DrRWQhAATG1AEMwr8lGKzvL1bVHar6YwDdOH9pWKJPSdvV88hoPwfwgojcB2DLLN7/TQCPWBcHmxB6lb5DAL4pIr8GcAzAr6I83rcAPCEiIwDemtL+QwAn4B+OOQD/JgRQ1S4R2QTgJRGxwb8Tyi3wD9+8KCK3A/gv8F9YXAL/Gfi7APZG+eenOYar51FasmZbbFbV6buQEKUcDn0QERmOZ9RERIbjGTURkeEY1EREhmNQExEZjkFNRC1BAaIAAAANSURBVGQ4BjURkeH+P1fFaUQNaBPgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#применим функцию get_info_column() к столбцу 'numofproducts'\n",
    "get_info_column(churn, 'numofproducts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что в данном столбце всего 4 уникальных значения. То есть 4 категории."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________\n",
      "\n",
      "Числовое описание данных столбца hascrcard\n",
      "\n",
      "Коэффициент корреляции Пирсона с целевым признаком: -0.01\n",
      "\n",
      "count    10000.00000\n",
      "mean         0.70550\n",
      "std          0.45584\n",
      "min          0.00000\n",
      "25%          0.00000\n",
      "50%          1.00000\n",
      "75%          1.00000\n",
      "max          1.00000\n",
      "Name: hascrcard, dtype: float64\n",
      "\n",
      "\n",
      "Распределение данных столбца hascrcard\n",
      "\n",
      "1    7055\n",
      "0    2945\n",
      "Name: hascrcard, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#применим функцию get_info_column() к столбцу 'hascrcard'\n",
    "get_info_column(churn, 'hascrcard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это тоже категориальный признак - имеется ли кредитная карта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________________\n",
      "\n",
      "Числовое описание данных столбца isactivemember\n",
      "\n",
      "Коэффициент корреляции Пирсона с целевым признаком: -0.16\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.515100\n",
      "std          0.499797\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: isactivemember, dtype: float64\n",
      "\n",
      "\n",
      "Распределение данных столбца isactivemember\n",
      "\n",
      "1    5151\n",
      "0    4849\n",
      "Name: isactivemember, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#применим функцию get_info_column() к столбцу 'isactivemember'\n",
    "get_info_column(churn, 'isactivemember')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично признаку владения кредитной картой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________\n",
      "\n",
      "Числовое описание данных столбца estimatedsalary\n",
      "\n",
      "Коэффициент корреляции Пирсона с целевым признаком: 0.01\n",
      "\n",
      "count     10000.000000\n",
      "mean     100090.239881\n",
      "std       57510.492818\n",
      "min          11.580000\n",
      "25%       51002.110000\n",
      "50%      100193.915000\n",
      "75%      149388.247500\n",
      "max      199992.480000\n",
      "Name: estimatedsalary, dtype: float64\n",
      "\n",
      "Наиболее частотные значения столбца\n",
      "\n",
      "24924.92     2\n",
      "109029.72    1\n",
      "182025.95    1\n",
      "82820.85     1\n",
      "30314.04     1\n",
      "Name: estimatedsalary, dtype: int64\n",
      "\n",
      "Наименее частотные значения столбца\n",
      "\n",
      "158302.59    1\n",
      "171037.63    1\n",
      "43036.60     1\n",
      "55034.02     1\n",
      "104181.78    1\n",
      "Name: estimatedsalary, dtype: int64\n",
      "\n",
      "\n",
      "Максимальные значения столбца\n",
      "\n",
      "estimatedsalary\n",
      "199992.48    1\n",
      "199970.74    1\n",
      "199953.33    1\n",
      "199929.17    1\n",
      "199909.32    1\n",
      "Name: estimatedsalary, dtype: int64\n",
      "\n",
      "Минимальные значения столбца\n",
      "\n",
      "estimatedsalary\n",
      "106.67    1\n",
      "96.27     1\n",
      "91.75     1\n",
      "90.07     1\n",
      "11.58     1\n",
      "Name: estimatedsalary, dtype: int64\n",
      "\n",
      "Диаграмма размаха столбца estimatedsalary\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Гистограмма для столбца estimatedsalary\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#применим функцию get_info_column() к столбцу 'estimatedsalary'\n",
    "get_info_column(churn, 'estimatedsalary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_______________________________________\n",
      "\n",
      "Числовое описание данных столбца exited\n",
      "\n",
      "Коэффициент корреляции Пирсона с целевым признаком: 1.00\n",
      "\n",
      "count    10000.000000\n",
      "mean         0.203700\n",
      "std          0.402769\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          0.000000\n",
      "75%          0.000000\n",
      "max          1.000000\n",
      "Name: exited, dtype: float64\n",
      "\n",
      "\n",
      "Распределение данных столбца exited\n",
      "\n",
      "0    7963\n",
      "1    2037\n",
      "Name: exited, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWIAAAFUCAYAAAAefzbKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd5xcdb3/8dfZkk0lgYSEhHYQEwgJTQSkg0DCZRQQ0AsWiqjAj2ZBOXpRR67gkStXugh6SegCUgKHovTemxQhlEOAhBZg0je7M+f3x5mQhLQtM/M55f18POax7mYz+17Dvvc733O+368TRREiImKnyTqAiEjeqYhFRIypiEVEjKmIRUSMqYhFRIypiEVEjKmIRUSMqYhFRIypiEVEjKmIRUSMqYhFRIypiEVEjKmIRUSMqYhFRIypiEVEjKmIRUSMqYhFRIypiEVEjKmIRUSMqYhFRIypiEVEjKmIRUSMqYhFRIypiEVEjKmIRUSMqYhFRIypiEVEjKmIRUSMqYhFRIypiEVEjKmIRUSMqYhFRIypiEVEjKmIRUSMqYhFRIypiEVEjKmIRUSMqYhFRIy1WAcQ6QrXC9qAocAan3l89mMDiAcYix4OEAEdn3nMAz4E3l/eI/QLCxv0rYngRFFknUEE+LRsPw9sBIxZ4u0YYFiD45SIS3kG8DLwAvAi8GLoF95pcBbJOBWxmHC9YGNgB2AzFhfu+qRjuqxEtZSXePxLBS09pSKWuquOdL9IXLw7AtsTTylkTQjcC9wD3Bv6hTdM00hqqIil5lwvGALsTFy6OwBbAW2moWy8RVzM9xIX81TjPJJQKmKpCdcLRgNfrT52RBeCl2c6cBdwI3Br6BfmGueRhFARS4+5XrAVcCDwNeJ5Xum6BcA/geuBKaFfmGmcRwypiKVbXC/4IvAN4ADgc8ZxsqKTuJSvAq4P/cJs4zzSYCpiWSXXCwYD3wGOBMYbx8m6BcAtwCQgCP1CxTaONIKKWFbI9YJtiMv3IKC/cZw8CoE/AX/V1EW2qYhlKa4XDAK+RVzAWxjHkdgC4mmLc0O/8KR1GKk9FbEA4HrBeOA44JvAQOM4smKPAucCV2sZdnaoiHPO9YLNgF8B+xPvyyDp8D7wZ+CPoV/42DqM9I6KOKdcL9gc+DWwHyrgNCsB/0tcyLrbIqVUxDnjesEWxAW8LyrgLJkJnE48jzzPOox0j4o4J1wv2JK4gPdBBZxl7wKnAX/WHHJ6qIgzzvWC9YAziFfASX5MA34LXBz6hU7rMLJyKuKMqu54diLwC3QPcJ5NBY4J/cI/rYPIiqmIM8j1gr2As4HR1lkkMa4EfhT6hfesg8iyVMQZ4nrB+sCZxHdCiHzWJ8SvkP6spdPJoiLOgOo0xE+Jf8j6GceR5HsUODL0C89aB5GYijjlXC/YDbiQ+Kw3ka4qA2cBv9K+yPZUxCnlekFf4HfACeh2NOm5t4CjQr9wi3WQPFMRp5DrBV8ALgU2sc4imXEecGLoFxZYB8kjFXGKuF7QBJwE/AZoNY4j2fMC8M3QLzxnHSRvVMQp4XrBSOAy4MvWWSTT2oGTQr9wlnWQPFERp4DrBXsTn9iwpnEUyY/rgO+GfqFkHSQPVMQJVp2KOA34GbogJ433OvANbUZffyrihKqelHEF8BXrLJJrC4ETQr9wgXWQLFMRJ1B1hdxNwKbWWUSqzgR+ohV59aEiThjXC3YArkfzwZI8NxLfVaH9jmusyTqALOZ6wSHAnaiEJZn2Be51vWAt6yBZoxFxAixxUe4k6ywiXTANKIR+4XnrIFmhIjbmesEA4HLi0YZIWswCDtQ+x7WhqQlDrhesBvwTlbCkz2rALa4XfM86SBaoiI24XjAEuAPYzjqLSA+1ABe5XvBL6yBppyI24HrBUOAuYGvrLCI1cIrrBSdbh0gzzRE3mOsFaxKPhDezziJSY/8V+oXTrEOkkUbEDeR6wQjgblTCkk2nul7wc+sQaaQRcYNUd0+7C9jYOotInXmhX/i9dYg00Yi4AVwvWAe4F5Ww5IPvesFPrUOkiYoYcBxnL8dxXnYc51XHcbxaPnf1wtyd6Gh7yZfTXS/4iXWItMj91ITjOM3AK8CewNvA48DBURS92Nvnrp4rdwewQ2+fSySljgv9wrnWIZJOI2LYBng1iqLXoyhaCFxFDRZYuF7gAJegEpZ8O8v1Am3lugoqYlib+CTbRd6ufqy3/gf4eg2eRyTNmoArXS/Y3DpIkqmIl3/yRa/ma1wvOBbQ/JhIbCBwk3ZtWzEVcTwCXneJ99cBpvf0yVwv2AfQwYsiS1sXmOJ6QT/rIEmkIo4vzo12HGcDx3H6AAcBU3ryRK4XbA1cif5/FVmerYFLq9dPZAm5L4woijqBY4HbgZeAq6MoeqG7z+N6gQvcDPSvaUCRbDmAeO9tWULub1+rheptag8BW1pnEUmJw0O/MMk6RFLkfkRcI2ejEhbpjgtdL9jWOkRSaETcS64XfIf4fmER6Z4Q2DL0C59YB7GmEXEvuF4wDrjAOodISrnAX61DJIGKuIeqt+H8DV2cE+mN/av33eeairjn/giMsw4hkgF/yPvKO80R94DrBV8DrrPOIZIhzwNbh35hgXUQCxoRd1N1b+G/WOcQyZjxgG8dwoqKuPsuAtawDiGSQce7XrCHdQgLmproBtcLDiJewiwi9fEOMC70CyXrII2kEXEXuV6wOnCmdQ6RjFsbONU6RKOpiLvu98AI6xAiOXC06wVfsA7RSJqa6ALXC3YE7mP5exeLSO09BmwX+oWKdZBG0Ih4FVwv6ANciEpYpJG2AX5gHaJRVMSrdhIw1jqESA6d5nrBmtYhGkFFvBKuF4wB/ss6h0hOrU589mPmqYhX7k9Am3UIkRw7xPWCnaxD1Jsu1q2A6wV7Abda5xARnifeLrPTOki9aES8HNUztXJ3L6NIQo0HjrQOUU8q4uU7EMjVfYwiCfeL6pFkmaQi/gzXC5qBU6xziMhSRgFHWYeoFxXxsg4FNrYOISLL8FwvyORBDCriJbhe0Ab82jqHiCzXCOD/WYeoBxXx0o4C1rMOISIr9DPXCwZYh6g1FXGV6wUDgV9Y5xCRlVoTOM46RK2piBc7DhhuHUJEVulE1wsGWYeoJRUx4HpBKxn8LSuSUUOB461D1JKKOPZ1YKR1CBHpsh9n6Q4KFXEsU79dRXJgDeAg6xC1kvsidr1gG2Bb6xwi0m1HWweoldwXMRoNi6TVF10v2No6RC20WAew5HrBCOL54cTrmPk2H0z5/afvd37yLkN2/DZ919+UmbefR7RwAS2DhzPsqz+lqW3ZqbPKgjnMvPVsFn44DYBhe59A29pj+fiei5n/+pP0Gb4Bw77yEwDmPH8XlQWzWe2L+zbmmxPpuaOBx61D9FbeR8RHAX2sQ3RF69B1GHX4OYw6/BxGHnomTmsb/cdsx8xbz2H1XQ5j1BHn0X/Mdsx69O/L/fsf3XkhfT+3FWt//wJGffccWoeuS6V9Lu3vvMSo755LFFVY+EFIpaOduc/fwaAtCw3+DkV65KDqCeupltsirt6ylspNRBa8+SytQ0bSMng4HR+9Tdu64wHo627JvFceWubzK+3zWPDWCwzcbAIATnMrTX0HAg5RuZMoiog6F+I0NTPrsesYtNU+OM25frEk6dEPOMw6RG/ltoiJpyTWsg7RE3Nfuo/+Y3cGoM+w9Zn/6qMAzPv3A3TO/nCZz+/85F2a+6/GzFvOZPrFxzPz1rOpLFxAU1t/+m+0PTMmHU/L4BE4bQNYOOMV+o/+UkO/H5FeOqq6h3hq5bmIv2cdoCeicgfzX32MARvvCMDQvU9g9lMBMyadQGXhfJymZUeyUaXMwndfY9CWezPq8LNxWtuY9cg1AAze9kBGHX4Oa3z5e5Tuv4whO32b2c/ezgc3+Hzy0FUN/d5EemgMsLt1iN7IZRG7XrA2sIt1jp6Y//qT9BmxIc0D4mmx1qHrMuI//5uRh53FgE12oWX1ZQf5LYOG0TxoGG2jNgKg/0Y7sPC915b6nEXvt6y+NnOfv4s19/Po+OBNOj56p87fkUhNpPpWtlwWMXAwKf3e5754LwOq0xIA5bmfABBFFUoPXcWgLf5jmb/TPHB1WlYbRsfMt4HqHPOwpTeZ++T+yxi847eg0glRJf6g00TU2V6n70Skpr7iesFg6xA9ldcrMt+yDtATlY4FLAifYehex376sbkv3cvspwIA+o/ZngGb7glA5+yZzLztbEZ8/TcArLHHUXx48x+Iyp20DFmLoXv/8NPnmPfKw/RZazQtg4YC0DZqY6b/9Rhah7v0Gf65Rn17Ir3RB9gPmGwdpCdyd4qz6wVjgRetc4hIzd0a+oW9rUP0RCpfnvdSKhZwiEi37eF6wRrWIXoij0V8oHUAEamLVuLpidTJVRG7XjAa2NQ6h4jUjYo4BQ6wDiAidbVHGvcpzlsR728dQETqqh8w0TpEd+WmiKsbg2xlnUNE6i512wbmpoiJV9Ll6fsVyatC2vaeyFMx7WYdQEQaYhiwiXWI7lARi0gW7WQdoDtyUcSuFwwDxlvnEJGG2XnVn5IcuShiYFcgVXNGItIrGhEnkKYlRPJlHdcLXOsQXaUiFpGsSs2oOPNF7HrBWsBY6xwi0nAq4gRJ1aS9iNSMijhBtrAOICImNna9YE3rEF2RhyLWbWsi+ZWKI8lVxCKSZalYYZfpIna9YADgWucQETOpuFCf6SIGxqGFHCJ5trF1gK7IehHrNA6RfFMRJ4Dmh0XybbDrBSOtQ6yKilhEsi7xo2IVsYhknYrYSvVopLWsc4iIucTfOZHZIgbWsQ4gIomgEbEhjYZFBGCMdYBVyXIRj7AOICKJMNw6wKpkuYg1IhYRgH6uF/S3DrEyLSv7Q8dx9l/Zn0dRdF1t49SUilhEFhkGTLMOsSIrLWLgq9W3w4Htgbuq7+8G3AOoiEUkDdJbxFEUHQ7gOM7NwCZRFM2ovj8SOK/+8XpFRSwiiwyzDrAyXZ0jdheVcNV7JP9KpIpYRBZJdBGvampikXscx7kduBKIgIOAu+uWqjZ014SILJL+Io6i6FjHcb7G4vPfLoyi6Pr6xeod1wuagaHWOUQkMdJfxFVPAbOjKLrDcZz+juMMiqJodr2C9dIAtA+xiCyW6CLu0hyx4zjfB64F/lz90NrADfUKVQNt1gFEJFES/Qq5qxfrjgF2AGYBRFE0lWSvVlERi8iS+lgHWJmuFnF7FEULF73jOE4L8UW7pFIRi8iSmq0DrExXi/hex3F+AfRzHGdP4BrgpvrF6jUVsYgsqTvXwxquq0XsAR8A/wKOBG6Joui/6paq9xL9209EGi7RndDV3xLHRVF0FnDRog84jnNC9WMiibWlM/XldZ3351jnEFudtMyAgnWMFepqER8KfLZ0D1vOx0QS5bCW22fs2/zQrtY5xNzHcIp1hhVa1e5rBwPfBDZwHGfKEn80CJhZz2AitTCpc+KofZsfso4h9jqtA6zMqkbEDwEziG+GPmOJj88GnqtXqBpI8h0d0kBPR6PHdEZN01ucyijrLGKqbB1gZVa1+9qbwJvAdo2JUzMLrANIcjwbbfjqVs5UFXG+JXpEvNK7JhzHeaD6drbjOLOWeMx2HGdWYyL2yCfWASQ5LuvcI9GnM0hDJHpEvNIijqJox+rbQVEUrbbEY1AURas1JmKPqIjlU7dUth0fRcyzziGmkrovDtD1vSb2WM7HDq19nNoI/UInoFuWBIB2+vR9h2H/ss4hpj60DrAyXV3Q8SvHcf7kOM4Ax3FGOI5zE4uPUUoqjYrlU9eVd+ywziCmPrAOsDJdLeJdgNeAZ4AHgCuiKDqwbqlqQ0Usn7q8c48xUaS7aXIsEyPi1YFticu4HVjfcZyk7/f7sXUASY73WGP4XPq+ZJ1DzGRiRPwIcGsURXsBWwOjgAfrlqo2NCKWpdxd2eJ96wxiJhNFvAfQ4TjOr6Iomg/8gXgjoCRTEctSLu7cSwfK5lcmpiZ+DnwJOLj6/myWXmmXRJqakKU8FY3ZuBw1zVj1Z0oGZWJEvG0URcdQXbEWRdHHJHzHe+Kl2SJLeS7aYKp1Bmm4DoqlRL9C7moRdziO00x1DwfHcdYEKnVLVRuvWQeQ5Lmsc89+1hmk4RK/QVlXi/hs4HpguOM4pxLfwnZa3VLVhkY+soybK1/aVKvscudd6wCr0qX9iKMoutxxnCeB3YmPqd8viqKk3wr0qnUASZ52+vSdztDH1mbmNtZZpGH+bR1gVbo6IiaKon9HUXReFEXnpqCECf3CHFLwm1Aa7/ryju3WGaShXrQOsCpdLuKU0qhYlnF55x6jtcouVxI/cMx6EWueWJYxg6FrzaNv4l+uSs1oRGxMI2JZrrsrm79nnUEaopMUDMiyXsSJ/wcQG5M7J2qVXT68SrGU+J33sl7EGhHLcj0ebbRROXJ0MTf7Ej8/DNkv4n+T8LOqxIrjPK9VdnmQ+PlhyHgRh35hPvCsdQ5JpsvKe7RZZ5C604g4IR62DiDJdFN5u02jiPnWOaSuNCJOCBWxLNcC2vq9yxrPW+eQupkDpOKsQhWx5NoN5R00Is6uBymWUnGNKPNFHPqFN9BSZ1mBSzv3HG2dQermPusAXZX5Iq7SqFiWazrDRs6N2rTKLpvutQ7QVSpiyb17K5vrFVP2zAcetw7RVXkp4oesA0hyTeqcONw6g9TcIxRLC61DdFVeivhJIPHLHMXGY9HGY8uRo70nsiU10xKQkyIO/cIC4BHrHJJUjvNC5L5inUJqSkWcUIF1AEkurbLLlIWkbOClIhYBbipvPz6K4lPKJfUeo1hK1b9lboo49AvPA9Osc0gyzaet/3usnopVWLJKU6wDdFduirhKo2JZoRvLO6RqFCUrdI11gO7KWxHfaB1AkuvS8p4bWmeQXnuCYim0DtFdeSviu4CSdQhJprejNUfNi9pets4hvZK60TDkrIhDv9AB3GydQ5LrvsqmWmWXbirilPi7dQBJrknlicOsM0iPPUWx9IZ1iJ7IYxHfBsyzDiHJ9Ehlk03KkfO+dY5F3ipV2G3yXMaeN4dx58/hrEfaAfhofsSel85l9Dlz2PPSuXw8P1rhc8xqj1j7f2dz7C3xjp/tnRF7XTaX8efP4fzHF68C/sFN83l6Rrm+31B9pXI0DDks4urxSdda55CkcpyXovUTs8qupQnOmNCXl44ZyCNHDOC8xzt48YMy/gPt7L5BC1OPG8juG7TgP9C+wuf45V3t7LJ+86fv3/5aJ1uNbOa5owdw4ZNxET/7bplKBFuObF7R06SBijhlLrIOIMl1eXn3VusMi4wc1MQXquU4qM1h7JpNvDMr4saXOzl08zjmoZu3csPLy9///MnpZd6bW2HChi2ffqy1CeZ3Qmdl8ef98u52Ttkt1YsLn6ZYes06RE/lsohDv/AAKTlUUBrvxvIO46OIFQ8xjYSfVHh6Rplt12nmvTkVRg6Kf3xHDmri/bmVZT6/EkX85B8L+J89+y718T03bOHdORW2/ctcfrZDG1Ne7mCrkc2MGpTqOrjaOkBvtKz6UzLrL8AZ1iEkeebRd8D7DHliBJ980TrLInMWRhxw9TzO3Ksvq7U5Xfo75z/ewd6jW1h38NIF29LkcMUB/QHoKEdMvGweUw7uz49vX8C0UoVDNm9ln40S86KgKxYCF1uH6I1U/wrspcmQvFGPJMON5e0Tc0G3oxyX8Lc2bWX/sXFBjhjYxIzZ8Sh4xuwKwwcs+6P88NudnPvYQtwzZ3PiP9q55NkOvDuWXjx4/uMLOXTzVh5+q0yfZvjbgf347X2p+7G4lmIp1duY5raIQ78wE7jeOock06XlCYlYZRdFEUdMWcDYYc38eLvFc7j7jGlh8rPxFtuTn+1g342WfXF7+f79mfajQYQ/HMQfJrRxyOat+Hssnqb4eH7EzVM7OWTzVuZ1RDQ54DiwIBXHbS7lXOsAvZXbIq7SRTtZrrei4WvPj/qY3z3x4FtlLn2ug7ve6GSLC+awxQVzuGVqB96Offjn652MPmcO/3y9E2/HuKSfmF7me1O6djD1Kfe2c/JObTiOw8TPt/DE9DKb/mku3/9Cn3p+S7X2JMVS6o9Cc6JoxfcfZp3rBQ7wCvB56yySPBe2nnHPhOYnd7XOISv1XYqlVM8PQ85HxKFfiIgv2oksY3J54prWGWSlZgJXWoeohVwXcdXFoA3BZVkPVTYZW4mcD6xzyAr9NW0bwK9I7os49Avvo7liWY6IpqaXovW0G1syVYDzrUPUSu6LuOp04nsRRZZyRYJW2clSbqZYetM6RK2oiIHQL7xNym8Il/q4obzDuCSushP+YB2gllTEi/lA+u6glLqaS7+BHzBEZ9klyz8olu63DlFLKuKq0C+EwKXWOSR5ppS3S8wqOwHgZOsAtaYiXtppQKo3ZJXau6Q84XPWGeRTN1AsPW4dotZUxEsI/cKrwFXWOSRZpkUj1pkf9ZlqnUOokMHRMKiIl+dU4n9wkU89WBk/3TqDcCXF0gvWIepBRfwZoV94iZTvbSq1N6k8cQ3rDDnXCfzaOkS9qIiX7yR0rp0s4cHKuHGVyPnQOkeOXZzmEzhWRUW8HKFfmAb83jqHJEdEU9PL0br/ts6RU+3Af1uHqCcV8YqdDqTyaG6pjyvKX87ziTaWzqNYess6RD2piFcg9AsLgB9b55DkuK6807go0lL4BnubDM8NL6IiXonQL9wA/MM6hyTDXPoN+pDBWmXXWMdRLM2xDlFvKuJVOx7osA4hyXBTebvMl0KC3ECxdIN1iEZQEa9C6BdeBs6yziHJcEl5T62ya4zZwHHWIRpFRdw1pwDvWocQe2E0ct0FUeur1jly4GSKpbetQzSKirgLQr8wG/iRdQ5Jhgcr43NTEEaeIAMnM3eHiriLQr9wFXCtdQ6xN7k8Yah1hgwrAz+gWMrVNgMq4u45GnjPOoTYur+y6bhK5My0zpFRZ1EsPW0dotFUxN0Q+oUPgSOtc4itiKamV6J1XrLOkUGvAb+yDmFBRdxNoV+4EZhknUNsXalVdrXWARxMsTTXOogFFXHPHEf821ty6u/lnTaJIt1fXkO/zOKG712lIu6B0C/MAQ5GCz1yaw79V5vJas9b58iIO4j3dsktFXEPhX7hcXI6nyWxoPylWdYZMuAD4BCKpcg6iCUVce+cDtxpHUJsTCpP2MA6Q8pVgG9SLM2wDmJNRdwLoV+oAP+JtsvMpTeiUeu1R626VtBzv6FYusM6RBKoiHsp9AszgX2I18ZLzjxU2STT++TW0W1kfLP37lAR10DoF54Hvo0OHc2dyTrLrifeAr7d03lhx3H+z3Gc9x3HyczFUhVxjYR+YQoZPepbVuy+ymbjKpHzkXWOFCkBBYql3qxMnATsVZs4yaAirqHQL/wOuMI6hzROhabmqdHaWmXXNR3A/hRLvdpcP4qi+4BM/fJTEdfeEUBub0zPo6vKu+nnqGuOoFi6yzpEEuk/oBqrnnW3HzDdOos0xrXlnbXKbtVOpli61DpEUqmI6yD0C9OJyziX6+bzZjYDBn/EoMxcOKqDiyiWTrUOkWQq4jqprrz7KrDAOovUn1bZrdCtxNvHykqoiOso9At3A18DHcGedZPKE13rDAn0FPANiqVyLZ/UcZwrgYeBjRzHedtxnCNq+fwWnCjK9RLvhnC9YD/gGkBbJ2bYy22HvN7mdOpw0djrwA4USzrrsQs0Im6A0C/cQLzgo6YjA0mWR7TKbpGpwC4q4a5TETdI6Bf+BnwX0EuQjJpcnjDEOkMCvAjsnKcTmGtBRdxAoV+4BF24yKx7KluMr0R8bJ3D0HPArhoJd5+KuMFCv/Bn4IfWOaT2KjQ1v5bfVXZPArtRLH1gHSSNVMQGQr9wFvA9oNM6i9TWVeXdrCNYeATYnWIpU8uOG0lFbCT0C38F9kWLPjLlmvLO46IoV79g7wcmUCyVrIOkmYrYUOgXbgF2Bd43jiI1MouBgz/Ozyq7u4D/oFjSXty9pCI2FvqFJ4DtiG/5kQy4pbxNHkaHk4hLWK/oakBFnAChX3gd2B541DqL9N7k8sT1rTPUUQU4kWLpcIolrRitERVxQoR+4UPgy8BN1lmkd6ZG67gLo5YsnmM4C/gKxdIZ1kGyRkWcIKFfmEe8N8X51lmkdx6tjJ1mnaHGXgW+RLF0q3WQLFIRJ0zoF8qhXziGeBWedm5LqcnlPbO0yu5OYFuKpbzeI113KuKECv3CxcTzxll8iZt5d1e2HBdFZOGi3XnAXrpHuL5UxAkW+oWnga2Am62zSPeUaW55LRr1gnWOXpgPHEmxdCzFUp7uizahIk640C98DOwDnAg6jidNri7vah2hp+IBQLF0oXWQvNB+xCniesHWwFWA9rxNgcHM+eSZth8MdJzU7ENdAU4HfkWxpF/6DaQRcYpUj1/akriMJeFKDBzyCQPTssouJN457ecq4cZTEadM6BdmhX7hYOAAYIZ1Hlm5W9Oxyu4SYHOKpfutg+SVpiZSzPWCwcQvJb8POMZxZDk2cqa9cXubt4F1jhX4iPiC3LXWQfJORZwBrhfsDFwEjLHOIst6pe2QsI/T6Vrn+IwpwNEUS9Otg4imJjIh9Av3AZsBp6I7KxLn8cpGb1pnWMLLxJv17KsSTg4VcUaEfqE99AsnE993/Jh1HllscnnCatYZgNnAz4BNKZZusw4jS9PURAa5XtAEHAn8GhhhHCf3mil3vtr2nbmOw2CDLx8BlwEnUSzp4m5CqYgzzPWCAcCPgJ8CSRiV5dadfX7y8IZNM7Zr8Jd9EjieYumhBn9d6SZNTWRY6Bfmhn7ht8QLQP4ItBtHyq2ry7tWGvjl3gd+AGyjEk4HjYhzxPWC9YDfAIegX8INNYTZHz/dduRqjkNzHb/MO8S3M15EsTS/jl9HakxFnEOuF4wDTiPew0Ia5Jm27z87xJm7eR2e+g3g98DFOjUjnVTEOVbdu+JHwIFAq3GczPNbLrznoJZ7dq3hU74M/A64XDukpZuKWHC9YG3gGOJ5xaHGcTJrrPPm67e2/bwWGzb9i/ie8Wsolho59yx1oiKWT7le0A/4DnACsIlxnEya2vadN1udck8OF37I3aoAAAJuSURBVC0DtwEXAAHFkn5wM0RFLMvlesEE4IfAXmgfi5q5ovW3927f/OIu3fgrrwP/B0yiWHqnTrHEmIpYVsr1gjHAocC3gCwfE98QezU99vQFfc7cchWf1g5cB/wFuFuj3+xTEUuXuF7gADsD3wa+DiarxFKvhc6OqW2HzHec5S6weY64fC/XGXH5oiKWbnO9oA8wkbiQ90Gl3C139fnxw59renfRKrt/AdcD11MsPWMYSwypiKVXqqU8AfgasAewnm2ixOs8rPm2y4qtlzwP3ECx9Jp1ILGnIpaacr3g88Du1ceX0e1wANOBW6uPO0K/kIZTO6SBVMRSN9V55S1YXMw7AQNMQ9VfmXi64VHgEeDR0C+8ZBtJkk5FLA3jekEr8eGnm33msbplrl56m7h0FxXvk6FfmGcbSdJGRSzmXC9Yl2XLeTTJWXa9gHg/h9eI7+t9rfp4JvQLurdXek1FLIlUndZYExhVfYxc4n8v+f5welbYC4ESMOszb0vANJYu3OmhX9APitSNilhSz/WCZqDfEo++QDPxVp+L3gLMoVq4oV/Q3sySGCpiERFj2hxcRMSYilhExJiKWETEmIpYRMSYilhExJiKWETEmIpYRMSYilhExJiKWETEmIpYRMSYilhExJiKWETEmIpYRMSYilhExJiKWETEmIpYRMSYilhExJiKWETEmIpYRMSYilhExJiKWETEmIpYRMSYilhExJiKWETEmIpYRMSYilhExJiKWETEmIpYRMSYilhExJiKWETEmIpYRMSYilhExJiKWETEmIpYRMSYilhExJiKWETEmIpYRMSYilhExNj/B8Jmm9ZsxzOzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#применим функцию get_info_column() к столбцу 'exited'\n",
    "get_info_column(churn, 'exited')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col0 {\n",
       "            background-color:  #014636;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col1 {\n",
       "            background-color:  #fbf3f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col2 {\n",
       "            background-color:  #faf2f8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col3 {\n",
       "            background-color:  #fbf2f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col4 {\n",
       "            background-color:  #fbf3f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col5 {\n",
       "            background-color:  #d6d5e8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col6 {\n",
       "            background-color:  #d2d2e7;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col7 {\n",
       "            background-color:  #fdf5fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col8 {\n",
       "            background-color:  #e7dfee;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col9 {\n",
       "            background-color:  #fef6fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col10 {\n",
       "            background-color:  #ede3f1;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col0 {\n",
       "            background-color:  #fcf4f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col1 {\n",
       "            background-color:  #014636;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col2 {\n",
       "            background-color:  #faf2f8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col3 {\n",
       "            background-color:  #faf1f8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col4 {\n",
       "            background-color:  #fef6fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col5 {\n",
       "            background-color:  #d6d5e8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col6 {\n",
       "            background-color:  #d1d1e6;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col7 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col8 {\n",
       "            background-color:  #eae1ef;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col9 {\n",
       "            background-color:  #fbf3f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col10 {\n",
       "            background-color:  #ebe1f0;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col0 {\n",
       "            background-color:  #fcf4f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col1 {\n",
       "            background-color:  #fbf3f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col2 {\n",
       "            background-color:  #014636;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col3 {\n",
       "            background-color:  #fbf3f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col4 {\n",
       "            background-color:  #fbf2f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col5 {\n",
       "            background-color:  #d3d3e7;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col6 {\n",
       "            background-color:  #d2d2e7;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col7 {\n",
       "            background-color:  #fef6fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col8 {\n",
       "            background-color:  #e5deed;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col9 {\n",
       "            background-color:  #fef6fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col10 {\n",
       "            background-color:  #eee5f1;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col0 {\n",
       "            background-color:  #fdf4fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col1 {\n",
       "            background-color:  #fbf2f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col2 {\n",
       "            background-color:  #fcf4f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col3 {\n",
       "            background-color:  #014636;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col4 {\n",
       "            background-color:  #fdf4fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col5 {\n",
       "            background-color:  #ced0e6;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col6 {\n",
       "            background-color:  #d9d7e9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col7 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col8 {\n",
       "            background-color:  #d9d7e9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col9 {\n",
       "            background-color:  #fef6fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col10 {\n",
       "            background-color:  #a3bcda;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col0 {\n",
       "            background-color:  #fef6fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col1 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col2 {\n",
       "            background-color:  #fbf3f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col3 {\n",
       "            background-color:  #fdf4fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col4 {\n",
       "            background-color:  #014636;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col5 {\n",
       "            background-color:  #d5d4e8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col6 {\n",
       "            background-color:  #d2d2e7;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col7 {\n",
       "            background-color:  #f9f0f8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col8 {\n",
       "            background-color:  #efe5f2;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col9 {\n",
       "            background-color:  #fcf4f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col10 {\n",
       "            background-color:  #ede3f1;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col0 {\n",
       "            background-color:  #fef6fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col1 {\n",
       "            background-color:  #fef6fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col2 {\n",
       "            background-color:  #faf2f8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col3 {\n",
       "            background-color:  #f7eef6;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col4 {\n",
       "            background-color:  #fbf3f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col5 {\n",
       "            background-color:  #014636;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col6 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col7 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col8 {\n",
       "            background-color:  #ece2f0;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col9 {\n",
       "            background-color:  #fbf3f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col10 {\n",
       "            background-color:  #d3d3e7;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col0 {\n",
       "            background-color:  #fcf4f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col1 {\n",
       "            background-color:  #faf1f8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col2 {\n",
       "            background-color:  #faf1f8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col3 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col4 {\n",
       "            background-color:  #f9f0f8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col5 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col6 {\n",
       "            background-color:  #014636;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col7 {\n",
       "            background-color:  #fdf4fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col8 {\n",
       "            background-color:  #e8e0ef;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col9 {\n",
       "            background-color:  #fbf3f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col10 {\n",
       "            background-color:  #f1e8f3;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col0 {\n",
       "            background-color:  #fdf4fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col1 {\n",
       "            background-color:  #fef6fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col2 {\n",
       "            background-color:  #fcf4f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col3 {\n",
       "            background-color:  #fdf4fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col4 {\n",
       "            background-color:  #f7eef6;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col5 {\n",
       "            background-color:  #d7d5e8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col6 {\n",
       "            background-color:  #d3d3e7;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col7 {\n",
       "            background-color:  #014636;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col8 {\n",
       "            background-color:  #ede3f0;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col9 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col10 {\n",
       "            background-color:  #ece2f0;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col0 {\n",
       "            background-color:  #fbf2f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col1 {\n",
       "            background-color:  #fcf4f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col2 {\n",
       "            background-color:  #f7eef7;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col3 {\n",
       "            background-color:  #eee5f1;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col4 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col5 {\n",
       "            background-color:  #d6d5e8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col6 {\n",
       "            background-color:  #d2d2e7;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col7 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col8 {\n",
       "            background-color:  #014636;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col9 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col10 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col0 {\n",
       "            background-color:  #fef6fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col1 {\n",
       "            background-color:  #faf1f8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col2 {\n",
       "            background-color:  #fbf3f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col3 {\n",
       "            background-color:  #fcf4f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col4 {\n",
       "            background-color:  #f9f0f8;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col5 {\n",
       "            background-color:  #d2d2e7;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col6 {\n",
       "            background-color:  #d2d2e7;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col7 {\n",
       "            background-color:  #fef6fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col8 {\n",
       "            background-color:  #ece2f0;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col9 {\n",
       "            background-color:  #014636;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col10 {\n",
       "            background-color:  #e7dfee;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col0 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col1 {\n",
       "            background-color:  #fdf5fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col2 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col3 {\n",
       "            background-color:  #bdc8e1;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col4 {\n",
       "            background-color:  #fdf5fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col5 {\n",
       "            background-color:  #b8c6e0;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col6 {\n",
       "            background-color:  #dcd8ea;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col7 {\n",
       "            background-color:  #fef6fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col8 {\n",
       "            background-color:  #fff7fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col9 {\n",
       "            background-color:  #fcf4f9;\n",
       "            color:  #000000;\n",
       "        }    #T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col10 {\n",
       "            background-color:  #014636;\n",
       "            color:  #f1f1f1;\n",
       "        }</style><table id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >rownumber</th>        <th class=\"col_heading level0 col1\" >customerid</th>        <th class=\"col_heading level0 col2\" >creditscore</th>        <th class=\"col_heading level0 col3\" >age</th>        <th class=\"col_heading level0 col4\" >tenure</th>        <th class=\"col_heading level0 col5\" >balance</th>        <th class=\"col_heading level0 col6\" >numofproducts</th>        <th class=\"col_heading level0 col7\" >hascrcard</th>        <th class=\"col_heading level0 col8\" >isactivemember</th>        <th class=\"col_heading level0 col9\" >estimatedsalary</th>        <th class=\"col_heading level0 col10\" >exited</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9level0_row0\" class=\"row_heading level0 row0\" >rownumber</th>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col1\" class=\"data row0 col1\" >0.004202</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col2\" class=\"data row0 col2\" >0.005840</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col3\" class=\"data row0 col3\" >0.000783</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col4\" class=\"data row0 col4\" >-0.007322</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col5\" class=\"data row0 col5\" >-0.009067</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col6\" class=\"data row0 col6\" >0.007246</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col7\" class=\"data row0 col7\" >0.000599</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col8\" class=\"data row0 col8\" >0.012044</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col9\" class=\"data row0 col9\" >-0.005988</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row0_col10\" class=\"data row0 col10\" >-0.016571</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9level0_row1\" class=\"row_heading level0 row1\" >customerid</th>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col0\" class=\"data row1 col0\" >0.004202</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col2\" class=\"data row1 col2\" >0.005308</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col3\" class=\"data row1 col3\" >0.009497</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col4\" class=\"data row1 col4\" >-0.021418</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col5\" class=\"data row1 col5\" >-0.012419</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col6\" class=\"data row1 col6\" >0.016972</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col7\" class=\"data row1 col7\" >-0.014025</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col8\" class=\"data row1 col8\" >0.001665</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col9\" class=\"data row1 col9\" >0.015271</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row1_col10\" class=\"data row1 col10\" >-0.006248</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9level0_row2\" class=\"row_heading level0 row2\" >creditscore</th>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col0\" class=\"data row2 col0\" >0.005840</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col1\" class=\"data row2 col1\" >0.005308</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col3\" class=\"data row2 col3\" >-0.003965</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col4\" class=\"data row2 col4\" >-0.000062</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col5\" class=\"data row2 col5\" >0.006268</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col6\" class=\"data row2 col6\" >0.012238</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col7\" class=\"data row2 col7\" >-0.005458</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col8\" class=\"data row2 col8\" >0.025651</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col9\" class=\"data row2 col9\" >-0.001384</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row2_col10\" class=\"data row2 col10\" >-0.027094</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9level0_row3\" class=\"row_heading level0 row3\" >age</th>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col0\" class=\"data row3 col0\" >0.000783</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col1\" class=\"data row3 col1\" >0.009497</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col2\" class=\"data row3 col2\" >-0.003965</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col4\" class=\"data row3 col4\" >-0.013134</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col5\" class=\"data row3 col5\" >0.028308</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col6\" class=\"data row3 col6\" >-0.030680</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col7\" class=\"data row3 col7\" >-0.011721</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col8\" class=\"data row3 col8\" >0.085472</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col9\" class=\"data row3 col9\" >-0.007201</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row3_col10\" class=\"data row3 col10\" >0.285323</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9level0_row4\" class=\"row_heading level0 row4\" >tenure</th>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col0\" class=\"data row4 col0\" >-0.007322</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col1\" class=\"data row4 col1\" >-0.021418</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col2\" class=\"data row4 col2\" >-0.000062</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col3\" class=\"data row4 col3\" >-0.013134</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col5\" class=\"data row4 col5\" >-0.007911</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col6\" class=\"data row4 col6\" >0.011979</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col7\" class=\"data row4 col7\" >0.027232</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col8\" class=\"data row4 col8\" >-0.032178</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col9\" class=\"data row4 col9\" >0.010520</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row4_col10\" class=\"data row4 col10\" >-0.016761</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9level0_row5\" class=\"row_heading level0 row5\" >balance</th>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col0\" class=\"data row5 col0\" >-0.009067</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col1\" class=\"data row5 col1\" >-0.012419</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col2\" class=\"data row5 col2\" >0.006268</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col3\" class=\"data row5 col3\" >0.028308</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col4\" class=\"data row5 col4\" >-0.007911</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col5\" class=\"data row5 col5\" >1.000000</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col6\" class=\"data row5 col6\" >-0.304180</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col7\" class=\"data row5 col7\" >-0.014858</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col8\" class=\"data row5 col8\" >-0.010084</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col9\" class=\"data row5 col9\" >0.012797</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row5_col10\" class=\"data row5 col10\" >0.118533</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9level0_row6\" class=\"row_heading level0 row6\" >numofproducts</th>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col0\" class=\"data row6 col0\" >0.007246</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col1\" class=\"data row6 col1\" >0.016972</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col2\" class=\"data row6 col2\" >0.012238</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col3\" class=\"data row6 col3\" >-0.030680</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col4\" class=\"data row6 col4\" >0.011979</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col5\" class=\"data row6 col5\" >-0.304180</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col6\" class=\"data row6 col6\" >1.000000</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col7\" class=\"data row6 col7\" >0.003183</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col8\" class=\"data row6 col8\" >0.009612</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col9\" class=\"data row6 col9\" >0.014204</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row6_col10\" class=\"data row6 col10\" >-0.047820</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9level0_row7\" class=\"row_heading level0 row7\" >hascrcard</th>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col0\" class=\"data row7 col0\" >0.000599</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col1\" class=\"data row7 col1\" >-0.014025</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col2\" class=\"data row7 col2\" >-0.005458</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col3\" class=\"data row7 col3\" >-0.011721</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col4\" class=\"data row7 col4\" >0.027232</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col5\" class=\"data row7 col5\" >-0.014858</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col6\" class=\"data row7 col6\" >0.003183</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col8\" class=\"data row7 col8\" >-0.011866</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col9\" class=\"data row7 col9\" >-0.009933</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row7_col10\" class=\"data row7 col10\" >-0.007138</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9level0_row8\" class=\"row_heading level0 row8\" >isactivemember</th>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col0\" class=\"data row8 col0\" >0.012044</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col1\" class=\"data row8 col1\" >0.001665</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col2\" class=\"data row8 col2\" >0.025651</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col3\" class=\"data row8 col3\" >0.085472</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col4\" class=\"data row8 col4\" >-0.032178</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col5\" class=\"data row8 col5\" >-0.010084</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col6\" class=\"data row8 col6\" >0.009612</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col7\" class=\"data row8 col7\" >-0.011866</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col8\" class=\"data row8 col8\" >1.000000</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col9\" class=\"data row8 col9\" >-0.011421</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row8_col10\" class=\"data row8 col10\" >-0.156128</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9level0_row9\" class=\"row_heading level0 row9\" >estimatedsalary</th>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col0\" class=\"data row9 col0\" >-0.005988</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col1\" class=\"data row9 col1\" >0.015271</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col2\" class=\"data row9 col2\" >-0.001384</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col3\" class=\"data row9 col3\" >-0.007201</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col4\" class=\"data row9 col4\" >0.010520</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col5\" class=\"data row9 col5\" >0.012797</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col6\" class=\"data row9 col6\" >0.014204</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col7\" class=\"data row9 col7\" >-0.009933</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col8\" class=\"data row9 col8\" >-0.011421</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col9\" class=\"data row9 col9\" >1.000000</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row9_col10\" class=\"data row9 col10\" >0.012097</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9level0_row10\" class=\"row_heading level0 row10\" >exited</th>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col0\" class=\"data row10 col0\" >-0.016571</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col1\" class=\"data row10 col1\" >-0.006248</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col2\" class=\"data row10 col2\" >-0.027094</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col3\" class=\"data row10 col3\" >0.285323</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col4\" class=\"data row10 col4\" >-0.016761</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col5\" class=\"data row10 col5\" >0.118533</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col6\" class=\"data row10 col6\" >-0.047820</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col7\" class=\"data row10 col7\" >-0.007138</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col8\" class=\"data row10 col8\" >-0.156128</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col9\" class=\"data row10 col9\" >0.012097</td>\n",
       "                        <td id=\"T_f52a70ac_3933_11eb_b280_d850e622fec9row10_col10\" class=\"data row10 col10\" >1.000000</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1b76c1fd388>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# построим матрицу корреляции всего df\n",
    "churn.corr().style.background_gradient(cmap='PuBuGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Присутствует слабая прямая корреляция между возрастом и фактом ухода (0.285)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заменим значения в столбце tenure на характерные\n",
    "# для этого разделим значения кредитного рейтинга и возраста клиентов на группы\n",
    "def get_group_creditscore(row):\n",
    "    creditscore = row['creditscore']\n",
    "    return creditscore // 50\n",
    "def get_group_age(row):\n",
    "    age = row['age']\n",
    "    return age // 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим копию df\n",
    "churn_up = churn.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполним значения с помощью функции apply(), get_group_creditscore(), get_group_age()\n",
    "churn_up['creditscore_level'] = churn_up.apply(get_group_creditscore, axis=1)\n",
    "churn_up['age_level'] = churn_up.apply(get_group_age, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим функцию, которая по заданному df и столбцу, создавала бы сводную таблицу, в которой для каждой возрастной категории и \n",
    "# категории кредитного рейтинга считала бы среднее значение выбранного столбца\n",
    "# и затем для каждого соответствия категорий возраста и кредитного рейтинга исходного df заполняла бы пропуск в данном \n",
    "# столбце из этой сводной таблицы\n",
    "def fill_with_pivot_table(data, column):\n",
    "    # создадим сводную таблицу, в которой для каждой марки и модели автомобиля считается среднее значение столбца column\n",
    "    pivot_table = data.pivot_table(index=['creditscore_level', 'age_level'], values=column).astype(int)\n",
    "    # создадим df, который является срезом data, состоящим из пропусков в столбце column\n",
    "    df_nan = data.loc[data[column].isnull()]\n",
    "    def change_values(row):\n",
    "        creditscore_level = row['creditscore_level']\n",
    "        age_level = row['age_level']\n",
    "        # создадим конструкцию try/except на случай, если в pivot_table не найдется значения по двум категориям\n",
    "        try:\n",
    "            value = pivot_table.loc[(creditscore_level, age_level),:]\n",
    "        except:\n",
    "            value = np.nan\n",
    "        return value    \n",
    "    # заполним пропуски в df_nan с помощью функции change_values() и apply()  \n",
    "    df_nan = df_nan.copy()\n",
    "    df_nan[column] = df_nan.apply(change_values, axis=1)\n",
    "    # теперь надо заменить значения в data из df_nan\n",
    "    for i in df_nan.index:\n",
    "        if i in data.index:\n",
    "            data.loc[i,column] = df_nan.loc[i,column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# применим функцию fill_with_pivot_table()\n",
    "fill_with_pivot_table(churn_up, 'tenure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rownumber</th>\n",
       "      <th>customerid</th>\n",
       "      <th>surname</th>\n",
       "      <th>creditscore</th>\n",
       "      <th>geography</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>numofproducts</th>\n",
       "      <th>hascrcard</th>\n",
       "      <th>isactivemember</th>\n",
       "      <th>estimatedsalary</th>\n",
       "      <th>exited</th>\n",
       "      <th>creditscore_level</th>\n",
       "      <th>age_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6443</th>\n",
       "      <td>6444</td>\n",
       "      <td>15764927</td>\n",
       "      <td>Rogova</td>\n",
       "      <td>753</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121513.31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>195563.99</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rownumber  customerid surname  creditscore geography gender  age  \\\n",
       "6443       6444    15764927  Rogova          753    France   Male   92   \n",
       "\n",
       "      tenure    balance  numofproducts  hascrcard  isactivemember  \\\n",
       "6443     NaN  121513.31              1          0               1   \n",
       "\n",
       "      estimatedsalary  exited  creditscore_level  age_level  \n",
       "6443        195563.99       0                 15          9  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим на пропуски\n",
    "churn_up.loc[churn_up['tenure'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных остался только 1 пропуск. Мужчина 92 лет. Получим медианное значение для категории creditscore_level = 15 и возрастом не менее 70 лет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(churn_up.query('creditscore_level == 15 & age_level > 6')['tenure'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заменим пропуск на данное значение\n",
    "churn_up = churn_up.fillna(\n",
    "    round(churn_up.query(\"creditscore_level == 15 & age_level > 6\")[\"tenure\"].median())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# изменим тип данных в \"tenure\" на целочисленный\n",
    "churn_up['tenure'] = churn_up['tenure'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изучили общую информацию о датафрейме:\n",
    " - названия столбцов привели к нижнему регистру;\n",
    " - нет пропусков, кроме столбца с количеством недвижимости \"tenure\";\n",
    " - добавили два столца - категории кредитного рейтинга и возраста. На их основании заменили пропуски в столбце \"tenure\";\n",
    " - есть категориальные признаки: \"surname\", \"geography\", \"gender\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2-section'></a>\n",
    "### 1. Разбиение на выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо преобразовать категориальные признаки в численные. В этом нам поможет One-Hot Encoding (OHE). Учтём также, что факт ухода клиента не зависит от его фамилии, поэтому не будем работать с данным категориальным признаком. Также, как и признаки \"rownumber\", \"customerid\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creditscore</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>numofproducts</th>\n",
       "      <th>hascrcard</th>\n",
       "      <th>isactivemember</th>\n",
       "      <th>estimatedsalary</th>\n",
       "      <th>exited</th>\n",
       "      <th>geography_Germany</th>\n",
       "      <th>geography_Spain</th>\n",
       "      <th>gender_Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   creditscore  age  tenure    balance  numofproducts  hascrcard  \\\n",
       "0          619   42       2       0.00              1          1   \n",
       "1          608   41       1   83807.86              1          0   \n",
       "2          502   42       8  159660.80              3          1   \n",
       "3          699   39       1       0.00              2          0   \n",
       "4          850   43       2  125510.82              1          1   \n",
       "\n",
       "   isactivemember  estimatedsalary  exited  geography_Germany  \\\n",
       "0               1        101348.88       1                  0   \n",
       "1               1        112542.58       0                  0   \n",
       "2               0        113931.57       1                  0   \n",
       "3               0         93826.63       0                  0   \n",
       "4               1         79084.10       0                  0   \n",
       "\n",
       "   geography_Spain  gender_Male  \n",
       "0                0            0  \n",
       "1                1            0  \n",
       "2                0            0  \n",
       "3                0            0  \n",
       "4                1            0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# воспользуемся функцией get_dummies()\n",
    "churn_up = pd.get_dummies(churn_up.drop(['surname', 'rownumber', 'customerid','creditscore_level','age_level'], axis=1), drop_first=True)\n",
    "churn_up.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creditscore</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>hascrcard</th>\n",
       "      <th>isactivemember</th>\n",
       "      <th>estimatedsalary</th>\n",
       "      <th>exited</th>\n",
       "      <th>geography_Germany</th>\n",
       "      <th>geography_Spain</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>numofproducts_1</th>\n",
       "      <th>numofproducts_2</th>\n",
       "      <th>numofproducts_3</th>\n",
       "      <th>numofproducts_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>619</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>608</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>502</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>699</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>850</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   creditscore  age  tenure    balance  hascrcard  isactivemember  \\\n",
       "0          619   42       2       0.00          1               1   \n",
       "1          608   41       1   83807.86          0               1   \n",
       "2          502   42       8  159660.80          1               0   \n",
       "3          699   39       1       0.00          0               0   \n",
       "4          850   43       2  125510.82          1               1   \n",
       "\n",
       "   estimatedsalary  exited  geography_Germany  geography_Spain  gender_Male  \\\n",
       "0        101348.88       1                  0                0            0   \n",
       "1        112542.58       0                  0                1            0   \n",
       "2        113931.57       1                  0                0            0   \n",
       "3         93826.63       0                  0                0            0   \n",
       "4         79084.10       0                  0                1            0   \n",
       "\n",
       "   numofproducts_1  numofproducts_2  numofproducts_3  numofproducts_4  \n",
       "0                1                0                0                0  \n",
       "1                1                0                0                0  \n",
       "2                0                0                1                0  \n",
       "3                0                1                0                0  \n",
       "4                1                0                0                0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# вручную добавим столбцы по 'numofproducts'\n",
    "churn_up[['numofproducts_1', 'numofproducts_2', 'numofproducts_3', 'numofproducts_4']] = pd.get_dummies(churn_up['numofproducts'])\n",
    "churn_up = churn_up.drop('numofproducts', axis=1)\n",
    "churn_up.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все оставшиеся признаки значимы. Но в данных есть значения предполагаемой зарплаты - порядка сотен тысяч, а есть кредитный рейтинг - порядка нескольких сотен. Необходимо масштабировать признаки. Воспользуемся стандартизацией данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разделим данные по признакам и целевому признаку\n",
    "features = churn_up.drop('exited', axis=1)\n",
    "target = churn_up['exited']\n",
    "#сделаем 3 выборки: тренировочную, валидационную и тестовую в соотношении 60%, 20%, 20%\n",
    "#сделаем тренировочную выборку\n",
    "features_train, features_rest, target_train, target_rest = train_test_split(\n",
    "    features, target, test_size=0.4, random_state=12345, stratify=target)\n",
    "#оставшиеся данные разделим пополам на валидационную и тестовую выборки\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(\n",
    "    features_rest, target_rest, test_size=0.5, random_state=12345, stratify=target_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>creditscore</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>hascrcard</th>\n",
       "      <th>isactivemember</th>\n",
       "      <th>estimatedsalary</th>\n",
       "      <th>geography_Germany</th>\n",
       "      <th>geography_Spain</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>numofproducts_1</th>\n",
       "      <th>numofproducts_2</th>\n",
       "      <th>numofproducts_3</th>\n",
       "      <th>numofproducts_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>-1.040434</td>\n",
       "      <td>0.953312</td>\n",
       "      <td>0.375384</td>\n",
       "      <td>0.774657</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.119110</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9925</th>\n",
       "      <td>0.454006</td>\n",
       "      <td>-0.095244</td>\n",
       "      <td>-0.349296</td>\n",
       "      <td>1.910540</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.258658</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8746</th>\n",
       "      <td>0.103585</td>\n",
       "      <td>-0.476537</td>\n",
       "      <td>1.100065</td>\n",
       "      <td>0.481608</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.422836</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>-0.184996</td>\n",
       "      <td>0.190726</td>\n",
       "      <td>0.013044</td>\n",
       "      <td>0.088439</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.160427</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>-0.720933</td>\n",
       "      <td>1.620574</td>\n",
       "      <td>-1.436316</td>\n",
       "      <td>0.879129</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.113236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      creditscore       age    tenure   balance  hascrcard  isactivemember  \\\n",
       "2837    -1.040434  0.953312  0.375384  0.774657          0               1   \n",
       "9925     0.454006 -0.095244 -0.349296  1.910540          1               1   \n",
       "8746     0.103585 -0.476537  1.100065  0.481608          0               1   \n",
       "660     -0.184996  0.190726  0.013044  0.088439          1               1   \n",
       "3610    -0.720933  1.620574 -1.436316  0.879129          1               0   \n",
       "\n",
       "      estimatedsalary  geography_Germany  geography_Spain  gender_Male  \\\n",
       "2837        -0.119110                  1                0            0   \n",
       "9925        -0.258658                  0                0            0   \n",
       "8746         1.422836                  0                0            1   \n",
       "660         -1.160427                  1                0            0   \n",
       "3610         0.113236                  0                0            0   \n",
       "\n",
       "      numofproducts_1  numofproducts_2  numofproducts_3  numofproducts_4  \n",
       "2837                1                0                0                0  \n",
       "9925                1                0                0                0  \n",
       "8746                0                1                0                0  \n",
       "660                 1                0                0                0  \n",
       "3610                1                0                0                0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#создадим объект структуры данных StandardScaler\n",
    "scaler = StandardScaler()\n",
    "#выберем признаки для стандартизации\n",
    "numeric = ['creditscore', 'age', 'tenure', 'balance', 'estimatedsalary']\n",
    "#сделаем копии выборок\n",
    "features_train = features_train.copy()\n",
    "features_valid = features_valid.copy()\n",
    "features_test = features_test.copy()\n",
    "#настроим на обучающей выборке\n",
    "scaler.fit(features_train.loc[:, numeric])\n",
    "#преобразуем обучающую и валидационную выборки\n",
    "features_train.loc[:, numeric] = scaler.transform(features_train.loc[:, numeric])\n",
    "features_valid.loc[:, numeric] = scaler.transform(features_valid.loc[:, numeric])\n",
    "features_test.loc[:, numeric] = scaler.transform(features_test.loc[:, numeric])\n",
    "features_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы подготовили данные:\n",
    "- воспользовались техникой One-Hot Encoding (OHE);\n",
    "- разделили данные на тренировочную, валидационную и тестовую выборки с разбиением по признакам и целевому признаку;\n",
    "- масштабировали данные методом стандартизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Исследование моделей классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе будут исследованы три модели задачи классификации: \n",
    "- логистическая регрессия;\n",
    "- решающее дерево;\n",
    "- случайный лес."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#посмотрим, насколько часто клиенты уходят из банка\n",
    "churn_up['exited'].value_counts(normalize=True).plot(kind='bar', title='Доля клиентов, ушедших из банка')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что клиентов, оставшихся в банке, примерно в 4 раза больше ушедших клиентов. Достаточно большая разница."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_f1 = make_scorer(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию для моделей с использованием GridSearchCV()\n",
    "def get_best_model(model, parameters, x_train, x_valid, y_train, y_valid):\n",
    "    grid = GridSearchCV(estimator=model, param_grid=parameters, cv=5, scoring=get_f1)\n",
    "    grid.fit(x_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    best_model.fit(x_train, y_train)\n",
    "    predicted_valid = best_model.predict(x_valid)\n",
    "    # вычислим значение полноты, точности и F1-меры нашей модели\n",
    "    accuracy = accuracy_score(y_valid, predicted_valid)\n",
    "    recall = recall_score(y_valid, predicted_valid)\n",
    "    precision = precision_score(y_valid, predicted_valid)\n",
    "    f1 = f1_score(y_valid, predicted_valid)\n",
    "    # вычислим значение AUC-ROC\n",
    "    probabilities_valid = best_model.predict_proba(x_valid)\n",
    "    probabilities_one_valid = probabilities_valid[:, 1]\n",
    "    auc_roc = roc_auc_score(y_valid, probabilities_one_valid)\n",
    "    print('Лучшие параметры модели:', grid.best_params_)\n",
    "    print()\n",
    "    print('Матрица ошибок:')\n",
    "    sns.heatmap(confusion_matrix(y_valid, predicted_valid), annot=True, fmt=\"d\")\n",
    "    plt.show()\n",
    "    print()\n",
    "    print()\n",
    "    df = pd.DataFrame(data=[[accuracy], \n",
    "                            [recall], \n",
    "                            [precision], \n",
    "                            [f1], \n",
    "                            [auc_roc]], \n",
    "                      index=['accuracy', 'recall', 'precision', 'f1', 'auc_roc'], \n",
    "                      columns=['Значения метрик'])\n",
    "    return best_model, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3-section'></a>\n",
    "### 1. Константная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определим точность предсказания константной модели\n",
    "def dummy_model(x_train, x_test, y_train, y_test, constant):\n",
    "    # константная модель, всегда предсказывающая значение по выбранной стратегии\n",
    "    dummy = DummyClassifier(strategy='constant', constant=constant, random_state=12345)\n",
    "    dummy.fit(x_train, y_train)\n",
    "    pred_test = dummy.predict(x_test)\n",
    "    # вычислим значение точности, полноты, F1-меры и AUC-ROC константной модели\n",
    "    accuracy = accuracy_score(y_test, pred_test)\n",
    "    recall = recall_score(y_test, pred_test)\n",
    "    precision = precision_score(y_test, pred_test)\n",
    "    f1 = f1_score(y_test, pred_test)\n",
    "    dummy_probabilities_test = dummy.predict_proba(x_test)\n",
    "    dummy_probabilities_one_test = dummy_probabilities_test[:, 1]\n",
    "    auc_roc = roc_auc_score(y_test, dummy_probabilities_one_test)\n",
    "    df = pd.DataFrame(data=[[accuracy], \n",
    "                            [recall], \n",
    "                            [precision], \n",
    "                            [f1], \n",
    "                            [auc_roc]], \n",
    "                      index=['accuracy', 'recall', 'precision', 'f1', 'auc_roc'], \n",
    "                      columns=['Значения метрик'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilyae\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.7965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.5000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик\n",
       "accuracy            0.7965\n",
       "recall              0.0000\n",
       "precision           0.0000\n",
       "f1                  0.0000\n",
       "auc_roc             0.5000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_0 = dummy_model(features_train, features_test, target_train, target_test, 0)\n",
    "dummy_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.20350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.20350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.33818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.50000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик\n",
       "accuracy           0.20350\n",
       "recall             1.00000\n",
       "precision          0.20350\n",
       "f1                 0.33818\n",
       "auc_roc            0.50000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_1 = dummy_model(features_train, features_test, target_train, target_test, 1)\n",
    "dummy_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшее значение f1-меры для константной модели: 0.34\n"
     ]
    }
   ],
   "source": [
    "print('Лучшее значение f1-меры для константной модели: {:.2f}'.format(dummy_1.loc['f1','Значения метрик']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4-section'></a>\n",
    "### 2. Дисбаланс классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели: {'C': 15, 'random_state': 12345}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wall time: 13 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.409314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.690083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.513846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.840659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик\n",
       "accuracy          0.842000\n",
       "recall            0.409314\n",
       "precision         0.690083\n",
       "f1                0.513846\n",
       "auc_roc           0.840659"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# создадим словарь для логистической регрессии\n",
    "param_log_reg = {\n",
    "    'C'                 : list(range(1, 200, 2)),\n",
    "    'random_state'      : [12345],\n",
    "}\n",
    "# используем функцию get_best_model()\n",
    "log_reg_model, log_reg = get_best_model(LogisticRegression(solver='liblinear'), \n",
    "                                        param_log_reg, \n",
    "                                        features_train, \n",
    "                                        features_valid, \n",
    "                                        target_train, \n",
    "                                        target_valid)\n",
    "log_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели: {'max_depth': 7, 'min_samples_leaf': 1, 'min_samples_split': 2, 'random_state': 12345}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wall time: 3min 13s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.524510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.688103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.595271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.830639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик\n",
       "accuracy          0.854500\n",
       "recall            0.524510\n",
       "precision         0.688103\n",
       "f1                0.595271\n",
       "auc_roc           0.830639"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# создадим словарь для решающего дерева\n",
    "param_dec_tree = {\n",
    "    'max_depth'         : list(range(1, 50, 2)),\n",
    "    'min_samples_split' : list(range(2, 10, 1)),\n",
    "    'min_samples_leaf'  : list(range(1, 10, 1)), \n",
    "    'random_state'      : [12345],\n",
    "}\n",
    "# используем функцию get_best_model()\n",
    "dec_tree_model, dec_tree = get_best_model(DecisionTreeClassifier(), \n",
    "                                          param_dec_tree, \n",
    "                                          features_train, \n",
    "                                          features_valid, \n",
    "                                          target_train, \n",
    "                                          target_valid)\n",
    "dec_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели: {'max_depth': 16, 'n_estimators': 71, 'random_state': 12345}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAD4CAYAAAAw/yevAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAX2klEQVR4nO3de5xVdb3/8dcbRgWsBEQRBxQvlKmlx+OtY6aJFzAT/CWJphLSmY73k2naxUjThOon6S8zSUhMBQk1UFHi4IVjBmqaqHhhRA+OoGigpR6Vmfn8/pgFbWAz1z2zv7N8P3l8H7PXd333Xt+l8OHDZ333WooIzMwsLV3KPQEzM9uYg7OZWYIcnM3MEuTgbGaWIAdnM7MEVbT3Ada8udTLQWwj3bc/uNxTsATVfviq2voZLYk5m/XZuc3Hay/OnM3MEtTumbOZWYeqryv3DErCwdnM8qWuttwzKAkHZzPLlYj6ck+hJByczSxf6h2czczS48zZzCxBviBoZpYgZ85mZukJr9YwM0uQLwiamSXIZQ0zswT5gqCZWYKcOZuZJcgXBM3MEuQLgmZm6YnIR83Z93M2s3yJ+ua3JkiaLGmlpKeL7DtfUkjqk21L0tWSqiUtkrRPwdhRkpZkbVRzTsPB2czypb6++a1pNwBDNuyUNAA4AlhW0D0UGJS1KuDabGxvYCxwALA/MFZSr6YO7OBsZvlSwsw5IuYDq4rsmgB8Byh8JNYw4MZosADoKakfcBQwNyJWRcRqYC5FAv6GXHM2s3ypW9PsoZKqaMhy15oYERObeM+xwKsR8aS03iMIK4FXCrZrsr5N9TfKwdnM8qUFqzWyQNxoMC4kqQfwfeDIYruLHaKR/ka5rGFm+VLCskYRuwA7AU9KehnoDzwuaTsaMuIBBWP7A8sb6W+Ug7OZ5UtpLwiuJyKeiohtI2JgRAykIfDuExGvAbOAU7NVGwcCb0fECmAOcKSkXtmFwCOzvka5rGFm+VLCL6FImgocCvSRVAOMjYhJmxg+GzgaqAbeA0YDRMQqST8GHs3GXRoRxS4yrsfB2cxyJVpwQbDJz4o4sYn9AwteB3DmJsZNBia35NgOzmaWL77xkZlZgnxvDTOzBDlzNjNLkDNnM7MEOXM2M0tQrW+2b2aWHmfOZmYJcs3ZzCxBzpzNzBLkzNnMLEHOnM3MEuTVGmZmCYom72PfKTg4m1m+uOZsZpYgB2czswT5gqCZWYLq6so9g5JwcDazfHFZw8wsQQ7OZmYJyknNuUu5J2BmVkpRH81uTZE0WdJKSU8X9P1M0nOSFkm6Q1LPgn3flVQt6XlJRxX0D8n6qiVd1JzzcHA2s3ypr29+a9oNwJAN+uYCe0bEZ4EXgO8CSNodGAnskb3nV5K6SuoKXAMMBXYHTszGNsplDTPLlxKu1oiI+ZIGbtD3x4LNBcDx2ethwLSI+AB4SVI1sH+2rzoilgJImpaNXdzYsZ05m1m+lDZzbsppwD3Z60rglYJ9NVnfpvob5eBsZvnSguAsqUrSYwWtqrmHkfR9oBa4eW1XkWHRSH+jXNZoox/85Erm/+kRevfqyR9u+jUA10y6idtm3UuvnlsBcO43R/GFf9ufpxY/z4/GXw1AEJxx2tc4/JCD1n1WXV0dJ4w5h2236cOvfnZJx5+MdYjqFxbwj3feoa6untraWg783NHstdce/OqX49ii2xbU1tZy9tnf49HH/lruqXZOLbjxUURMBCa29BCSRgHHAIMj1h2wBhhQMKw/sDx7van+TXJwbqPhRx/BSV85lu/9+Ofr9Z9ywnBGn3T8en277rwjt066moqKrrzx5iq+MuoMDj3oQCoqugJw0+9nsvPAHXjn3fc6bP5WHocfMYK//W31uu1xP/k+P77sSu6dcz9DhxzGuCu+z+AjRpRxhp1YO69zljQEuBA4JCIK/7DOAm6RdCWwPTAIeISGzHmQpJ2AV2m4aHhSU8dpMjhL2o2G4nUlDan4cmBWRDzbojPKqX33/gyvrni9WWO7d+u27vUHH34I+ue/dl5b+QbzH36EqlEjmTLtjpLP09IWEXz8Ex8H4BNbfZzlzfw9ZUU0Y4lcc0maChwK9JFUA4ylYXXGFsBcNfwZXhAR/xERz0iaTsOFvlrgzIioyz7nLGAO0BWYHBHPNHXsRoOzpAuBE4FpNPwNAA0p+VRJ0yJiXEtP9qNi6m13Muveeeyx2yAuOOvf2Sr7g7fomee4+CcTWP76Sq64+Px1WfP4q67jvDPG8O57/1vOaVsHiAjumT2ViOA3v7mJ6yfdzHnnj2X2Xbfw03EX06WLOPiQYeWeZudV2tUaJxbpntTI+MuBy4v0zwZmt+TYTV0QHAPsFxHjIuKmrI2jYXnImE29qbDIfv2NU1syn1w44bgvcc/0ydx2wzVss3VvfvbL36zb99k9dmPmzdcx7fqruP530/nggw954E8L6d2rJ3vsNqiMs7aO8oVDh7P/AUM45ssnc/rpX+fgzx/AN6tO5dsX/IiddtmPb19wCb+57v+We5qdVtTXN7ulrKngXE9D7WRD/bJ9RUXExIjYNyL2/capxf7iybc+vXvRtWtXunTpwvHHDuXpxS9sNGaXgTvQvVs3lix9mScWLeaBhxZw5FdGccHYcTzylye58JKflmHm1hFWZCWLN974GzNn3sN+++3NqaeM4I47GhKrGTPuZL/99i7nFDu3+mh+S1hTNef/BOZJWsI/1+ntAOwKnNWeE+vM3nhzFdv06Q3AvAcfZteddwSgZvlrbLftNlRUdGX5a6/z8rIaKvv15Vunj+Zbp48G4JHHF3HD1NsYP/Y7ZZu/tZ8ePbrTpUsX3nnnXXr06M4Rhx/CZZdPYPmK1znkC5/jwfl/5rAvfp4l1S+Ve6qdV07urdFocI6IeyV9koYyRiUNVx1rgEfXFro/6i4YO45Hn1jEW2/9ncHDT+aMMafw6BOLeH7JUhBUbteXsd85B4DHFz3DpN9Np6Kigi5dxA/OP3Pdcjv7aOjbdxtm/L6hZFlR0ZVp0/7AnD8+wDv/cQFXXnkpFRUVfPD++5x+uv9ybrXEM+LmUrTzwxDXvLk0H/+lrKS6b39wuadgCar98NViX9hokXd/OLLZMWfLS6e1+XjtxeuczSxfPgplDTOzTicnZQ0HZzPLldSXyDWXg7OZ5YszZzOzBDk4m5klqIRf3y4nB2czy5XmPBuwM3BwNrN8cXA2M0uQV2uYmSXImbOZWYIcnM3M0hN1LmuYmaXHmbOZWXq8lM7MLEUOzmZmCcpHybnJZwiamXUqUVvf7NYUSZMlrZT0dEFfb0lzJS3JfvbK+iXpaknVkhZJ2qfgPaOy8UskjWrOeTg4m1m+1LegNe0GYMgGfRcB8yJiEDAv2wYYCgzKWhVwLTQEc2AscAANj/wbuzagN8bB2cxyJeqj2a3Jz4qYD6zaoHsYMCV7PQUYXtB/YzRYAPSU1A84CpgbEasiYjUwl40D/kYcnM0sX1qQOUuqkvRYQatqxhH6RsQKgOzntll/JfBKwbiarG9T/Y3yBUEzy5WWLKWLiInAxBIdutjDYqOR/kY5czazfCltzbmY17NyBdnPlVl/DTCgYFx/YHkj/Y1ycDazXIna5rdWmgWsXXExCphZ0H9qtmrjQODtrOwxBzhSUq/sQuCRWV+jXNYws1yJEq5zljQVOBToI6mGhlUX44DpksYAy4AR2fDZwNFANfAeMBogIlZJ+jHwaDbu0ojY8CLjRhyczSxfShicI+LETewaXGRsAGdu4nMmA5NbcmwHZzPLlVJmzuXk4GxmueLgbGaWoKgrtnKt83FwNrNcceZsZpagqHfmbGaWHGfOZmYJinDmbGaWHGfOZmYJqvdqDTOz9PiCoJlZghyczcwSFPl4+LaDs5nlizNnM7MEeSmdmVmC6rxaw8wsPc6czcwS5JqzmVmCvFrDzCxBzpzNzBJUV9+l3FMoiXychZlZJqL5rSmSviXpGUlPS5oqqZuknSQtlLRE0q2SNs/GbpFtV2f7B7blPByczSxX6kPNbo2RVAmcA+wbEXsCXYGRwHhgQkQMAlYDY7K3jAFWR8SuwIRsXKs5OJtZrkSo2a0ZKoDukiqAHsAK4DBgRrZ/CjA8ez0s2ybbP1hSqwvgDs5mlistKWtIqpL0WEGr+ufnxKvAz4FlNATlt4G/AG9FRG02rAaozF5XAq9k763Nxm/d2vNo9wuCu396RHsfwjqhXXtuX+4pWE41Va4oFBETgYnF9knqRUM2vBPwFvB7YGixj1n7lkb2tZhXa5hZrpRwtcbhwEsR8QaApNuBfwN6SqrIsuP+wPJsfA0wAKjJyiBbAatae3CXNcwsV6IFrQnLgAMl9chqx4OBxcD9wPHZmFHAzOz1rGybbP99Ea3/SowzZzPLlZaUNRoTEQslzQAeB2qBJ2gogdwNTJN0WdY3KXvLJOB3kqppyJhHtuX4Ds5mliulvPFRRIwFxm7QvRTYv8jY94GSXWRzcDazXMnJw7cdnM0sX6LooonOx8HZzHKl1vdzNjNLjzNnM7MEueZsZpYgZ85mZgly5mxmlqA6Z85mZunJyVOqHJzNLF/qnTmbmaUnJw/fdnA2s3zxBUEzswTVt/7JUElxcDazXKkr9wRKxMHZzHLFqzXMzBLk1RpmZgnyag0zswS5rGFmliAvpTMzS1CdM2czs/TkJXPuUu4JmJmVUn0LWlMk9ZQ0Q9Jzkp6V9DlJvSXNlbQk+9krGytJV0uqlrRI0j5tOQ8HZzPLlVDzWzNcBdwbEbsBewHPAhcB8yJiEDAv2wYYCgzKWhVwbVvOw8HZzHKlVJmzpE8AXwAmAUTEhxHxFjAMmJINmwIMz14PA26MBguAnpL6tfY8HJzNLFfqWtAkVUl6rKBVFXzUzsAbwG8lPSHpeklbAn0jYgVA9nPbbHwl8ErB+2uyvlbxBUEzy5WWrHOOiInAxE3srgD2Ac6OiIWSruKfJYxiih251d+JceZsZrlSwguCNUBNRCzMtmfQEKxfX1uuyH6uLBg/oOD9/YHlrT0PB2czy5VSBeeIeA14RdKnsq7BwGJgFjAq6xsFzMxezwJOzVZtHAi8vbb80Roua5hZrpT43hpnAzdL2hxYCoymIamdLmkMsAwYkY2dDRwNVAPvZWNbzcHZzHKllPfWiIi/AvsW2TW4yNgAzizVsR2czSxXfLN9M7ME1efkpqEOzmaWK3m5t4aDs5nlSj7yZgdnM8sZZ85mZgmqVT5yZwdnM8uVfIRmB2czyxmXNczMEuSldGZmCcpHaHZwNrOccVnDzCxBdTnJnR2czSxXnDmbmSUonDmbmaUnL5mzn4RSQldc9UMWLJ7L3fNvXde32x6DmD77t9z14K1cd9MEPvaxLQGoqKhg/C8v4a4Hb+XeP83gm+e26b7clqjttu/LlNuv5e6HpnPn/Fs55d9HAnDUlwdz5/xbWfzaQvbc69PrvafqnK8zZ+Ht3PPwDD7/xQPLMe1OrZ5odkuZg3MJ3T7tTk4befZ6fZdPuJifX/b/OOaQE5g7+36+cdapAAw99nA233wzjjnkBIYffjIjT/0/VA5o9VPULVF1tbWMH/sLvvT5rzJy6Gi+dtrx7PLJnVjy3IucM/o7PPbnJ9Ybv8snd+Lo447gmINP4Bsjz+GH4y+kSxf/MW2JaEFLmf+vl9Cjf36Ct1e/vV7fzrvuyCMPPw7AQw8s5KhjDgMgIujRoztdu3alW7ctWLNmDe/8490On7O1rzdW/o3FTz0PwLvvvseLL7xM337bsHTJy7z04v9sNH7wkEOYfcdc1ny4hleXLWfZS6/w2X326Ohpd2q1RLNbyhyc29kLz77I4CGHAA3Z8naVfQG49855vPfe//Lw03N48Im7mXTN73j7rb+Xc6rWzioH9OPTn/kUT/7lmU2O6dtvG1Ysf33d9mvLV9J3u206Ynq5ES34lbJWB2dJmyySSqqS9Jikx95+/83WHiIXvnvupZx82le5479uYsuP9WDNh2sA+Ow+e1BfV89BnxnCF/f9MqedcTIDdqws82ytvfTYsjtXTx7PFRdfybvvNPIvJG38ALzUg0hqSvX07XJrS+Z8yaZ2RMTEiNg3IvbdqlufNhyi81ta/TKjv3omxx1+MnfdPodlL9cA8OWvDGH+fQ9TW1vLqjdX8/gjT7Ln3ruXebbWHioqunL15PHcedu9zL37/kbHvr58Jf2277tue7vtt2Xlax/tBKelSp05S+oq6QlJd2XbO0laKGmJpFuzJ3MjaYtsuzrbP7At59FocJa0aBPtKaBvY++1Br379AJAEmecN4ZpU24DYEXNa3zu4P0A6N6jG3v/62dYuuSlss3T2s9lv7iYF194mRt+fUuTY++bM5+jjzuCzTbfjModtmfHnXdg0eObLoPYxtohcz4XeLZgezwwISIGAauBMVn/GGB1ROwKTMjGtVpT65z7AkdlEygk4OG2HDiPJlx3OfsftC+9evfkv5+czVU/vY4tt+zB104bAcAf776fGbfMAuCmydMZd/WPmP3f05HEbVNn8fzi6nJO39rBPgfsxfCvfonnFy/hjvtuBmDC5dew+Rab84OfnE/vrXvx61sm8NzTL/CNE86h+vml3DPzv7j7oenU1dZx6YU/pb4+9X+Ap6UuSlcGktQf+BJwOXCeJAGHASdlQ6YAPwKuBYZlrwFmAL+UpIjWTUiNvU/SJOC3EfFQkX23RMRJRd62nkHb/KsLZraRrvK1aNvYcysf3bjo3kIn7Xhcs2PO1GV/+CZQVdA1MSImrt2QNAO4Avg4cD7wdWBBlh0jaQBwT0TsKelpYEhE1GT7XgQOiIhW1aUazZwjYkwj+5oMzGZmHa0lF1CzQDyx2D5JxwArI+Ivkg5d2130kE3vazF/fdvMcqWERaCDgGMlHQ10Az4B/ALoKakiImqB/sDybHwNMACokVQBbAWsau3B/W9LM8uVUn19OyK+GxH9I2IgMBK4LyK+BtwPHJ8NGwXMzF7PyrbJ9t/X2nozODibWc50wJdQLqTh4mA1sDUwKeufBGyd9Z8HXNSW83BZw8xypZSrNdaKiAeAB7LXS4H9i4x5HxhRqmM6OJtZrqR+t7nmcnA2s1zJy6pwB2czy5W83IvEwdnMcsVlDTOzBLVh9VpSHJzNLFfqnDmbmaXHZQ0zswS5rGFmliBnzmZmCfJSOjOzBLXH17fLwcHZzHLFZQ0zswQ5OJuZJcirNczMEuTM2cwsQV6tYWaWoLrIx01DHZzNLFdcczYzS5BrzmZmCXLN2cwsQfU5KWt0KfcEzMxKKVrwqzGSBki6X9Kzkp6RdG7W31vSXElLsp+9sn5JulpStaRFkvZpy3k4OJtZrtRFfbNbE2qBb0fEp4EDgTMl7Q5cBMyLiEHAvGwbYCgwKGtVwLVtOQ8HZzPLlfqIZrfGRMSKiHg8e/0P4FmgEhgGTMmGTQGGZ6+HATdGgwVAT0n9WnseDs5mlistKWtIqpL0WEGrKvaZkgYC/wIsBPpGxApoCODAttmwSuCVgrfVZH2t4guCZpYrLbkgGBETgYmNjZH0MeA24D8j4u+SNjm02CGaPZkNOHM2s1wp1QVBAEmb0RCYb46I27Pu19eWK7KfK7P+GmBAwdv7A8tbex4OzmaWK3VR1+zWGDWkyJOAZyPiyoJds4BR2etRwMyC/lOzVRsHAm+vLX+0hssaZpYrJfz69kHAKcBTkv6a9X0PGAdMlzQGWAaMyPbNBo4GqoH3gNFtObiDs5nlSqm+vh0RD1G8jgwwuMj4AM4sycFxcDaznPGNj8zMEpSXr287OJtZrvjGR2ZmCfLN9s3MEuSas5lZglxzNjNLkDNnM7ME+TFVZmYJcuZsZpYgr9YwM0uQLwiamSXIZQ0zswT5G4JmZgly5mxmlqC81JyVl79lOgNJVdkzy8zW8e8LK8aPqepYRZ/sax95/n1hG3FwNjNLkIOzmVmCHJw7luuKVox/X9hGfEHQzCxBzpzNzBLk4GxmliAH5w4iaYik5yVVS7qo3POx8pM0WdJKSU+Xey6WHgfnDiCpK3ANMBTYHThR0u7lnZUl4AZgSLknYWlycO4Y+wPVEbE0Ij4EpgHDyjwnK7OImA+sKvc8LE0Ozh2jEnilYLsm6zMzK8rBuWOoSJ/XMJrZJjk4d4waYEDBdn9geZnmYmadgINzx3gUGCRpJ0mbAyOBWWWek5klzMG5A0RELXAWMAd4FpgeEc+Ud1ZWbpKmAn8GPiWpRtKYcs/J0uGvb5uZJciZs5lZghyczcwS5OBsZpYgB2czswQ5OJuZJcjB2cwsQQ7OZmYJ+v/oIsM7HN3YaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wall time: 2min 7s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.514706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.783582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.621302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.862280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик\n",
       "accuracy          0.872000\n",
       "recall            0.514706\n",
       "precision         0.783582\n",
       "f1                0.621302\n",
       "auc_roc           0.862280"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# создадим словарь для случайного леса\n",
    "param_rand_for = {\n",
    "    'n_estimators'      : list(range(1, 100, 10)),\n",
    "    'max_depth'         : list(range(1, 51, 5)), \n",
    "    'random_state'      : [12345],\n",
    "}\n",
    "# используем функцию get_best_model()\n",
    "rand_for_model, rand_for = get_best_model(RandomForestClassifier(), \n",
    "                                          param_rand_for,\n",
    "                                          features_train, \n",
    "                                          features_valid, \n",
    "                                          target_train, \n",
    "                                          target_valid)\n",
    "rand_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели: {'max_depth': 21, 'n_estimators': 41}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wall time: 1min 23s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.534314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.719472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.613221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.853863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик\n",
       "accuracy          0.862500\n",
       "recall            0.534314\n",
       "precision         0.719472\n",
       "f1                0.613221\n",
       "auc_roc           0.853863"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# создадим словарь для xgboost\n",
    "param_xgb = {\n",
    "    \"n_estimators\": list(range(1, 101, 10)),\n",
    "    \"max_depth\": list(range(1, 51, 10)),\n",
    "}\n",
    "# используем функцию get_best_model()\n",
    "xgb_model, df_xgb = get_best_model(\n",
    "    xgb.XGBClassifier(learning_rate=0.05, random_state=12345),\n",
    "    param_xgb,\n",
    "    features_train,\n",
    "    features_valid,\n",
    "    target_train,\n",
    "    target_valid,\n",
    ")\n",
    "df_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000454 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000584 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000644 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000327 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000448 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000408 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000509 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000537 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000842 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000400 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000384 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000552 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000464 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000445 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000473 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000428 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000498 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000268 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000447 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000427 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000569 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000370 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000194 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000574 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000347 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000457 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000388 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000459 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000494 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000318 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000557 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000331 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000330 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000416 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000461 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000390 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000476 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000640 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000577 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000489 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000436 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000417 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000462 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000351 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000455 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000441 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000468 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000500 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000502 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000432 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000424 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203750 -> initscore=-1.363019\n",
      "[LightGBM] [Info] Start training from score -1.363019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000440 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 857\n",
      "[LightGBM] [Info] Number of data points in the train set: 4800, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203542 -> initscore=-1.364304\n",
      "[LightGBM] [Info] Start training from score -1.364304\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 1222, number of negative: 4778\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 6000, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203667 -> initscore=-1.363533\n",
      "[LightGBM] [Info] Start training from score -1.363533\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 1222, number of negative: 4778\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 6000, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.203667 -> initscore=-1.363533\n",
      "[LightGBM] [Info] Start training from score -1.363533\n",
      "Лучшие параметры модели: {'max_depth': 11, 'n_estimators': 91}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wall time: 20.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.502451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.753676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.602941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.879171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик\n",
       "accuracy          0.865000\n",
       "recall            0.502451\n",
       "precision         0.753676\n",
       "f1                0.602941\n",
       "auc_roc           0.879171"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# создадим словарь для lightgbm\n",
    "param_lgb = {\n",
    "    \"n_estimators\": list(range(1, 101, 10)),\n",
    "    \"max_depth\"   : list(range(1, 51, 10)),\n",
    "}\n",
    "# используем функцию get_best_model()\n",
    "lgb_model, df_lgb = get_best_model(\n",
    "    lgb.LGBMClassifier(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"binary\",\n",
    "        learning_rate=0.05,\n",
    "        feature_fraction=0.9,\n",
    "        random_state=12345,\n",
    "        silent=False,\n",
    "    ),\n",
    "    param_lgb,\n",
    "    features_train,\n",
    "    features_valid,\n",
    "    target_train,\n",
    "    target_valid,\n",
    ")\n",
    "df_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6731996\ttotal: 68.3ms\tremaining: 6.76s\n",
      "50:\tlearn: 0.4210790\ttotal: 134ms\tremaining: 129ms\n",
      "99:\tlearn: 0.3872363\ttotal: 209ms\tremaining: 0us\n",
      "0:\tlearn: 0.6733710\ttotal: 1.24ms\tremaining: 123ms\n",
      "50:\tlearn: 0.4231286\ttotal: 77ms\tremaining: 74ms\n",
      "99:\tlearn: 0.3909143\ttotal: 156ms\tremaining: 0us\n",
      "0:\tlearn: 0.6733186\ttotal: 1.64ms\tremaining: 163ms\n",
      "50:\tlearn: 0.4263487\ttotal: 79.1ms\tremaining: 76ms\n",
      "99:\tlearn: 0.3941888\ttotal: 166ms\tremaining: 0us\n",
      "0:\tlearn: 0.6732922\ttotal: 1.63ms\tremaining: 161ms\n",
      "50:\tlearn: 0.4202649\ttotal: 75.2ms\tremaining: 72.2ms\n",
      "99:\tlearn: 0.3894061\ttotal: 148ms\tremaining: 0us\n",
      "0:\tlearn: 0.6735490\ttotal: 1.56ms\tremaining: 155ms\n",
      "50:\tlearn: 0.4225782\ttotal: 77.2ms\tremaining: 74.2ms\n",
      "99:\tlearn: 0.3898163\ttotal: 190ms\tremaining: 0us\n",
      "0:\tlearn: 0.6699468\ttotal: 2.81ms\tremaining: 278ms\n",
      "50:\tlearn: 0.3748498\ttotal: 108ms\tremaining: 104ms\n",
      "99:\tlearn: 0.3477796\ttotal: 200ms\tremaining: 0us\n",
      "0:\tlearn: 0.6701703\ttotal: 1.7ms\tremaining: 168ms\n",
      "50:\tlearn: 0.3770784\ttotal: 88.1ms\tremaining: 84.7ms\n",
      "99:\tlearn: 0.3509057\ttotal: 188ms\tremaining: 0us\n",
      "0:\tlearn: 0.6701605\ttotal: 2.04ms\tremaining: 202ms\n",
      "50:\tlearn: 0.3798596\ttotal: 95.7ms\tremaining: 92ms\n",
      "99:\tlearn: 0.3525680\ttotal: 183ms\tremaining: 0us\n",
      "0:\tlearn: 0.6700154\ttotal: 1.87ms\tremaining: 185ms\n",
      "50:\tlearn: 0.3767255\ttotal: 97ms\tremaining: 93.2ms\n",
      "99:\tlearn: 0.3501410\ttotal: 187ms\tremaining: 0us\n",
      "0:\tlearn: 0.6689242\ttotal: 2.27ms\tremaining: 224ms\n",
      "50:\tlearn: 0.3761504\ttotal: 110ms\tremaining: 106ms\n",
      "99:\tlearn: 0.3480204\ttotal: 214ms\tremaining: 0us\n",
      "0:\tlearn: 0.6693536\ttotal: 3.38ms\tremaining: 335ms\n",
      "50:\tlearn: 0.3586034\ttotal: 177ms\tremaining: 170ms\n",
      "99:\tlearn: 0.3324082\ttotal: 336ms\tremaining: 0us\n",
      "0:\tlearn: 0.6698723\ttotal: 2.66ms\tremaining: 263ms\n",
      "50:\tlearn: 0.3626599\ttotal: 143ms\tremaining: 138ms\n",
      "99:\tlearn: 0.3357608\ttotal: 273ms\tremaining: 0us\n",
      "0:\tlearn: 0.6699002\ttotal: 3.44ms\tremaining: 341ms\n",
      "50:\tlearn: 0.3656973\ttotal: 140ms\tremaining: 135ms\n",
      "99:\tlearn: 0.3379827\ttotal: 255ms\tremaining: 0us\n",
      "0:\tlearn: 0.6691699\ttotal: 5.05ms\tremaining: 500ms\n",
      "50:\tlearn: 0.3619296\ttotal: 142ms\tremaining: 137ms\n",
      "99:\tlearn: 0.3344143\ttotal: 261ms\tremaining: 0us\n",
      "0:\tlearn: 0.6677829\ttotal: 2.54ms\tremaining: 252ms\n",
      "50:\tlearn: 0.3609413\ttotal: 119ms\tremaining: 114ms\n",
      "99:\tlearn: 0.3340925\ttotal: 235ms\tremaining: 0us\n",
      "0:\tlearn: 0.6690219\ttotal: 3.77ms\tremaining: 373ms\n",
      "50:\tlearn: 0.3516816\ttotal: 177ms\tremaining: 170ms\n",
      "99:\tlearn: 0.3227064\ttotal: 354ms\tremaining: 0us\n",
      "0:\tlearn: 0.6702075\ttotal: 3.66ms\tremaining: 363ms\n",
      "50:\tlearn: 0.3542935\ttotal: 187ms\tremaining: 179ms\n",
      "99:\tlearn: 0.3239071\ttotal: 368ms\tremaining: 0us\n",
      "0:\tlearn: 0.6689947\ttotal: 3.68ms\tremaining: 364ms\n",
      "50:\tlearn: 0.3581427\ttotal: 199ms\tremaining: 191ms\n",
      "99:\tlearn: 0.3275855\ttotal: 418ms\tremaining: 0us\n",
      "0:\tlearn: 0.6691699\ttotal: 3.14ms\tremaining: 311ms\n",
      "50:\tlearn: 0.3526125\ttotal: 219ms\tremaining: 211ms\n",
      "99:\tlearn: 0.3242185\ttotal: 430ms\tremaining: 0us\n",
      "0:\tlearn: 0.6677829\ttotal: 2.7ms\tremaining: 267ms\n",
      "50:\tlearn: 0.3533395\ttotal: 188ms\tremaining: 181ms\n",
      "99:\tlearn: 0.3242250\ttotal: 369ms\tremaining: 0us\n",
      "0:\tlearn: 0.6696424\ttotal: 9.4ms\tremaining: 931ms\n",
      "50:\tlearn: 0.3494795\ttotal: 399ms\tremaining: 383ms\n",
      "99:\tlearn: 0.3102616\ttotal: 803ms\tremaining: 0us\n",
      "0:\tlearn: 0.6711677\ttotal: 8.18ms\tremaining: 810ms\n",
      "50:\tlearn: 0.3504385\ttotal: 411ms\tremaining: 395ms\n",
      "99:\tlearn: 0.3110162\ttotal: 803ms\tremaining: 0us\n",
      "0:\tlearn: 0.6702805\ttotal: 9.83ms\tremaining: 973ms\n",
      "50:\tlearn: 0.3526472\ttotal: 408ms\tremaining: 392ms\n",
      "99:\tlearn: 0.3158699\ttotal: 820ms\tremaining: 0us\n",
      "0:\tlearn: 0.6691699\ttotal: 2.77ms\tremaining: 275ms\n",
      "50:\tlearn: 0.3485872\ttotal: 388ms\tremaining: 373ms\n",
      "99:\tlearn: 0.3118908\ttotal: 771ms\tremaining: 0us\n",
      "0:\tlearn: 0.6677829\ttotal: 2.77ms\tremaining: 275ms\n",
      "50:\tlearn: 0.3490631\ttotal: 423ms\tremaining: 406ms\n",
      "99:\tlearn: 0.3109815\ttotal: 882ms\tremaining: 0us\n",
      "0:\tlearn: 0.6736908\ttotal: 31.8ms\tremaining: 3.15s\n",
      "50:\tlearn: 0.3458045\ttotal: 1.5s\tremaining: 1.44s\n",
      "99:\tlearn: 0.2974097\ttotal: 3.07s\tremaining: 0us\n",
      "0:\tlearn: 0.6759045\ttotal: 31ms\tremaining: 3.07s\n",
      "50:\tlearn: 0.3464356\ttotal: 1.52s\tremaining: 1.46s\n",
      "99:\tlearn: 0.2947579\ttotal: 3.05s\tremaining: 0us\n",
      "0:\tlearn: 0.6748586\ttotal: 31.3ms\tremaining: 3.1s\n",
      "50:\tlearn: 0.3497249\ttotal: 1.51s\tremaining: 1.45s\n",
      "99:\tlearn: 0.2975415\ttotal: 3.25s\tremaining: 0us\n",
      "0:\tlearn: 0.6691699\ttotal: 2.81ms\tremaining: 278ms\n",
      "50:\tlearn: 0.3461249\ttotal: 1.44s\tremaining: 1.38s\n",
      "99:\tlearn: 0.2975554\ttotal: 2.92s\tremaining: 0us\n",
      "0:\tlearn: 0.6677829\ttotal: 2.96ms\tremaining: 293ms\n",
      "50:\tlearn: 0.3454765\ttotal: 1.64s\tremaining: 1.58s\n",
      "99:\tlearn: 0.2951659\ttotal: 3.1s\tremaining: 0us\n",
      "0:\tlearn: 0.6778673\ttotal: 126ms\tremaining: 12.5s\n",
      "50:\tlearn: 0.3496539\ttotal: 5.44s\tremaining: 5.23s\n",
      "99:\tlearn: 0.2839526\ttotal: 10.9s\tremaining: 0us\n",
      "0:\tlearn: 0.6803773\ttotal: 128ms\tremaining: 12.6s\n",
      "50:\tlearn: 0.3527137\ttotal: 5.43s\tremaining: 5.22s\n",
      "99:\tlearn: 0.2825382\ttotal: 12.1s\tremaining: 0us\n",
      "0:\tlearn: 0.6793466\ttotal: 163ms\tremaining: 16.1s\n",
      "50:\tlearn: 0.3518158\ttotal: 5.33s\tremaining: 5.12s\n",
      "99:\tlearn: 0.2856878\ttotal: 11.3s\tremaining: 0us\n",
      "0:\tlearn: 0.6691699\ttotal: 3.24ms\tremaining: 321ms\n",
      "50:\tlearn: 0.3493043\ttotal: 5.28s\tremaining: 5.07s\n",
      "99:\tlearn: 0.2838386\ttotal: 11.2s\tremaining: 0us\n",
      "0:\tlearn: 0.6677829\ttotal: 3.06ms\tremaining: 303ms\n",
      "50:\tlearn: 0.3499019\ttotal: 5.52s\tremaining: 5.3s\n",
      "99:\tlearn: 0.2851884\ttotal: 11s\tremaining: 0us\n",
      "0:\tlearn: 0.6826640\ttotal: 558ms\tremaining: 55.2s\n",
      "50:\tlearn: 0.3549031\ttotal: 17.9s\tremaining: 17.2s\n",
      "99:\tlearn: 0.2794058\ttotal: 44.1s\tremaining: 0us\n",
      "0:\tlearn: 0.6844156\ttotal: 557ms\tremaining: 55.2s\n",
      "50:\tlearn: 0.3613209\ttotal: 22.6s\tremaining: 21.8s\n",
      "99:\tlearn: 0.2790402\ttotal: 45.7s\tremaining: 0us\n",
      "0:\tlearn: 0.6836882\ttotal: 524ms\tremaining: 51.9s\n",
      "50:\tlearn: 0.3601925\ttotal: 21.1s\tremaining: 20.2s\n",
      "99:\tlearn: 0.2819178\ttotal: 44.3s\tremaining: 0us\n",
      "0:\tlearn: 0.6691699\ttotal: 2.54ms\tremaining: 251ms\n",
      "50:\tlearn: 0.3573656\ttotal: 21.1s\tremaining: 20.3s\n",
      "99:\tlearn: 0.2794639\ttotal: 44.9s\tremaining: 0us\n",
      "0:\tlearn: 0.6677829\ttotal: 2.66ms\tremaining: 263ms\n",
      "50:\tlearn: 0.3587596\ttotal: 19.2s\tremaining: 18.4s\n",
      "99:\tlearn: 0.2805713\ttotal: 41.1s\tremaining: 0us\n",
      "0:\tlearn: 0.6747213\ttotal: 32.9ms\tremaining: 3.26s\n",
      "50:\tlearn: 0.3445204\ttotal: 1.6s\tremaining: 1.54s\n",
      "99:\tlearn: 0.3005087\ttotal: 3.24s\tremaining: 0us\n",
      "0:\tlearn: 0.6747213\ttotal: 32.3ms\tremaining: 3.2s\n",
      "50:\tlearn: 0.3445204\ttotal: 1.59s\tremaining: 1.53s\n",
      "99:\tlearn: 0.3005087\ttotal: 3.57s\tremaining: 0us\n",
      "Лучшие параметры модели: {'depth': 11}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wall time: 5min 13s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.477941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.776892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.591806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.874340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик\n",
       "accuracy          0.865500\n",
       "recall            0.477941\n",
       "precision         0.776892\n",
       "f1                0.591806\n",
       "auc_roc           0.874340"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# создадим словарь для catboost\n",
    "param_ctb = {\n",
    "    \"depth\": list(range(1, 17, 2)),\n",
    "}\n",
    "# используем функцию get_best_model()\n",
    "ctb_model, df_ctb = get_best_model(\n",
    "    ctb.CatBoostClassifier(iterations=100, learning_rate=0.05, verbose=50),\n",
    "    param_ctb,\n",
    "    features_train,\n",
    "    features_valid,\n",
    "    target_train,\n",
    "    target_valid,\n",
    ")\n",
    "df_ctb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='5-section'></a>\n",
    "### 3. Баланс классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим данные, чтобы они стали более сбалансированными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# используем технику upsampling увеличения выборки\n",
    "def upsample(features, target):\n",
    "    # разделим обучающую выборку на отрицательные и положительные объекты\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "    # скопируем несколько раз положительные объекты, создаем новую обучающую выборку\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * round(len(features_zeros) / len(features_ones)))\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * round(len(features_zeros) / len(features_ones)))\n",
    "    # перемешиваем данные\n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345)\n",
    "    \n",
    "    return features_upsampled, target_upsampled\n",
    "# получили новую обучающую выборку\n",
    "features_upsampled, target_upsampled = upsample(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели: {'C': 1, 'random_state': 12345}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wall time: 23.3 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.772500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.794118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.466187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.587489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.843719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик\n",
       "accuracy          0.772500\n",
       "recall            0.794118\n",
       "precision         0.466187\n",
       "f1                0.587489\n",
       "auc_roc           0.843719"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# используем функцию get_best_model() для LogisticRegression()\n",
    "log_reg_model_up, log_reg_up = get_best_model(LogisticRegression(solver='liblinear'), \n",
    "                                              param_log_reg, \n",
    "                                              features_upsampled, \n",
    "                                              features_valid, \n",
    "                                              target_upsampled, \n",
    "                                              target_valid)\n",
    "log_reg_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для сравнения объединим таблицы\n",
    "log_reg_up = pd.merge(log_reg_up, log_reg, suffixes=('_up', '_init'), on=log_reg.index).set_index('key_0')\n",
    "log_reg_up.index.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик_up</th>\n",
       "      <th>Значения метрик_init</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.772500</td>\n",
       "      <td>0.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.409314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.466187</td>\n",
       "      <td>0.690083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.587489</td>\n",
       "      <td>0.513846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.843719</td>\n",
       "      <td>0.840659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик_up  Значения метрик_init\n",
       "accuracy             0.772500              0.842000\n",
       "recall               0.794118              0.409314\n",
       "precision            0.466187              0.690083\n",
       "f1                   0.587489              0.513846\n",
       "auc_roc              0.843719              0.840659"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели: {'max_depth': 23, 'min_samples_leaf': 1, 'min_samples_split': 2, 'random_state': 12345}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wall time: 4min 15s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик_up</th>\n",
       "      <th>Значения метрик_init</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.804500</td>\n",
       "      <td>0.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.517157</td>\n",
       "      <td>0.524510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.520988</td>\n",
       "      <td>0.688103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.519065</td>\n",
       "      <td>0.595271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.698461</td>\n",
       "      <td>0.830639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик_up  Значения метрик_init\n",
       "accuracy             0.804500              0.854500\n",
       "recall               0.517157              0.524510\n",
       "precision            0.520988              0.688103\n",
       "f1                   0.519065              0.595271\n",
       "auc_roc              0.698461              0.830639"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# используем функцию get_best_model() для DecisionTreeClassifier()\n",
    "dec_tree_model_up, dec_tree_up = get_best_model(DecisionTreeClassifier(), \n",
    "                                                param_dec_tree, \n",
    "                                                features_upsampled, \n",
    "                                                features_valid, \n",
    "                                                target_upsampled, \n",
    "                                                target_valid)\n",
    "#для сравнения объединим таблицы\n",
    "dec_tree_up = pd.merge(dec_tree_up, dec_tree, suffixes=('_up', '_init'), on=dec_tree.index).set_index('key_0')\n",
    "dec_tree_up.index.name = None\n",
    "dec_tree_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели: {'max_depth': 26, 'n_estimators': 61, 'random_state': 12345}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wall time: 2min 32s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик_up</th>\n",
       "      <th>Значения метрик_init</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.854500</td>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.580882</td>\n",
       "      <td>0.514706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.663866</td>\n",
       "      <td>0.783582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.619608</td>\n",
       "      <td>0.621302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.858262</td>\n",
       "      <td>0.862280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик_up  Значения метрик_init\n",
       "accuracy             0.854500              0.872000\n",
       "recall               0.580882              0.514706\n",
       "precision            0.663866              0.783582\n",
       "f1                   0.619608              0.621302\n",
       "auc_roc              0.858262              0.862280"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#используем функцию get_best_model() для RandomForestClassifier()\n",
    "rand_for_model_up, rand_for_up = get_best_model(RandomForestClassifier(), \n",
    "                                                param_rand_for, \n",
    "                                                features_upsampled, \n",
    "                                                features_valid, \n",
    "                                                target_upsampled, \n",
    "                                                target_valid)\n",
    "#для сравнения объединим таблицы\n",
    "rand_for_up = pd.merge(rand_for_up, rand_for, suffixes=('_up', '_init'), on=rand_for.index).set_index('key_0')\n",
    "rand_for_up.index.name = None\n",
    "rand_for_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели: {'max_depth': 31, 'n_estimators': 91}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.836500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.601504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.594796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.853825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик\n",
       "accuracy          0.836500\n",
       "recall            0.588235\n",
       "precision         0.601504\n",
       "f1                0.594796\n",
       "auc_roc           0.853825"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# используем функцию get_best_model()\n",
    "xgb_model_up, df_xgb_up = get_best_model(\n",
    "    xgb.XGBClassifier(learning_rate=0.05, random_state=12345),\n",
    "    param_xgb,\n",
    "    features_upsampled,\n",
    "    features_valid,\n",
    "    target_upsampled,\n",
    "    target_valid,\n",
    ")\n",
    "df_xgb_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик_up</th>\n",
       "      <th>Значения метрик_init</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.836500</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.534314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.601504</td>\n",
       "      <td>0.719472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.594796</td>\n",
       "      <td>0.613221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.853825</td>\n",
       "      <td>0.853863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик_up  Значения метрик_init\n",
       "accuracy             0.836500              0.862500\n",
       "recall               0.588235              0.534314\n",
       "precision            0.601504              0.719472\n",
       "f1                   0.594796              0.613221\n",
       "auc_roc              0.853825              0.853863"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#для сравнения объединим таблицы\n",
    "df_xgb_up = pd.merge(df_xgb_up, df_xgb, suffixes=('_up', '_init'), on=df_xgb.index).set_index('key_0')\n",
    "df_xgb_up.index.name = None\n",
    "df_xgb_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000205 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000385 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000647 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000919 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000669 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000704 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000257 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000261 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000676 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000659 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000238 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000689 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000758 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000553 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000889 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000610 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000701 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000853 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000294 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000306 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000248 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000887 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000716 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000240 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000197 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000672 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000176 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000166 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000642 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000697 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000221 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000232 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000176 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000728 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000590 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000919 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000830 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000740 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000280 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000274 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000250 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000266 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000605 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000242 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000727 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000753 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000674 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000864 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000243 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000255 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000298 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000178 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000256 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001002 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000825 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000224 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000234 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000731 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000671 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000599 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000255 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000643 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000220 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000199 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000201 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000216 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000156 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7732, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505691 -> initscore=0.022764\n",
      "[LightGBM] [Info] Start training from score 0.022764\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3911, number of negative: 3822\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505755 -> initscore=0.023019\n",
      "[LightGBM] [Info] Start training from score 0.023019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3910, number of negative: 3823\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 7733, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505625 -> initscore=0.022502\n",
      "[LightGBM] [Info] Start training from score 0.022502\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 4888, number of negative: 4778\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 9666, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505690 -> initscore=0.022761\n",
      "[LightGBM] [Info] Start training from score 0.022761\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 4888, number of negative: 4778\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 9666, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.505690 -> initscore=0.022761\n",
      "[LightGBM] [Info] Start training from score 0.022761\n",
      "Лучшие параметры модели: {'max_depth': 21, 'n_estimators': 91}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.827000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.772059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.554577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.645492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.878658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик\n",
       "accuracy          0.827000\n",
       "recall            0.772059\n",
       "precision         0.554577\n",
       "f1                0.645492\n",
       "auc_roc           0.878658"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# используем функцию get_best_model()\n",
    "lgb_model_up, df_lgb_up = get_best_model(\n",
    "    lgb.LGBMClassifier(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"binary\",\n",
    "        learning_rate=0.05,\n",
    "        feature_fraction=0.9,\n",
    "        random_state=12345,\n",
    "        silent=False,\n",
    "    ),\n",
    "    param_lgb,\n",
    "    features_upsampled,\n",
    "    features_valid,\n",
    "    target_upsampled,\n",
    "    target_valid,\n",
    ")\n",
    "df_lgb_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик_up</th>\n",
       "      <th>Значения метрик_init</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.827000</td>\n",
       "      <td>0.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.772059</td>\n",
       "      <td>0.502451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.554577</td>\n",
       "      <td>0.753676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.645492</td>\n",
       "      <td>0.602941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.878658</td>\n",
       "      <td>0.879171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик_up  Значения метрик_init\n",
       "accuracy             0.827000              0.865000\n",
       "recall               0.772059              0.502451\n",
       "precision            0.554577              0.753676\n",
       "f1                   0.645492              0.602941\n",
       "auc_roc              0.878658              0.879171"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#для сравнения объединим таблицы\n",
    "df_lgb_up = pd.merge(df_lgb_up, df_lgb, suffixes=('_up', '_init'), on=df_lgb.index).set_index('key_0')\n",
    "df_lgb_up.index.name = None\n",
    "df_lgb_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6864193\ttotal: 1.96ms\tremaining: 194ms\n",
      "50:\tlearn: 0.5556938\ttotal: 95.2ms\tremaining: 91.5ms\n",
      "99:\tlearn: 0.5230803\ttotal: 184ms\tremaining: 0us\n",
      "0:\tlearn: 0.6861476\ttotal: 2.02ms\tremaining: 200ms\n",
      "50:\tlearn: 0.5574025\ttotal: 123ms\tremaining: 118ms\n",
      "99:\tlearn: 0.5252025\ttotal: 235ms\tremaining: 0us\n",
      "0:\tlearn: 0.6862848\ttotal: 1.72ms\tremaining: 170ms\n",
      "50:\tlearn: 0.5569130\ttotal: 71.8ms\tremaining: 69ms\n",
      "99:\tlearn: 0.5241800\ttotal: 142ms\tremaining: 0us\n",
      "0:\tlearn: 0.6862977\ttotal: 1.45ms\tremaining: 144ms\n",
      "50:\tlearn: 0.5550505\ttotal: 82.9ms\tremaining: 79.7ms\n",
      "99:\tlearn: 0.5225379\ttotal: 192ms\tremaining: 0us\n",
      "0:\tlearn: 0.6859797\ttotal: 1.95ms\tremaining: 194ms\n",
      "50:\tlearn: 0.5532692\ttotal: 116ms\tremaining: 111ms\n",
      "99:\tlearn: 0.5199124\ttotal: 202ms\tremaining: 0us\n",
      "0:\tlearn: 0.6803939\ttotal: 2.05ms\tremaining: 203ms\n",
      "50:\tlearn: 0.5004778\ttotal: 113ms\tremaining: 109ms\n",
      "99:\tlearn: 0.4694970\ttotal: 233ms\tremaining: 0us\n",
      "0:\tlearn: 0.6801674\ttotal: 2.1ms\tremaining: 208ms\n",
      "50:\tlearn: 0.5013278\ttotal: 139ms\tremaining: 134ms\n",
      "99:\tlearn: 0.4725389\ttotal: 252ms\tremaining: 0us\n",
      "0:\tlearn: 0.6803533\ttotal: 2.51ms\tremaining: 248ms\n",
      "50:\tlearn: 0.5002247\ttotal: 116ms\tremaining: 112ms\n",
      "99:\tlearn: 0.4693074\ttotal: 223ms\tremaining: 0us\n",
      "0:\tlearn: 0.6799458\ttotal: 2.79ms\tremaining: 276ms\n",
      "50:\tlearn: 0.5001718\ttotal: 137ms\tremaining: 132ms\n",
      "99:\tlearn: 0.4697798\ttotal: 262ms\tremaining: 0us\n",
      "0:\tlearn: 0.6799256\ttotal: 2.43ms\tremaining: 240ms\n",
      "50:\tlearn: 0.4959360\ttotal: 124ms\tremaining: 119ms\n",
      "99:\tlearn: 0.4634131\ttotal: 257ms\tremaining: 0us\n",
      "0:\tlearn: 0.6777256\ttotal: 3.58ms\tremaining: 354ms\n",
      "50:\tlearn: 0.4753573\ttotal: 153ms\tremaining: 147ms\n",
      "99:\tlearn: 0.4466346\ttotal: 306ms\tremaining: 0us\n",
      "0:\tlearn: 0.6772306\ttotal: 3.52ms\tremaining: 349ms\n",
      "50:\tlearn: 0.4765445\ttotal: 195ms\tremaining: 188ms\n",
      "99:\tlearn: 0.4483902\ttotal: 329ms\tremaining: 0us\n",
      "0:\tlearn: 0.6774056\ttotal: 2.6ms\tremaining: 257ms\n",
      "50:\tlearn: 0.4743013\ttotal: 146ms\tremaining: 140ms\n",
      "99:\tlearn: 0.4467203\ttotal: 281ms\tremaining: 0us\n",
      "0:\tlearn: 0.6788055\ttotal: 2.56ms\tremaining: 254ms\n",
      "50:\tlearn: 0.4758554\ttotal: 163ms\tremaining: 157ms\n",
      "99:\tlearn: 0.4483463\ttotal: 319ms\tremaining: 0us\n",
      "0:\tlearn: 0.6770898\ttotal: 4.11ms\tremaining: 406ms\n",
      "50:\tlearn: 0.4687263\ttotal: 146ms\tremaining: 141ms\n",
      "99:\tlearn: 0.4402792\ttotal: 295ms\tremaining: 0us\n",
      "0:\tlearn: 0.6766689\ttotal: 4.41ms\tremaining: 436ms\n",
      "50:\tlearn: 0.4584808\ttotal: 231ms\tremaining: 222ms\n",
      "99:\tlearn: 0.4241666\ttotal: 467ms\tremaining: 0us\n",
      "0:\tlearn: 0.6766458\ttotal: 4.35ms\tremaining: 431ms\n",
      "50:\tlearn: 0.4624879\ttotal: 236ms\tremaining: 227ms\n",
      "99:\tlearn: 0.4280330\ttotal: 508ms\tremaining: 0us\n",
      "0:\tlearn: 0.6762967\ttotal: 6.17ms\tremaining: 611ms\n",
      "50:\tlearn: 0.4569332\ttotal: 223ms\tremaining: 215ms\n",
      "99:\tlearn: 0.4225954\ttotal: 446ms\tremaining: 0us\n",
      "0:\tlearn: 0.6771472\ttotal: 4.28ms\tremaining: 423ms\n",
      "50:\tlearn: 0.4594282\ttotal: 233ms\tremaining: 224ms\n",
      "99:\tlearn: 0.4226833\ttotal: 457ms\tremaining: 0us\n",
      "0:\tlearn: 0.6765139\ttotal: 4.36ms\tremaining: 431ms\n",
      "50:\tlearn: 0.4534303\ttotal: 237ms\tremaining: 228ms\n",
      "99:\tlearn: 0.4190310\ttotal: 440ms\tremaining: 0us\n",
      "0:\tlearn: 0.6767574\ttotal: 9.22ms\tremaining: 913ms\n",
      "50:\tlearn: 0.4388908\ttotal: 485ms\tremaining: 466ms\n",
      "99:\tlearn: 0.3921243\ttotal: 951ms\tremaining: 0us\n",
      "0:\tlearn: 0.6767950\ttotal: 9.26ms\tremaining: 916ms\n",
      "50:\tlearn: 0.4398981\ttotal: 537ms\tremaining: 516ms\n",
      "99:\tlearn: 0.3920613\ttotal: 1.08s\tremaining: 0us\n",
      "0:\tlearn: 0.6766937\ttotal: 14.3ms\tremaining: 1.42s\n",
      "50:\tlearn: 0.4431390\ttotal: 472ms\tremaining: 453ms\n",
      "99:\tlearn: 0.3941679\ttotal: 926ms\tremaining: 0us\n",
      "0:\tlearn: 0.6776477\ttotal: 8.87ms\tremaining: 878ms\n",
      "50:\tlearn: 0.4387388\ttotal: 448ms\tremaining: 431ms\n",
      "99:\tlearn: 0.3880958\ttotal: 905ms\tremaining: 0us\n",
      "0:\tlearn: 0.6766522\ttotal: 9.18ms\tremaining: 909ms\n",
      "50:\tlearn: 0.4355238\ttotal: 532ms\tremaining: 511ms\n",
      "99:\tlearn: 0.3864138\ttotal: 1.03s\tremaining: 0us\n",
      "0:\tlearn: 0.6785894\ttotal: 40.8ms\tremaining: 4.04s\n",
      "50:\tlearn: 0.4167896\ttotal: 1.88s\tremaining: 1.81s\n",
      "99:\tlearn: 0.3525343\ttotal: 3.68s\tremaining: 0us\n",
      "0:\tlearn: 0.6784380\ttotal: 31.7ms\tremaining: 3.14s\n",
      "50:\tlearn: 0.4185908\ttotal: 1.76s\tremaining: 1.7s\n",
      "99:\tlearn: 0.3567502\ttotal: 3.51s\tremaining: 0us\n",
      "0:\tlearn: 0.6786927\ttotal: 35.1ms\tremaining: 3.48s\n",
      "50:\tlearn: 0.4154167\ttotal: 1.69s\tremaining: 1.62s\n",
      "99:\tlearn: 0.3484844\ttotal: 3.38s\tremaining: 0us\n",
      "0:\tlearn: 0.6794260\ttotal: 34.6ms\tremaining: 3.42s\n",
      "50:\tlearn: 0.4145935\ttotal: 1.81s\tremaining: 1.74s\n",
      "99:\tlearn: 0.3505565\ttotal: 3.57s\tremaining: 0us\n",
      "0:\tlearn: 0.6785148\ttotal: 34.7ms\tremaining: 3.44s\n",
      "50:\tlearn: 0.4088510\ttotal: 1.75s\tremaining: 1.68s\n",
      "99:\tlearn: 0.3459235\ttotal: 3.37s\tremaining: 0us\n",
      "0:\tlearn: 0.6810321\ttotal: 128ms\tremaining: 12.7s\n",
      "50:\tlearn: 0.3978370\ttotal: 6.44s\tremaining: 6.19s\n",
      "99:\tlearn: 0.3102485\ttotal: 12.6s\tremaining: 0us\n",
      "0:\tlearn: 0.6806935\ttotal: 143ms\tremaining: 14.2s\n",
      "50:\tlearn: 0.3988240\ttotal: 6.18s\tremaining: 5.94s\n",
      "99:\tlearn: 0.3104510\ttotal: 12.1s\tremaining: 0us\n",
      "0:\tlearn: 0.6808248\ttotal: 140ms\tremaining: 13.9s\n",
      "50:\tlearn: 0.3973363\ttotal: 6.57s\tremaining: 6.32s\n",
      "99:\tlearn: 0.3086636\ttotal: 12.5s\tremaining: 0us\n",
      "0:\tlearn: 0.6813711\ttotal: 133ms\tremaining: 13.1s\n",
      "50:\tlearn: 0.3962402\ttotal: 6.58s\tremaining: 6.32s\n",
      "99:\tlearn: 0.3054484\ttotal: 12.9s\tremaining: 0us\n",
      "0:\tlearn: 0.6807539\ttotal: 141ms\tremaining: 13.9s\n",
      "50:\tlearn: 0.3937064\ttotal: 6.52s\tremaining: 6.27s\n",
      "99:\tlearn: 0.3048949\ttotal: 12.2s\tremaining: 0us\n",
      "0:\tlearn: 0.6825078\ttotal: 530ms\tremaining: 52.5s\n",
      "50:\tlearn: 0.3921844\ttotal: 25s\tremaining: 24s\n",
      "99:\tlearn: 0.2840842\ttotal: 49.9s\tremaining: 0us\n",
      "0:\tlearn: 0.6823760\ttotal: 542ms\tremaining: 53.6s\n",
      "50:\tlearn: 0.3910594\ttotal: 25.6s\tremaining: 24.6s\n",
      "99:\tlearn: 0.2820420\ttotal: 52.5s\tremaining: 0us\n",
      "0:\tlearn: 0.6827685\ttotal: 587ms\tremaining: 58.1s\n",
      "50:\tlearn: 0.3909162\ttotal: 28.3s\tremaining: 27.2s\n",
      "99:\tlearn: 0.2834973\ttotal: 55.1s\tremaining: 0us\n",
      "0:\tlearn: 0.6829231\ttotal: 535ms\tremaining: 53s\n",
      "50:\tlearn: 0.3915297\ttotal: 24.4s\tremaining: 23.5s\n",
      "99:\tlearn: 0.2830432\ttotal: 50s\tremaining: 0us\n",
      "0:\tlearn: 0.6825376\ttotal: 517ms\tremaining: 51.1s\n",
      "50:\tlearn: 0.3888125\ttotal: 25.2s\tremaining: 24.2s\n",
      "99:\tlearn: 0.2786269\ttotal: 49.9s\tremaining: 0us\n",
      "0:\tlearn: 0.6814137\ttotal: 545ms\tremaining: 53.9s\n",
      "50:\tlearn: 0.3770392\ttotal: 28.6s\tremaining: 27.5s\n",
      "99:\tlearn: 0.2705604\ttotal: 55.5s\tremaining: 0us\n",
      "0:\tlearn: 0.6814137\ttotal: 614ms\tremaining: 1m\n",
      "50:\tlearn: 0.3770392\ttotal: 28s\tremaining: 26.9s\n",
      "99:\tlearn: 0.2705604\ttotal: 55.4s\tremaining: 0us\n",
      "Лучшие параметры модели: {'depth': 15}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.814000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.678922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.534749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.598272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.852036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик\n",
       "accuracy          0.814000\n",
       "recall            0.678922\n",
       "precision         0.534749\n",
       "f1                0.598272\n",
       "auc_roc           0.852036"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# используем функцию get_best_model()\n",
    "ctb_model_up, df_ctb_up = get_best_model(\n",
    "    ctb.CatBoostClassifier(iterations=100, learning_rate=0.05, verbose=50),\n",
    "    param_ctb,\n",
    "    features_upsampled,\n",
    "    features_valid,\n",
    "    target_upsampled,\n",
    "    target_valid,\n",
    ")\n",
    "df_ctb_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик_up</th>\n",
       "      <th>Значения метрик_init</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.678922</td>\n",
       "      <td>0.477941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.534749</td>\n",
       "      <td>0.776892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.598272</td>\n",
       "      <td>0.591806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.852036</td>\n",
       "      <td>0.874340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик_up  Значения метрик_init\n",
       "accuracy             0.814000              0.865500\n",
       "recall               0.678922              0.477941\n",
       "precision            0.534749              0.776892\n",
       "f1                   0.598272              0.591806\n",
       "auc_roc              0.852036              0.874340"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#для сравнения объединим таблицы\n",
    "df_ctb_up = pd.merge(df_ctb_up, df_ctb, suffixes=('_up', '_init'), on=df_ctb.index).set_index('key_0')\n",
    "df_ctb_up.index.name = None\n",
    "df_ctb_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# используем технику downsampling уменьшения выборки\n",
    "def downsample(features, target):\n",
    "    # разделим обучающую выборку на отрицательные и положительные объекты\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "    # случайным образом отбросим часть отрицательных объектов\n",
    "    fraction = round(len(features_ones) / len(features_zeros), 2)\n",
    "    features_downsampled = pd.concat([features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n",
    "    target_downsampled = pd.concat([target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n",
    "    # перемешиваем данные\n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=12345)\n",
    "    \n",
    "    return features_downsampled, target_downsampled\n",
    "# получили новую обучающую выборку\n",
    "features_downsampled, target_downsampled = downsample(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели: {'C': 3, 'random_state': 12345}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wall time: 5.98 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик_down</th>\n",
       "      <th>Значения метрик_up</th>\n",
       "      <th>Значения метрик_init</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.772500</td>\n",
       "      <td>0.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.786765</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.409314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.469298</td>\n",
       "      <td>0.466187</td>\n",
       "      <td>0.690083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.587912</td>\n",
       "      <td>0.587489</td>\n",
       "      <td>0.513846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.842221</td>\n",
       "      <td>0.843719</td>\n",
       "      <td>0.840659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик_down  Значения метрик_up  Значения метрик_init\n",
       "accuracy               0.775000            0.772500              0.842000\n",
       "recall                 0.786765            0.794118              0.409314\n",
       "precision              0.469298            0.466187              0.690083\n",
       "f1                     0.587912            0.587489              0.513846\n",
       "auc_roc                0.842221            0.843719              0.840659"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# используем функцию get_best_model() для LogisticRegression()\n",
    "log_reg_model_down, log_reg_down = get_best_model(LogisticRegression(solver='liblinear'), \n",
    "                                                  param_log_reg,\n",
    "                                                  features_downsampled, \n",
    "                                                  features_valid, \n",
    "                                                  target_downsampled, \n",
    "                                                  target_valid)\n",
    "# для сравнения объединим таблицы\n",
    "log_reg_down = pd.merge(log_reg_down, log_reg_up, on=log_reg_up.index).set_index('key_0')\n",
    "log_reg_down.index.name = None\n",
    "log_reg_down.rename(columns={'Значения метрик' : 'Значения метрик_down'}, inplace=True)\n",
    "log_reg_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели: {'max_depth': 5, 'min_samples_leaf': 7, 'min_samples_split': 2, 'random_state': 12345}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wall time: 1min 37s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик_down</th>\n",
       "      <th>Значения метрик_up</th>\n",
       "      <th>Значения метрик_init</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.769000</td>\n",
       "      <td>0.804500</td>\n",
       "      <td>0.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.786765</td>\n",
       "      <td>0.517157</td>\n",
       "      <td>0.524510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.461207</td>\n",
       "      <td>0.520988</td>\n",
       "      <td>0.688103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.581522</td>\n",
       "      <td>0.519065</td>\n",
       "      <td>0.595271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.845149</td>\n",
       "      <td>0.698461</td>\n",
       "      <td>0.830639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик_down  Значения метрик_up  Значения метрик_init\n",
       "accuracy               0.769000            0.804500              0.854500\n",
       "recall                 0.786765            0.517157              0.524510\n",
       "precision              0.461207            0.520988              0.688103\n",
       "f1                     0.581522            0.519065              0.595271\n",
       "auc_roc                0.845149            0.698461              0.830639"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# используем функцию get_best_model() для DecisionTreeClassifier()\n",
    "dec_tree_model_down, dec_tree_down = get_best_model(DecisionTreeClassifier(), \n",
    "                                                    param_dec_tree, \n",
    "                                                    features_downsampled, \n",
    "                                                    features_valid, \n",
    "                                                    target_downsampled, \n",
    "                                                    target_valid)\n",
    "# для сравнения объединим таблицы\n",
    "dec_tree_down = pd.merge(dec_tree_down, dec_tree_up, on=dec_tree_up.index).set_index('key_0')\n",
    "dec_tree_down.index.name = None\n",
    "dec_tree_down.rename(columns={'Значения метрик' : 'Значения метрик_down'}, inplace=True)\n",
    "dec_tree_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели: {'max_depth': 11, 'n_estimators': 91, 'random_state': 12345}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wall time: 1min 9s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик_down</th>\n",
       "      <th>Значения метрик_up</th>\n",
       "      <th>Значения метрик_init</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.793000</td>\n",
       "      <td>0.854500</td>\n",
       "      <td>0.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.580882</td>\n",
       "      <td>0.514706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.495399</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>0.783582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.609434</td>\n",
       "      <td>0.619608</td>\n",
       "      <td>0.621302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.859119</td>\n",
       "      <td>0.858262</td>\n",
       "      <td>0.862280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик_down  Значения метрик_up  Значения метрик_init\n",
       "accuracy               0.793000            0.854500              0.872000\n",
       "recall                 0.791667            0.580882              0.514706\n",
       "precision              0.495399            0.663866              0.783582\n",
       "f1                     0.609434            0.619608              0.621302\n",
       "auc_roc                0.859119            0.858262              0.862280"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#используем функцию get_best_model() для RandomForestClassifier()\n",
    "rand_for_model_down, rand_for_down = get_best_model(RandomForestClassifier(), \n",
    "                                                    param_rand_for, \n",
    "                                                    features_downsampled, \n",
    "                                                    features_valid, \n",
    "                                                    target_downsampled, \n",
    "                                                    target_valid)\n",
    "#для сравнения объединим таблицы\n",
    "rand_for_down = pd.merge(rand_for_down, rand_for_up, on=rand_for_up.index).set_index('key_0')\n",
    "rand_for_down.index.name = None\n",
    "rand_for_down.rename(columns={'Значения метрик' : 'Значения метрик_down'}, inplace=True)\n",
    "rand_for_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры модели: {'max_depth': 21, 'n_estimators': 41}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик_down</th>\n",
       "      <th>Значения метрик_up</th>\n",
       "      <th>Значения метрик_init</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.772500</td>\n",
       "      <td>0.836500</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.534314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.465390</td>\n",
       "      <td>0.601504</td>\n",
       "      <td>0.719472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.581417</td>\n",
       "      <td>0.594796</td>\n",
       "      <td>0.613221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.850560</td>\n",
       "      <td>0.853825</td>\n",
       "      <td>0.853863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик_down  Значения метрик_up  Значения метрик_init\n",
       "accuracy               0.772500            0.836500              0.862500\n",
       "recall                 0.774510            0.588235              0.534314\n",
       "precision              0.465390            0.601504              0.719472\n",
       "f1                     0.581417            0.594796              0.613221\n",
       "auc_roc                0.850560            0.853825              0.853863"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# используем функцию get_best_model()\n",
    "xgb_model_down, df_xgb_down = get_best_model(\n",
    "    xgb.XGBClassifier(learning_rate=0.05, random_state=12345),\n",
    "    param_xgb,\n",
    "    features_downsampled,\n",
    "    features_valid,\n",
    "    target_downsampled,\n",
    "    target_valid,\n",
    ")\n",
    "# для сравнения объединим таблицы\n",
    "df_xgb_down = pd.merge(df_xgb_down, df_xgb_up, on=df_xgb_up.index).set_index('key_0')\n",
    "df_xgb_down.index.name = None\n",
    "df_xgb_down.rename(columns={'Значения метрик' : 'Значения метрик_down'}, inplace=True)\n",
    "df_xgb_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000223 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000245 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000335 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000602 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000256 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000467 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000327 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000394 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000406 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000344 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000367 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000314 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000482 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000414 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000319 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000344 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000493 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000248 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000313 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000420 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000637 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000366 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000450 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000531 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000263 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000331 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000358 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000275 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000503 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000378 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000322 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000362 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000297 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000349 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000374 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000373 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000431 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000275 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000407 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000435 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000271 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000275 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000410 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000383 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000359 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000301 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000333 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000275 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000283 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000265 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000348 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000330 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000369 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000510 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000327 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000419 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000334 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000482 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000377 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000293 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000430 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000253 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000341 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000327 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000431 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000250 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000340 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000277 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000300 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000275 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000273 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000259 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000260 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000276 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000345 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000306 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000256 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000270 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000291 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000405 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 851\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 993\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000274 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496195 -> initscore=-0.015221\n",
      "[LightGBM] [Info] Start training from score -0.015221\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 977, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 853\n",
      "[LightGBM] [Info] Number of data points in the train set: 1971, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495687 -> initscore=-0.017251\n",
      "[LightGBM] [Info] Start training from score -0.017251\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 978, number of negative: 994\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 852\n",
      "[LightGBM] [Info] Number of data points in the train set: 1972, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495943 -> initscore=-0.016228\n",
      "[LightGBM] [Info] Start training from score -0.016228\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 1222, number of negative: 1242\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000102 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 854\n",
      "[LightGBM] [Info] Number of data points in the train set: 2464, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495942 -> initscore=-0.016234\n",
      "[LightGBM] [Info] Start training from score -0.016234\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 1222, number of negative: 1242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 854\n",
      "[LightGBM] [Info] Number of data points in the train set: 2464, number of used features: 14\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.495942 -> initscore=-0.016234\n",
      "[LightGBM] [Info] Start training from score -0.016234\n",
      "Лучшие параметры модели: {'max_depth': 11, 'n_estimators': 31}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик_down</th>\n",
       "      <th>Значения метрик_up</th>\n",
       "      <th>Значения метрик_init</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.827000</td>\n",
       "      <td>0.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.789216</td>\n",
       "      <td>0.772059</td>\n",
       "      <td>0.502451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.554577</td>\n",
       "      <td>0.753676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.620424</td>\n",
       "      <td>0.645492</td>\n",
       "      <td>0.602941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.870582</td>\n",
       "      <td>0.878658</td>\n",
       "      <td>0.879171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик_down  Значения метрик_up  Значения метрик_init\n",
       "accuracy               0.803000            0.827000              0.865000\n",
       "recall                 0.789216            0.772059              0.502451\n",
       "precision              0.511111            0.554577              0.753676\n",
       "f1                     0.620424            0.645492              0.602941\n",
       "auc_roc                0.870582            0.878658              0.879171"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# используем функцию get_best_model()\n",
    "lgb_model_down, df_lgb_down = get_best_model(\n",
    "    lgb.LGBMClassifier(\n",
    "        boosting_type=\"gbdt\",\n",
    "        objective=\"binary\",\n",
    "        learning_rate=0.05,\n",
    "        feature_fraction=0.9,\n",
    "        random_state=12345,\n",
    "        silent=False,\n",
    "    ),\n",
    "    param_lgb,\n",
    "    features_downsampled,\n",
    "    features_valid,\n",
    "    target_downsampled,\n",
    "    target_valid,\n",
    ")\n",
    "# для сравнения объединим таблицы\n",
    "df_lgb_down = pd.merge(df_lgb_down, df_lgb_up, on=df_lgb_up.index).set_index('key_0')\n",
    "df_lgb_down.index.name = None\n",
    "df_lgb_down.rename(columns={'Значения метрик' : 'Значения метрик_down'}, inplace=True)\n",
    "df_lgb_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6856446\ttotal: 893us\tremaining: 88.5ms\n",
      "50:\tlearn: 0.5471776\ttotal: 39.2ms\tremaining: 37.6ms\n",
      "99:\tlearn: 0.5144137\ttotal: 85ms\tremaining: 0us\n",
      "0:\tlearn: 0.6859437\ttotal: 949us\tremaining: 94ms\n",
      "50:\tlearn: 0.5526628\ttotal: 41ms\tremaining: 39.4ms\n",
      "99:\tlearn: 0.5181124\ttotal: 86.4ms\tremaining: 0us\n",
      "0:\tlearn: 0.6865511\ttotal: 1.02ms\tremaining: 101ms\n",
      "50:\tlearn: 0.5577333\ttotal: 49.3ms\tremaining: 47.3ms\n",
      "99:\tlearn: 0.5260941\ttotal: 98.4ms\tremaining: 0us\n",
      "0:\tlearn: 0.6856239\ttotal: 919us\tremaining: 91ms\n",
      "50:\tlearn: 0.5518028\ttotal: 44.4ms\tremaining: 42.7ms\n",
      "99:\tlearn: 0.5203725\ttotal: 94.5ms\tremaining: 0us\n",
      "0:\tlearn: 0.6859228\ttotal: 885us\tremaining: 87.6ms\n",
      "50:\tlearn: 0.5537984\ttotal: 37.8ms\tremaining: 36.3ms\n",
      "99:\tlearn: 0.5202164\ttotal: 78ms\tremaining: 0us\n",
      "0:\tlearn: 0.6795900\ttotal: 1.25ms\tremaining: 124ms\n",
      "50:\tlearn: 0.4962027\ttotal: 61.7ms\tremaining: 59.2ms\n",
      "99:\tlearn: 0.4658234\ttotal: 120ms\tremaining: 0us\n",
      "0:\tlearn: 0.6799577\ttotal: 1.5ms\tremaining: 148ms\n",
      "50:\tlearn: 0.4977814\ttotal: 62.3ms\tremaining: 59.9ms\n",
      "99:\tlearn: 0.4679010\ttotal: 123ms\tremaining: 0us\n",
      "0:\tlearn: 0.6805709\ttotal: 1.34ms\tremaining: 132ms\n",
      "50:\tlearn: 0.5067257\ttotal: 70.9ms\tremaining: 68.1ms\n",
      "99:\tlearn: 0.4763160\ttotal: 129ms\tremaining: 0us\n",
      "0:\tlearn: 0.6798297\ttotal: 1.22ms\tremaining: 120ms\n",
      "50:\tlearn: 0.5008620\ttotal: 62ms\tremaining: 59.6ms\n",
      "99:\tlearn: 0.4700082\ttotal: 119ms\tremaining: 0us\n",
      "0:\tlearn: 0.6803009\ttotal: 1.6ms\tremaining: 159ms\n",
      "50:\tlearn: 0.5014271\ttotal: 72.9ms\tremaining: 70ms\n",
      "99:\tlearn: 0.4698926\ttotal: 134ms\tremaining: 0us\n",
      "0:\tlearn: 0.6791406\ttotal: 1.7ms\tremaining: 168ms\n",
      "50:\tlearn: 0.4749940\ttotal: 89.6ms\tremaining: 86.1ms\n",
      "99:\tlearn: 0.4406208\ttotal: 186ms\tremaining: 0us\n",
      "0:\tlearn: 0.6792757\ttotal: 1.9ms\tremaining: 188ms\n",
      "50:\tlearn: 0.4780966\ttotal: 95.9ms\tremaining: 92.1ms\n",
      "99:\tlearn: 0.4409432\ttotal: 197ms\tremaining: 0us\n",
      "0:\tlearn: 0.6800884\ttotal: 1.88ms\tremaining: 186ms\n",
      "50:\tlearn: 0.4846715\ttotal: 97.8ms\tremaining: 93.9ms\n",
      "99:\tlearn: 0.4486553\ttotal: 189ms\tremaining: 0us\n",
      "0:\tlearn: 0.6792922\ttotal: 1.74ms\tremaining: 172ms\n",
      "50:\tlearn: 0.4788246\ttotal: 96ms\tremaining: 92.2ms\n",
      "99:\tlearn: 0.4432054\ttotal: 193ms\tremaining: 0us\n",
      "0:\tlearn: 0.6796081\ttotal: 1.88ms\tremaining: 186ms\n",
      "50:\tlearn: 0.4784744\ttotal: 97.6ms\tremaining: 93.7ms\n",
      "99:\tlearn: 0.4395236\ttotal: 186ms\tremaining: 0us\n",
      "0:\tlearn: 0.6800504\ttotal: 4.74ms\tremaining: 469ms\n",
      "50:\tlearn: 0.4615202\ttotal: 164ms\tremaining: 158ms\n",
      "99:\tlearn: 0.4184846\ttotal: 337ms\tremaining: 0us\n",
      "0:\tlearn: 0.6799737\ttotal: 3.54ms\tremaining: 350ms\n",
      "50:\tlearn: 0.4649270\ttotal: 165ms\tremaining: 158ms\n",
      "99:\tlearn: 0.4214164\ttotal: 324ms\tremaining: 0us\n",
      "0:\tlearn: 0.6813803\ttotal: 3.9ms\tremaining: 386ms\n",
      "50:\tlearn: 0.4711844\ttotal: 178ms\tremaining: 171ms\n",
      "99:\tlearn: 0.4308276\ttotal: 337ms\tremaining: 0us\n",
      "0:\tlearn: 0.6801141\ttotal: 4.24ms\tremaining: 420ms\n",
      "50:\tlearn: 0.4636394\ttotal: 175ms\tremaining: 168ms\n",
      "99:\tlearn: 0.4227579\ttotal: 338ms\tremaining: 0us\n",
      "0:\tlearn: 0.6804267\ttotal: 3.44ms\tremaining: 341ms\n",
      "50:\tlearn: 0.4637122\ttotal: 164ms\tremaining: 157ms\n",
      "99:\tlearn: 0.4193567\ttotal: 338ms\tremaining: 0us\n",
      "0:\tlearn: 0.6815418\ttotal: 11.9ms\tremaining: 1.18s\n",
      "50:\tlearn: 0.4529245\ttotal: 418ms\tremaining: 402ms\n",
      "99:\tlearn: 0.3981955\ttotal: 828ms\tremaining: 0us\n",
      "0:\tlearn: 0.6826898\ttotal: 9.27ms\tremaining: 918ms\n",
      "50:\tlearn: 0.4564553\ttotal: 412ms\tremaining: 396ms\n",
      "99:\tlearn: 0.3952902\ttotal: 820ms\tremaining: 0us\n",
      "0:\tlearn: 0.6828060\ttotal: 10ms\tremaining: 992ms\n",
      "50:\tlearn: 0.4625684\ttotal: 405ms\tremaining: 389ms\n",
      "99:\tlearn: 0.4041312\ttotal: 821ms\tremaining: 0us\n",
      "0:\tlearn: 0.6826675\ttotal: 9.8ms\tremaining: 970ms\n",
      "50:\tlearn: 0.4575889\ttotal: 420ms\tremaining: 404ms\n",
      "99:\tlearn: 0.4037317\ttotal: 779ms\tremaining: 0us\n",
      "0:\tlearn: 0.6827937\ttotal: 7.78ms\tremaining: 770ms\n",
      "50:\tlearn: 0.4555530\ttotal: 405ms\tremaining: 389ms\n",
      "99:\tlearn: 0.3959712\ttotal: 812ms\tremaining: 0us\n",
      "0:\tlearn: 0.6847870\ttotal: 35.8ms\tremaining: 3.54s\n",
      "50:\tlearn: 0.4495659\ttotal: 1.63s\tremaining: 1.57s\n",
      "99:\tlearn: 0.3738803\ttotal: 3.39s\tremaining: 0us\n",
      "0:\tlearn: 0.6854672\ttotal: 34.8ms\tremaining: 3.44s\n",
      "50:\tlearn: 0.4506451\ttotal: 1.73s\tremaining: 1.66s\n",
      "99:\tlearn: 0.3749402\ttotal: 3.46s\tremaining: 0us\n",
      "0:\tlearn: 0.6853875\ttotal: 33.1ms\tremaining: 3.27s\n",
      "50:\tlearn: 0.4586285\ttotal: 1.63s\tremaining: 1.57s\n",
      "99:\tlearn: 0.3848454\ttotal: 3.31s\tremaining: 0us\n",
      "0:\tlearn: 0.6854986\ttotal: 31.3ms\tremaining: 3.1s\n",
      "50:\tlearn: 0.4522084\ttotal: 1.71s\tremaining: 1.64s\n",
      "99:\tlearn: 0.3783579\ttotal: 3.47s\tremaining: 0us\n",
      "0:\tlearn: 0.6854581\ttotal: 36.1ms\tremaining: 3.57s\n",
      "50:\tlearn: 0.4514754\ttotal: 1.74s\tremaining: 1.68s\n",
      "99:\tlearn: 0.3727110\ttotal: 3.61s\tremaining: 0us\n",
      "0:\tlearn: 0.6868583\ttotal: 149ms\tremaining: 14.8s\n",
      "50:\tlearn: 0.4595070\ttotal: 6.69s\tremaining: 6.43s\n",
      "99:\tlearn: 0.3650545\ttotal: 13.6s\tremaining: 0us\n",
      "0:\tlearn: 0.6872894\ttotal: 144ms\tremaining: 14.3s\n",
      "50:\tlearn: 0.4556014\ttotal: 5.87s\tremaining: 5.64s\n",
      "99:\tlearn: 0.3659448\ttotal: 12.5s\tremaining: 0us\n",
      "0:\tlearn: 0.6871874\ttotal: 142ms\tremaining: 14s\n",
      "50:\tlearn: 0.4654832\ttotal: 6.36s\tremaining: 6.11s\n",
      "99:\tlearn: 0.3693893\ttotal: 13.4s\tremaining: 0us\n",
      "0:\tlearn: 0.6874050\ttotal: 140ms\tremaining: 13.9s\n",
      "50:\tlearn: 0.4578574\ttotal: 6.24s\tremaining: 5.99s\n",
      "99:\tlearn: 0.3664682\ttotal: 12.1s\tremaining: 0us\n",
      "0:\tlearn: 0.6872154\ttotal: 135ms\tremaining: 13.4s\n",
      "50:\tlearn: 0.4584945\ttotal: 6.48s\tremaining: 6.23s\n",
      "99:\tlearn: 0.3642749\ttotal: 13.3s\tremaining: 0us\n",
      "0:\tlearn: 0.6882247\ttotal: 550ms\tremaining: 54.4s\n",
      "50:\tlearn: 0.4603017\ttotal: 21.3s\tremaining: 20.5s\n",
      "99:\tlearn: 0.3628021\ttotal: 49.1s\tremaining: 0us\n",
      "0:\tlearn: 0.6880054\ttotal: 650ms\tremaining: 1m 4s\n",
      "50:\tlearn: 0.4640287\ttotal: 23s\tremaining: 22.1s\n",
      "99:\tlearn: 0.3627247\ttotal: 46.1s\tremaining: 0us\n",
      "0:\tlearn: 0.6882332\ttotal: 554ms\tremaining: 54.8s\n",
      "50:\tlearn: 0.4689107\ttotal: 22.8s\tremaining: 21.9s\n",
      "99:\tlearn: 0.3675674\ttotal: 45.8s\tremaining: 0us\n",
      "0:\tlearn: 0.6881690\ttotal: 552ms\tremaining: 54.7s\n",
      "50:\tlearn: 0.4637644\ttotal: 22.2s\tremaining: 21.4s\n",
      "99:\tlearn: 0.3646390\ttotal: 46.6s\tremaining: 0us\n",
      "0:\tlearn: 0.6883214\ttotal: 507ms\tremaining: 50.2s\n",
      "50:\tlearn: 0.4673006\ttotal: 24.2s\tremaining: 23.2s\n",
      "99:\tlearn: 0.3641690\ttotal: 49.1s\tremaining: 0us\n",
      "0:\tlearn: 0.6798769\ttotal: 4.3ms\tremaining: 426ms\n",
      "50:\tlearn: 0.4652189\ttotal: 149ms\tremaining: 143ms\n",
      "99:\tlearn: 0.4260138\ttotal: 307ms\tremaining: 0us\n",
      "0:\tlearn: 0.6798769\ttotal: 3.4ms\tremaining: 336ms\n",
      "50:\tlearn: 0.4652189\ttotal: 186ms\tremaining: 179ms\n",
      "99:\tlearn: 0.4260138\ttotal: 337ms\tremaining: 0us\n",
      "Лучшие параметры модели: {'depth': 7}\n",
      "\n",
      "Матрица ошибок:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Wall time: 5min 32s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик_down</th>\n",
       "      <th>Значения метрик_up</th>\n",
       "      <th>Значения метрик_init</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.796500</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.678922</td>\n",
       "      <td>0.477941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.500759</td>\n",
       "      <td>0.534749</td>\n",
       "      <td>0.776892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.618557</td>\n",
       "      <td>0.598272</td>\n",
       "      <td>0.591806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.871090</td>\n",
       "      <td>0.852036</td>\n",
       "      <td>0.874340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик_down  Значения метрик_up  Значения метрик_init\n",
       "accuracy               0.796500            0.814000              0.865500\n",
       "recall                 0.808824            0.678922              0.477941\n",
       "precision              0.500759            0.534749              0.776892\n",
       "f1                     0.618557            0.598272              0.591806\n",
       "auc_roc                0.871090            0.852036              0.874340"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# используем функцию get_best_model()\n",
    "ctb_model_down, df_ctb_down = get_best_model(\n",
    "    ctb.CatBoostClassifier(iterations=100, learning_rate=0.05, verbose=50),\n",
    "    param_ctb,\n",
    "    features_downsampled,\n",
    "    features_valid,\n",
    "    target_downsampled,\n",
    "    target_valid,\n",
    ")\n",
    "# для сравнения объединим таблицы\n",
    "df_ctb_down = pd.merge(df_ctb_down, df_ctb_up, on=df_ctb_up.index).set_index('key_0')\n",
    "df_ctb_down.index.name = None\n",
    "df_ctb_down.rename(columns={'Значения метрик' : 'Значения метрик_down'}, inplace=True)\n",
    "df_ctb_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик_down</th>\n",
       "      <th>Значения метрик_up</th>\n",
       "      <th>Значения метрик_init</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.796500</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.678922</td>\n",
       "      <td>0.477941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.500759</td>\n",
       "      <td>0.534749</td>\n",
       "      <td>0.776892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.618557</td>\n",
       "      <td>0.598272</td>\n",
       "      <td>0.591806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.871090</td>\n",
       "      <td>0.852036</td>\n",
       "      <td>0.874340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик_down  Значения метрик_up  Значения метрик_init\n",
       "accuracy               0.796500            0.814000              0.865500\n",
       "recall                 0.808824            0.678922              0.477941\n",
       "precision              0.500759            0.534749              0.776892\n",
       "f1                     0.618557            0.598272              0.591806\n",
       "auc_roc                0.871090            0.852036              0.874340"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ctb_down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшее значение F1-меры оказалось равным `0.6455`. Это значение удалось достичь для модели LightGBM с помощью техники upsampling. Техника downsampling для LightGBM дала результат `0.6204` и для CatBoost - `0.6186`.\n",
    "\n",
    "Случайный лес показал результат немного хуже - `0.6213`.\n",
    "\n",
    "Решающее дерево и логистическая регрессия показали себя хуже: `0.5952` и `0.5873` соответственно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='6-section'></a>\n",
    "### 4. Изменение порога"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию значение порога равно 0.5. Что, если мы поменяем это значение, может быть наша модель станет лучше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию, которая принимает на вход модель и возвращает значение f1-меры и значение порога. \n",
    "def get_f1_threshold(model):\n",
    "    # воспользуемся функцией predict_proba()\n",
    "    predicted_valid = model.predict(features_valid)\n",
    "    # значения вероятности классов для валидационной выборки\n",
    "    probabilities_valid = model.predict_proba(features_valid)\n",
    "    probabilities_one_valid = probabilities_valid[:, 1]\n",
    "    # переберем значения порогов от 0 до 1 с шагом в 0.01\n",
    "    best_f1 = 0\n",
    "    for threshold in np.arange(0, 1, 0.01):\n",
    "        predicted_valid = probabilities_one_valid > threshold\n",
    "        f1 = f1_score(target_valid, predicted_valid)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    print('Лучшее значение F1-меры - {:.4f} - достигается при значении порога - {:.2f}'.format(best_f1, best_threshold))\n",
    "    predicted_valid = probabilities_one_valid > best_threshold\n",
    "    return best_f1, best_threshold, probabilities_valid, probabilities_one_valid, predicted_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшее значение F1-меры - 0.6504 - достигается при значении порога - 0.61\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    f1_lgb_down,\n",
    "    threshold_lgb_down,\n",
    "    probabilities_valid_lgb_down,\n",
    "    probabilities_one_valid_lgb_down,\n",
    "    predicted_valid_lgb_down,\n",
    ") = get_f1_threshold(lgb_model_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшее значение F1-меры - 0.6489 - достигается при значении порога - 0.55\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    f1_lgb_up,\n",
    "    threshold_lgb_up,\n",
    "    probabilities_valid_lgb_up,\n",
    "    probabilities_one_valid_lgb_up,\n",
    "    predicted_valid_lgb_up,\n",
    ") = get_f1_threshold(lgb_model_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшее значение F1-меры - 0.6586 - достигается при значении порога - 0.59\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    f1_ctb_down,\n",
    "    threshold_ctb_down,\n",
    "    probabilities_valid_ctb_down,\n",
    "    probabilities_one_valid_ctb_down,\n",
    "    predicted_valid_ctb_down,\n",
    ") = get_f1_threshold(ctb_model_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшее значение F1-меры - 0.6278 - достигается при значении порога - 0.45\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    f1_xgb,\n",
    "    threshold_xgb,\n",
    "    probabilities_valid_xgb,\n",
    "    probabilities_one_valid_xgb,\n",
    "    predicted_valid_xgb,\n",
    ") = get_f1_threshold(xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшее значение F1-меры - 0.6460 - достигается при значении порога - 0.33\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    f1_rand_for,\n",
    "    threshold_rand_for,\n",
    "    probabilities_valid_rand_for,\n",
    "    probabilities_one_valid_rand_for,\n",
    "    predicted_valid_rand_for,\n",
    ") = get_f1_threshold(rand_for_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, нам удалось достичь значения F1-меры `0.6586` на валидационной выборке при значении порога - `0.59` для модели CatBoost с помощью техники downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ctb_down['Значения метрик_threshold'] = [accuracy_score(target_valid, predicted_valid_ctb_down),\n",
    "                                           recall_score(target_valid, predicted_valid_ctb_down),\n",
    "                                           precision_score(target_valid, predicted_valid_ctb_down),\n",
    "                                           f1_score(target_valid, predicted_valid_ctb_down),\n",
    "                                           roc_auc_score(target_valid, probabilities_one_valid_ctb_down)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7-section'></a>\n",
    "### 5. Визуализация метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#построим PR-кривую\n",
    "precision, recall, thresholds = precision_recall_curve(target_valid, probabilities_valid_ctb_down[:, 1])\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.step(recall, precision, where='post')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Кривая Precision-Recall')\n",
    "plt.scatter(df_ctb_down.loc['recall', 'Значения метрик_threshold'], df_ctb_down.loc['precision', 'Значения метрик_threshold'], \\\n",
    "            color='black', s=40)\n",
    "plt.annotate('Max F1-score', xy=(df_ctb_down.loc['recall', 'Значения метрик_threshold'], \\\n",
    "                                 df_ctb_down.loc['precision', 'Значения метрик_threshold']), xytext=(0.8, 0.8),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05)\n",
    "             )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5879"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# воспользуемся функцией predict_proba() для лучшей модели логистической регрессии\n",
    "predicted_valid_log_reg = log_reg_model_down.predict(features_valid)\n",
    "# значения вероятности классов для валидационной выборки\n",
    "probabilities_valid_log_reg = log_reg_model_down.predict_proba(features_valid)\n",
    "probabilities_one_valid_log_reg = probabilities_valid_log_reg[:, 1]\n",
    "round(f1_score(target_valid, predicted_valid_log_reg),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5953"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# воспользуемся функцией predict_proba() для лучшей модели решающего дерева\n",
    "predicted_valid_dec_tree = dec_tree_model.predict(features_valid)\n",
    "# значения вероятности классов для валидационной выборки\n",
    "probabilities_valid_dec_tree = dec_tree_model.predict_proba(features_valid)\n",
    "probabilities_one_valid_dec_tree = probabilities_valid_dec_tree[:, 1]\n",
    "round(f1_score(target_valid, predicted_valid_dec_tree),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# построим ROC-кривую\n",
    "# для сравнения на графике представлена ROC-кривая случайной модели\n",
    "fpr_lgb, tpr_lgb, thresholds_lgb = roc_curve(\n",
    "    target_valid, probabilities_one_valid_lgb_down)\n",
    "fpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(\n",
    "    target_valid, probabilities_one_valid_xgb)\n",
    "fpr_ctb, tpr_ctb, thresholds_ctb = roc_curve(\n",
    "    target_valid, probabilities_one_valid_ctb_down)\n",
    "fpr_rand_for, tpr_rand_for, thresholds_rand_for = roc_curve(\n",
    "    target_valid, probabilities_one_valid_rand_for)\n",
    "fpr_log_reg, tpr_log_reg, thresholds_log_reg = roc_curve(\n",
    "    target_valid, probabilities_one_valid_log_reg)\n",
    "fpr_dec_tree, tpr_dec_tree, thresholds_dec_tree = roc_curve(\n",
    "    target_valid, probabilities_one_valid_dec_tree)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.plot(fpr_lgb, tpr_lgb, label='LightGBM')\n",
    "plt.plot(fpr_xgb, tpr_xgb, label='XGBoost')\n",
    "plt.plot(fpr_ctb, tpr_ctb, label='CatBoost')\n",
    "plt.plot(fpr_rand_for, tpr_rand_for, label='Случайный лес')\n",
    "plt.plot(fpr_log_reg, tpr_log_reg, label='Логистическая регрессия')\n",
    "plt.plot(fpr_dec_tree, tpr_dec_tree, label='Решающее дерево')\n",
    "plt.legend()\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC-кривая')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате исследования моделей классификации было выявлено, что лучшая модель для данной задачи - CatBoost. Максимальное значение F1-меры такой модели равно `0.6586`. Оно получается после изменения порога до значения `0.59`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8-section'></a>\n",
    "### 1. Проверка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Значения метрик_down</th>\n",
       "      <th>Значения метрик_up</th>\n",
       "      <th>Значения метрик_init</th>\n",
       "      <th>Значения метрик_threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.796500</td>\n",
       "      <td>0.814000</td>\n",
       "      <td>0.865500</td>\n",
       "      <td>0.844500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.678922</td>\n",
       "      <td>0.477941</td>\n",
       "      <td>0.735294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.500759</td>\n",
       "      <td>0.534749</td>\n",
       "      <td>0.776892</td>\n",
       "      <td>0.596421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.618557</td>\n",
       "      <td>0.598272</td>\n",
       "      <td>0.591806</td>\n",
       "      <td>0.658617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>auc_roc</th>\n",
       "      <td>0.871090</td>\n",
       "      <td>0.852036</td>\n",
       "      <td>0.874340</td>\n",
       "      <td>0.871090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Значения метрик_down  Значения метрик_up  Значения метрик_init  \\\n",
       "accuracy               0.796500            0.814000              0.865500   \n",
       "recall                 0.808824            0.678922              0.477941   \n",
       "precision              0.500759            0.534749              0.776892   \n",
       "f1                     0.618557            0.598272              0.591806   \n",
       "auc_roc                0.871090            0.852036              0.874340   \n",
       "\n",
       "           Значения метрик_threshold  \n",
       "accuracy                    0.844500  \n",
       "recall                      0.735294  \n",
       "precision                   0.596421  \n",
       "f1                          0.658617  \n",
       "auc_roc                     0.871090  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ctb_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.623"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# значения вероятности классов для тестовой выборки\n",
    "probabilities_test = ctb_model_down.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "predicted_test = probabilities_one_test > best_threshold\n",
    "round(f1_score(target_test, predicted_test),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9-section'></a>\n",
    "## V. Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе была получена модель, которая прогнозирует, уйдёт клиент из банка в ближайшее время или нет.\n",
    "\n",
    "Данная модель - это модель градиентного бустинга CatBoost, которая успешно предсказывает результат примерно в `84.5%` случаев.\n",
    "\n",
    "Также были исследованы модели логистической регрессии, решающего дерева, случайного леса, LightGBM и XGBoost, но они показали результаты хуже CatBoost.\n",
    "\n",
    "Изначально при обучении модели на выборке с дисбалансом классов (отрицательных ответов в 4 раза больше положительных) модель случайного леса показала лучшее значение F1-меры - `0.6213`.\n",
    "\n",
    "После балансировки классов техникой увеличения / уменьшения выборки, значение метрики f1 у случайного леса стало только хуже. А вот CatBoost показал себя хорошо, увеличив значение с `0.5918` до `0.6185`. \n",
    "\n",
    "F1-меру удалось поднять благодаря изменению порога до `0.59`. Значение F1-меры увеличилось до `0.6586`.\n",
    "\n",
    "Также были построены кривые метрик: PR-кривая и ROC-кривая. Значение AUC-ROC `0.8445` говорит о том, что наша модель сильно отличается от случайной, т.к. чем график выше, тем больше значение TPR и, соответственно, лучше качество модели. \n",
    "\n",
    "На тестовой выборке модель показала себя хорошо: значение F1-меры равно `0.623`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
